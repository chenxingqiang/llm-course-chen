{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21c879e8",
   "metadata": {},
   "source": [
    "# Lesson 2: NLP Fundamentals\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Natural Language Processing (NLP) is a subfield of artificial intelligence that focuses on the interaction between computers and humans using natural language. This lesson will explore why NLP is crucial, what we can accomplish with it, and how NLP algorithms and models have evolved over time.\n",
    "\n",
    "## Lesson Objectives\n",
    "\n",
    "By the end of this lesson, you will:\n",
    "1. Understand the importance of NLP in modern AI applications\n",
    "2. Recognize the main types of NLP tasks\n",
    "3. Comprehend the evolution of NLP algorithms and models\n",
    "4. Gain insights into state-of-the-art NLP models and their applications\n",
    "\n",
    "## 1. Why We Need NLP\n",
    "\n",
    "Natural Language Processing is essential for several reasons:\n",
    "\n",
    "1. **Human-Computer Interaction**: NLP enables more natural and intuitive interaction between humans and computers.\n",
    "2. **Information Extraction**: It helps in extracting meaningful information from vast amounts of unstructured text data.\n",
    "3. **Automation**: NLP can automate many language-related tasks, saving time and resources.\n",
    "4. **Accessibility**: It can make information and services more accessible to people with disabilities or language barriers.\n",
    "5. **Decision Making**: NLP can assist in data-driven decision making by analyzing text-based information.\n",
    "\n",
    "## 2. What Can We Do with NLP?\n",
    "\n",
    "NLP encompasses a wide range of tasks, which can be broadly categorized into three main types:\n",
    "\n",
    "### 2.1 Classification Tasks\n",
    "\n",
    "Classification tasks involve categorizing text into predefined classes or categories. Examples include:\n",
    "\n",
    "- Sentiment Analysis\n",
    "- Spam Detection\n",
    "- Topic Classification\n",
    "- Language Identification\n",
    "\n",
    "Let's look at a simple example of sentiment analysis using the NLTK library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09fbf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "text = \"I love this course on NLP! It's very informative and engaging.\"\n",
    "sentiment_scores = sia.polarity_scores(text)\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Sentiment Scores: {sentiment_scores}\")\n",
    "print(f\"Overall Sentiment: {'Positive' if sentiment_scores['compound'] > 0 else 'Negative' if sentiment_scores['compound'] < 0 else 'Neutral'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ae4746",
   "metadata": {},
   "source": [
    "### 2.2 Extraction Tasks\n",
    "\n",
    "Extraction tasks involve identifying and extracting specific information from text. Examples include:\n",
    "\n",
    "- Named Entity Recognition (NER)\n",
    "- Relation Extraction\n",
    "- Key Phrase Extraction\n",
    "- Information Retrieval\n",
    "\n",
    "Here's a simple example of Named Entity Recognition using spaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e580dbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"Apple Inc. was founded by Steve Jobs in Cupertino, California.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f\"Entity: {ent.text}, Label: {ent.label_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6485328",
   "metadata": {},
   "source": [
    "### 2.3 Generation Tasks\n",
    "\n",
    "Generation tasks involve creating human-readable text based on input or prompts. Examples include:\n",
    "\n",
    "- Machine Translation\n",
    "- Text Summarization\n",
    "- Question Answering\n",
    "- Dialogue Systems\n",
    "- Text Completion\n",
    "\n",
    "Here's a simple example of text generation using a pre-trained GPT-2 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e6e759",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "prompt = \"Natural Language Processing is\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "output = model.generate(input_ids, max_length=50, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Generated Text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acbba9b",
   "metadata": {},
   "source": [
    "## 3. Evolution of NLP Algorithms and Models\n",
    "\n",
    "NLP has seen significant advancements over the years. Let's explore the key milestones in the evolution of NLP algorithms and models:\n",
    "\n",
    "### 3.1 Bag-of-Words Model\n",
    "\n",
    "The Bag-of-Words (BoW) model is one of the simplest representations of text in NLP. It represents text as an unordered set of words, disregarding grammar and word order but keeping multiplicity.\n",
    "\n",
    "Example using scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7131925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    \"Natural language processing is fascinating.\",\n",
    "    \"NLP models have evolved significantly over time.\",\n",
    "    \"Modern NLP uses advanced neural networks.\"\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "print(\"BoW representation:\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9485921f",
   "metadata": {},
   "source": [
    "### 3.2 Word2Vec\n",
    "\n",
    "Word2Vec, introduced by Google in 2013, is a group of related models used to produce word embeddings. These models are shallow, two-layer neural networks trained to reconstruct linguistic contexts of words.\n",
    "\n",
    "Example using gensim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a7b756",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [\n",
    "    [\"natural\", \"language\", \"processing\", \"is\", \"fascinating\"],\n",
    "    [\"nlp\", \"models\", \"have\", \"evolved\", \"significantly\", \"over\", \"time\"],\n",
    "    [\"modern\", \"nlp\", \"uses\", \"advanced\", \"neural\", \"networks\"]\n",
    "]\n",
    "\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "print(\"Similarity between 'nlp' and 'language':\", model.wv.similarity('nlp', 'language'))\n",
    "print(\"Most similar words to 'neural':\", model.wv.most_similar('neural'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b5f9b6",
   "metadata": {},
   "source": [
    "### 3.3 LSTM (Long Short-Term Memory)\n",
    "\n",
    "LSTM is a type of recurrent neural network (RNN) architecture designed to address the vanishing gradient problem faced by traditional RNNs. LSTMs are capable of learning long-term dependencies, making them particularly useful for sequential data like text.\n",
    "\n",
    "Example using Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e09e0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "texts = [\n",
    "    \"Natural language processing is fascinating\",\n",
    "    \"NLP models have evolved significantly over time\",\n",
    "    \"Modern NLP uses advanced neural networks\"\n",
    "]\n",
    "labels = [0, 1, 1]  # Binary classification example\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "padded_sequences = pad_sequences(sequences)\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=64),\n",
    "    LSTM(64),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(padded_sequences, labels, epochs=10, verbose=0)\n",
    "\n",
    "print(\"Model summary:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4084b2c",
   "metadata": {},
   "source": [
    "### 3.4 Transformer\n",
    "\n",
    "The Transformer model, introduced in the paper \"Attention Is All You Need\" by Vaswani et al. in 2017, revolutionized NLP by introducing the self-attention mechanism. This allowed for more parallelization during training and better handling of long-range dependencies.\n",
    "\n",
    "While implementing a full Transformer is beyond the scope of this lesson, here's a simplified example of the self-attention mechanism:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcee7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def self_attention(query, key, value):\n",
    "    attention_scores = np.dot(query, key.T) / np.sqrt(key.shape[1])\n",
    "    attention_weights = np.exp(attention_scores) / np.sum(np.exp(attention_scores), axis=1, keepdims=True)\n",
    "    output = np.dot(attention_weights, value)\n",
    "    return output\n",
    "\n",
    "# Example usage\n",
    "query = np.random.randn(2, 4)  # 2 queries, each with dimension 4\n",
    "key = np.random.randn(3, 4)    # 3 keys, each with dimension 4\n",
    "value = np.random.randn(3, 4)  # 3 values, each with dimension 4\n",
    "\n",
    "attention_output = self_attention(query, key, value)\n",
    "print(\"Self-attention output shape:\", attention_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42211a5",
   "metadata": {},
   "source": [
    "### 3.5 BERT (Bidirectional Encoder Representations from Transformers)\n",
    "\n",
    "BERT, introduced by Google in 2018, is a transformer-based model that uses bidirectional training of Transformer, allowing it to learn contextual relations between words (or sub-words) in a text.\n",
    "\n",
    "Example using the Transformers library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8f8dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "text = \"Natural language processing has advanced significantly.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "print(\"BERT output shape:\", outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a5f5e2",
   "metadata": {},
   "source": [
    "### 3.6 T5 (Text-to-Text Transfer Transformer)\n",
    "\n",
    "T5, introduced by Google in 2019, treats every NLP task as a \"text-to-text\" problem, where both the input and output are always text strings.\n",
    "\n",
    "Example using the Transformers library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce4e90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "input_text = \"translate English to German: Natural language processing is fascinating.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "outputs = model.generate(input_ids)\n",
    "translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Translated text:\", translated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb007a4",
   "metadata": {},
   "source": [
    "### 3.7 GPT-2 (Generative Pre-trained Transformer 2)\n",
    "\n",
    "GPT-2, introduced by OpenAI in 2019, is a large transformer-based language model with 1.5 billion parameters, trained on a dataset of 8 million web pages. It's known for its impressive text generation capabilities.\n",
    "\n",
    "We've already seen a GPT-2 example earlier in this lesson, but here's another one focusing on text completion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49be8371",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "prompt = \"The future of NLP is\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "output = model.generate(input_ids, max_length=50, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "completed_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Completed Text: {completed_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cc0da0",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this lesson, we've explored the fundamentals of Natural Language Processing, including why it's important, what tasks it can perform, and how NLP algorithms and models have evolved over time. From simple Bag-of-Words representations to sophisticated models like BERT and GPT-2, NLP has come a long way in its ability to understand and generate human language.\n",
    "\n",
    "As we progress through this course, we'll delve deeper into these models and learn how to apply them to solve real-world problems. The field of NLP is rapidly evolving, and staying updated with the latest advancements will be crucial for anyone working in this exciting domain.\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "1. \"Speech and Language Processing\" by Dan Jurafsky and James H. Martin\n",
    "2. \"Natural Language Processing with Transformers\" by Lewis Tunstall, Leandro von Werra, and Thomas Wolf\n",
    "3. The Illustrated Transformer by Jay Alammar: http://jalammar.github.io/illustrated-transformer/\n",
    "4. BERT paper: \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\"\n",
    "5. GPT-2 paper: \"Language Models are Unsupervised Multitask Learners\"\n",
    "\n",
    "In the next lesson, we'll explore the basic knowledge and architectural characteristics of Large Language Models in more detail."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
