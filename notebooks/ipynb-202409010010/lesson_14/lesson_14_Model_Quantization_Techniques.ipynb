{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35fbad26",
   "metadata": {},
   "source": [
    "# 1.Course Title: Model Quantization Techniques: A Comprehensive Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d130871",
   "metadata": {},
   "source": [
    "![Mermaid diagram](lesson_14_mermaid_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5f6862",
   "metadata": {},
   "source": [
    "Advanced Model Quantization Techniques for Efficient Inference\n",
    "\n",
    "## 2. Learning Objectives\n",
    "\n",
    "This course combines theory with hands-on practice. Upon completion, students will be able to:\n",
    "\n",
    "- 2.1 Comprehend the fundamental principles and necessity of model quantization\n",
    "- 2.2 Distinguish between various quantization methods and their applications\n",
    "- 2.3 Implement different quantization techniques using popular deep learning frameworks\n",
    "- 2.4 Evaluate the impact of quantization on model performance and resource utilization\n",
    "- 2.5 Apply quantization strategies to real-world deep learning models\n",
    "\n",
    "## 3. Overview\n",
    "\n",
    "This comprehensive lesson covers 5 key concepts, 4 case studies, and 3 hands-on experiments to achieve the learning objectives. We will explore:\n",
    "\n",
    "- 3.1 Foundations of model quantization and its importance in modern AI deployment\n",
    "- 3.2 Detailed examination of various quantization techniques\n",
    "- 3.3 Practical implementation of quantization using PyTorch, TensorFlow, and ONNX\n",
    "- 3.4 Performance analysis and trade-offs in quantized models\n",
    "- 3.5 Advanced quantization strategies for different model architectures\n",
    "\n",
    "## 4. Detailed Content\n",
    "\n",
    "### 4.1 Concept 1: Foundations of Model Quantization\n",
    "\n",
    "#### 4.1.1 Explanation\n",
    "\n",
    "Model quantization is a technique used to reduce the precision of weights and activations in neural networks. It's crucial for deploying large models on resource-constrained devices, improving inference speed, and reducing memory footprint.\n",
    "\n",
    "Key aspects:\n",
    "\n",
    "- Reduced precision representation (e.g., FP32 to INT8)\n",
    "- Storage and computational benefits\n",
    "- Potential impact on model accuracy\n",
    "\n",
    "#### 4.1.2 Case Study: MobileNetV2 Deployment on Smartphones\n",
    "\n",
    "Examine how Google successfully deployed MobileNetV2 on various smartphone devices using quantization, achieving significant speedup with minimal accuracy loss.\n",
    "\n",
    "#### 4.1.3 Code: Basic Int8 Quantization in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180aecad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Load a pre-trained model\n",
    "model = torch.load('pretrained_mobilenetv2.pth')\n",
    "\n",
    "# Define quantization configuration\n",
    "model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "\n",
    "# Prepare model for quantization\n",
    "model_prepared = torch.quantization.prepare(model)\n",
    "\n",
    "# Calibrate the model (usually done with a representative dataset)\n",
    "def calibrate(model, data_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in data_loader:\n",
    "            model(inputs)\n",
    "\n",
    "calibrate(model_prepared, calibration_data_loader)\n",
    "\n",
    "# Convert to quantized model\n",
    "quantized_model = torch.quantization.convert(model_prepared)\n",
    "\n",
    "# Compare model sizes\n",
    "original_size = sum(p.numel() for p in model.parameters()) * 4  # Assuming FP32\n",
    "quantized_size = sum(p.numel() for p in quantized_model.parameters())\n",
    "print(f\"Size reduction: {original_size / quantized_size:.2f}x\")\n",
    "\n",
    "# Evaluate and compare performance\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "original_accuracy = evaluate(model, test_data_loader)\n",
    "quantized_accuracy = evaluate(quantized_model, test_data_loader)\n",
    "\n",
    "print(f\"Original accuracy: {original_accuracy:.4f}\")\n",
    "print(f\"Quantized accuracy: {quantized_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593484e8",
   "metadata": {},
   "source": [
    "#### 4.1.4 Reflection\n",
    "\n",
    "Discuss the trade-offs between model size, inference speed, and accuracy when applying quantization. Consider:\n",
    "\n",
    "- How does the reduction in precision affect different types of neural network architectures?\n",
    "- What are the implications for energy consumption in mobile and edge devices?\n",
    "- How might quantization affect the interpretability of model decisions?\n",
    "\n",
    "### 4.2 Concept 2: Types of Quantization Techniques\n",
    "\n",
    "#### 4.2.1 Explanation\n",
    "\n",
    "We'll explore various quantization techniques, each with its own strengths and use cases:\n",
    "\n",
    "1. Post-training quantization\n",
    "   - Dynamic range quantization\n",
    "   - Static quantization\n",
    "2. Quantization-aware training\n",
    "3. Mixed-precision quantization\n",
    "\n",
    "figure-4.2.1.1 Quantization Techniques Comparison\n",
    "\n",
    "[A detailed comparison chart showing different quantization techniques, their characteristics, and suitable use cases]\n",
    "\n",
    "explanation of the figure: This comprehensive chart compares post-training quantization (both dynamic and static), quantization-aware training, and mixed-precision quantization. It highlights factors such as accuracy preservation, implementation complexity, training requirements, and suitable model types for each technique.\n",
    "\n",
    "table-4.2.1.1 Quantization Techniques Pros and Cons\n",
    "\n",
    "| Technique | Pros | Cons | Best For |\n",
    "|-----------|------|------|----------|\n",
    "| Dynamic Range Quantization | - Easy to implement<br>- No need for calibration data | - Lower accuracy compared to other methods<br>- Limited to activations | - Quick deployment<br>- Models with dynamic range of activations |\n",
    "| Static Quantization | - Better accuracy than dynamic<br>- Weights and activations quantized | - Requires calibration data<br>- More complex implementation | - Models with stable activation ranges<br>- When accuracy is crucial |\n",
    "| Quantization-Aware Training | - Highest accuracy<br>- Can recover from quantization errors | - Requires full training or fine-tuning<br>- Most time-consuming | - Mission-critical applications<br>- When resources for retraining are available |\n",
    "| Mixed-Precision Quantization | - Balances performance and accuracy<br>- Flexible for different layer requirements | - Complex to implement<br>- Requires careful tuning | - Large models with varying layer sensitivities<br>- When fine-grained control is needed |\n",
    "\n",
    "explanation of the table: This table provides a quick reference for the pros, cons, and ideal use cases of each quantization technique, helping developers choose the most appropriate method for their specific requirements.\n",
    "\n",
    "#### 4.2.2 Case Study: BERT Quantization for Natural Language Processing\n",
    "\n",
    "Explore how Hugging Face implemented efficient quantization for BERT models, enabling faster inference in production NLP systems.\n",
    "\n",
    "#### 4.2.3 Code: Implementing Quantization-Aware Training in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b4451d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define a simple model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Define quantization-aware training\n",
    "quantize_model = tf.keras.models.clone_model(\n",
    "    model,\n",
    "    clone_function=lambda layer: tf.keras.layers.quantization.quantize_annotate_layer(layer)\n",
    ")\n",
    "\n",
    "# Apply quantization to the model\n",
    "quantize_model = tf.keras.models.clone_model(\n",
    "    quantize_model,\n",
    "    clone_function=lambda layer: tf.keras.layers.quantization.quantize_apply(layer)\n",
    ")\n",
    "\n",
    "# Compile the quantized model\n",
    "quantize_model.compile(optimizer='adam',\n",
    "                       loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                       metrics=['accuracy'])\n",
    "\n",
    "# Train the quantized model\n",
    "quantize_model.fit(train_images, train_labels, epochs=5, validation_split=0.2)\n",
    "\n",
    "# Convert to TFLite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(quantize_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "quantized_tflite_model = converter.convert()\n",
    "\n",
    "# Save the quantized model\n",
    "with open('quantized_model.tflite', 'wb') as f:\n",
    "    f.write(quantized_tflite_model)\n",
    "\n",
    "# Evaluate the quantized model\n",
    "interpreter = tf.lite.Interpreter(model_content=quantized_tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "test_accuracy = 0\n",
    "\n",
    "for i, (test_image, test_label) in enumerate(zip(test_images, test_labels)):\n",
    "    interpreter.set_tensor(input_index, test_image)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_index)\n",
    "    predicted_label = output.argmax()\n",
    "    test_accuracy += 1 if predicted_label == test_label else 0\n",
    "\n",
    "print(f\"Quantized model accuracy: {test_accuracy / len(test_labels):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b248b952",
   "metadata": {},
   "source": [
    "#### 4.2.4 Reflection\n",
    "\n",
    "Consider the challenges and benefits of implementing different quantization techniques in real-world scenarios:\n",
    "\n",
    "- How does the choice of quantization method affect the development lifecycle?\n",
    "- What are the implications for model maintenance and updates?\n",
    "- How might different quantization techniques affect model robustness and generalization?\n",
    "\n",
    "### 4.3 Concept 3: Quantization for Different Model Architectures\n",
    "\n",
    "#### 4.3.1 Explanation\n",
    "\n",
    "Different neural network architectures may require specialized quantization approaches:\n",
    "\n",
    "- Convolutional Neural Networks (CNNs)\n",
    "- Recurrent Neural Networks (RNNs)\n",
    "- Transformer-based models\n",
    "- Graph Neural Networks (GNNs)\n",
    "\n",
    "#### 4.3.2 Case Study: Quantizing Transformer Models for Machine Translation\n",
    "\n",
    "Analyze how Google achieved significant speedup in neural machine translation by quantizing Transformer models.\n",
    "\n",
    "#### 4.3.3 Code: Quantizing a Pre-trained BERT Model with Hugging Face Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c3125f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Prepare the model for quantization\n",
    "model.eval()\n",
    "\n",
    "# Define quantization configuration\n",
    "model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "\n",
    "# Fuse modules\n",
    "model = torch.quantization.fuse_modules(model, [['bert.encoder.layer.0.attention.self.query', 'bert.encoder.layer.0.attention.self.key', 'bert.encoder.layer.0.attention.self.value']])\n",
    "\n",
    "# Prepare model for static quantization\n",
    "model_prepared = torch.quantization.prepare(model)\n",
    "\n",
    "# Calibrate the model (you would typically do this with a calibration dataset)\n",
    "def calibrate(model, data_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "calibrate(model_prepared, calibration_data_loader)\n",
    "\n",
    "# Convert to quantized model\n",
    "quantized_model = torch.quantization.convert(model_prepared)\n",
    "\n",
    "# Evaluate the quantized model\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            labels = batch['labels']\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "original_accuracy = evaluate(model, test_data_loader)\n",
    "quantized_accuracy = evaluate(quantized_model, test_data_loader)\n",
    "\n",
    "print(f\"Original accuracy: {original_accuracy:.4f}\")\n",
    "print(f\"Quantized accuracy: {quantized_accuracy:.4f}\")\n",
    "\n",
    "# Save the quantized model\n",
    "torch.save(quantized_model.state_dict(), 'quantized_bert.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a25557",
   "metadata": {},
   "source": [
    "#### 4.3.4 Reflection\n",
    "\n",
    "Discuss the challenges of quantizing complex model architectures:\n",
    "\n",
    "- How do attention mechanisms in Transformers affect quantization strategies?\n",
    "- What are the specific considerations for quantizing recurrent layers in RNNs?\n",
    "- How might quantization impact the graph structure in GNNs?\n",
    "\n",
    "### 4.4 Concept 4: Hardware-Aware Quantization\n",
    "\n",
    "#### 4.4.1 Explanation\n",
    "\n",
    "Different hardware platforms have varying support for quantized operations. Hardware-aware quantization optimizes models for specific devices:\n",
    "\n",
    "- CPUs (e.g., x86, ARM)\n",
    "- GPUs (e.g., NVIDIA, AMD)\n",
    "- Specialized AI accelerators (e.g., Google TPU, Apple Neural Engine)\n",
    "\n",
    "#### 4.4.2 Case Study: TensorRT Optimization for NVIDIA GPUs\n",
    "\n",
    "Examine how NVIDIA's TensorRT uses quantization and other optimizations to accelerate inference on GPUs.\n",
    "\n",
    "#### 4.4.3 Code: Quantization for TensorFlow Lite on Mobile Devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc6f15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load a pre-trained model\n",
    "model = tf.keras.applications.MobileNetV2(weights='imagenet', input_shape=(224, 224, 3))\n",
    "\n",
    "# Convert the model to TensorFlow Lite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "# Enable quantization\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# Define a representative dataset for quantization\n",
    "def representative_dataset_gen():\n",
    "    for _ in range(100):\n",
    "        yield [np.random.rand(1, 224, 224, 3).astype(np.float32)]\n",
    "\n",
    "converter.representative_dataset = representative_dataset_gen\n",
    "\n",
    "# Set the inference input and output types\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "\n",
    "# Convert the model\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model\n",
    "with open('quantized_mobilenet.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "# Load and run inference on the quantized model\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Prepare input data\n",
    "input_data = np.array(np.random.random_sample((1, 224, 224, 3)), dtype=np.float32)\n",
    "input_data = tf.keras.applications.mobilenet_v2.preprocess_input(input_data)\n",
    "\n",
    "# Set input tensor\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "# Run inference\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get output tensor\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "# Process the output\n",
    "predicted_label = np.argmax(output_data)\n",
    "print(f\"Predicted class: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94080aa1",
   "metadata": {},
   "source": [
    "#### 4.4.4 Reflection\n",
    "\n",
    "Consider the implications of hardware-aware quantization:\n",
    "\n",
    "- How does the choice of target hardware affect the quantization process?\n",
    "- What are the trade-offs between model portability and optimized performance?\n",
    "- How might hardware-aware quantization influence model design decisions?\n",
    "\n",
    "## 5. Summary\n",
    "\n",
    "### 5.1 Conclusion\n",
    "\n",
    "In this comprehensive lesson, we've explored the fundamental concepts of model quantization, various quantization techniques, and their practical implementations across different deep learning frameworks. We've seen how quantization can significantly reduce model size and improve inference speed, making it crucial for deploying large language models and other neural networks on resource-constrained devices.\n",
    "\n",
    "Key takeaKey takeaways include:\n",
    "\n",
    "- The importance of quantization in modern AI deployment\n",
    "- Various quantization techniques and their specific use cases\n",
    "- Practical implementation strategies using popular frameworks\n",
    "- Considerations for different model architectures and hardware platforms\n",
    "\n",
    "### 5.2 Mind Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adad0e49",
   "metadata": {},
   "source": [
    "![Mermaid diagram](lesson_14_mermaid_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd4568e",
   "metadata": {},
   "source": [
    "### 5.3 Preview\n",
    "\n",
    "In our next lesson, we will delve into advanced quantization techniques for edge computing scenarios. We'll explore how to optimize quantized models for ultra-low-power devices and examine emerging research in this rapidly evolving field.\n",
    "\n",
    "## 6. Homework\n",
    "\n",
    "1. Implement post-training static quantization on a pre-trained ResNet50 model using PyTorch. Compare its performance (accuracy and inference speed) with the original model on both CPU and GPU.\n",
    "\n",
    "2. Using TensorFlow Lite, quantize a pre-trained MobileNetV2 model to uint8 precision. Deploy this model on an Android device and measure the inference time improvement compared to the floating-point model.\n",
    "\n",
    "3. Research and write a short report (1000 words) on the latest advancements in quantization techniques for transformer-based models, focusing on their application in natural language processing tasks.\n",
    "\n",
    "4. Experiment with mixed-precision quantization on a custom neural network of your choice. Analyze how different bit-widths for weights and activations affect model performance and size.\n",
    "\n",
    "5. Implement quantization-aware training for a simple convolutional neural network on the CIFAR-10 dataset. Compare the final accuracy of the quantized model with a non-quantized baseline trained for the same number of epochs.\n",
    "\n",
    "## 7. Reference and Citation\n",
    "\n",
    "1. Jacob, B., et al. (2018). \"Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n",
    "\n",
    "2. Krishnamoorthi, R. (2018). \"Quantizing deep convolutional networks for efficient inference: A whitepaper.\" arXiv preprint arXiv:1806.08342.\n",
    "\n",
    "3. Gholami, A., et al. (2021). \"A Survey of Quantization Methods for Efficient Neural Network Inference.\" arXiv preprint arXiv:2103.13630.\n",
    "\n",
    "4. Wu, H., et al. (2020). \"Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation.\" arXiv preprint arXiv:2004.09602.\n",
    "\n",
    "5. Nagel, M., et al. (2021). \"A White Paper on Neural Network Quantization.\" arXiv preprint arXiv:2106.08295.\n",
    "\n",
    "6. PyTorch Quantization Documentation: <https://pytorch.org/docs/stable/quantization.html>\n",
    "\n",
    "7. TensorFlow Model Optimization Toolkit: <https://www.tensorflow.org/model_optimization>\n",
    "\n",
    "8. NVIDIA TensorRT Documentation: <https://developer.nvidia.com/tensorrt>\n",
    "\n",
    "9. Hugging Face Transformers Quantization Guide: <https://huggingface.co/docs/transformers/performance>\n",
    "\n",
    "10. Dettmers, T., et al. (2022). \"LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale.\" arXiv preprint arXiv:2208.07339.\n",
    "\n",
    "11. Han, S., Mao, H., & Dally, W. J. (2015). \"Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding.\" arXiv preprint arXiv:1510.00149.\n",
    "\n",
    "12. Zhu, M., & Gupta, S. (2017). \"To Prune, or Not to Prune: Exploring the Efficacy of Pruning for Model Compression.\" arXiv preprint arXiv:1710.01878."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
