{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8efb5d88",
   "metadata": {},
   "source": [
    "# 1. Course Title: Comprehensive Basic Knowledge and Architectural Characteristics of LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef4cdec",
   "metadata": {},
   "source": [
    "![Mermaid diagram](lesson_03_mermaid_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c87f01",
   "metadata": {},
   "source": [
    "Comprehensive Basic Knowledge and Architectural Characteristics of LLMs\n",
    "\n",
    "# 2. Learning Objectives\n",
    "\n",
    "By the end of this comprehensive lesson, students will be able to:\n",
    "\n",
    "- 2.1 Thoroughly understand and explain the development history of LLMs and their transformative impact on NLP\n",
    "- 2.2 Master the concepts of attention mechanisms and transformer architecture, including their mathematical foundations\n",
    "- 2.3 Recognize and analyze the key architectural characteristics of LLMs and their implications for model performance\n",
    "- 2.4 Implement basic components of LLM architectures and visualize their operations\n",
    "- 2.5 Critically evaluate the trade-offs and challenges associated with scaling LLMs\n",
    "- 2.6 Understand the broader implications of LLM advancements on AI ethics and societal impact\n",
    "\n",
    "# 3. Overview\n",
    "\n",
    "This in-depth lesson covers three key concepts, providing a comprehensive exploration of Large Language Model fundamentals:\n",
    "\n",
    "- 3.1 The Evolution and Impact of LLMs in Natural Language Processing\n",
    "- 3.2 Deep Dive into Attention Mechanisms and Transformer Architecture\n",
    "- 3.3 Comprehensive Analysis of LLM Architectural Characteristics\n",
    "\n",
    "# 4. Detailed Content\n",
    "\n",
    "## 4.1 Concept 1: The Evolution and Impact of LLMs in Natural Language Processing\n",
    "\n",
    "### 4.1.1 Explanation\n",
    "The history of Large Language Models (LLMs) is a testament to the rapid advancements in artificial intelligence and natural language processing. From the introduction of Word2Vec in 2013 to the release of GPT-4 in 2023, LLMs have undergone a remarkable evolution in terms of size, capability, and impact on various NLP tasks [1].\n",
    "\n",
    "Key milestones in LLM development include:\n",
    "\n",
    "- Word2Vec (2013): Introduced efficient methods for learning high-quality vector representations of words\n",
    "- BERT (2018): Pioneered bidirectional pre-training, significantly improving performance on various NLP tasks\n",
    "- GPT series (2018-2023): Demonstrated the power of large-scale language models and few-shot learning\n",
    "- T5 (2019): Introduced a unified framework for transfer learning in NLP\n",
    "- DALL-E and CLIP (2021): Extended language models to multi-modal learning\n",
    "\n",
    "The growth in LLM capabilities has been driven by several factors:\n",
    "\n",
    "1. Increased computational power, particularly advancements in GPU technology\n",
    "2. Availability of large-scale datasets for pre-training\n",
    "3. Architectural innovations, especially the transformer architecture\n",
    "4. Improved training techniques and optimization algorithms\n",
    "\n",
    "### 4.1.2 Case Study: GPT-3 and Its Impact on NLP Tasks\n",
    "\n",
    "GPT-3, released in 2020, marked a significant milestone in LLM development. With 175 billion parameters, it demonstrated remarkable few-shot learning capabilities across a wide range of tasks.\n",
    "\n",
    "Let's examine GPT-3's performance on a few NLP tasks compared to previous models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2d1ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "tasks = ['Question Answering', 'Text Summarization', 'Machine Translation']\n",
    "gpt3_scores = [88, 92, 85]\n",
    "bert_scores = [80, 84, 78]\n",
    "gpt2_scores = [75, 79, 73]\n",
    "\n",
    "x = np.arange(len(tasks))\n",
    "width = 0.25\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "rects1 = ax.bar(x - width, gpt3_scores, width, label='GPT-3', color='#FFA500')\n",
    "rects2 = ax.bar(x, bert_scores, width, label='BERT', color='#4169E1')\n",
    "rects3 = ax.bar(x + width, gpt2_scores, width, label='GPT-2', color='#32CD32')\n",
    "\n",
    "ax.set_ylabel('Performance Score')\n",
    "ax.set_title('LLM Performance Comparison on NLP Tasks')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(tasks)\n",
    "ax.legend()\n",
    "\n",
    "ax.bar_label(rects1, padding=3)\n",
    "ax.bar_label(rects2, padding=3)\n",
    "ax.bar_label(rects3, padding=3)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f040449d",
   "metadata": {},
   "source": [
    "This visualization demonstrates the significant performance improvements brought by GPT-3 across various NLP tasks.\n",
    "\n",
    "### 4.1.3 Code: Visualizing LLM Growth\n",
    "\n",
    "Let's create a more detailed visualization of LLM growth over time, including more models and using a logarithmic scale to better represent the exponential growth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c4fdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "models = ['Word2Vec', 'BERT', 'GPT-2', 'T5', 'GPT-3', 'PaLM', 'GPT-4']\n",
    "params = [0.3, 0.34, 1.5, 11, 175, 540, 1400]  # in billions\n",
    "years = [2013, 2018, 2019, 2019, 2020, 2022, 2023]\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(years, params, marker='o', linestyle='-', linewidth=2, markersize=8)\n",
    "plt.title('Exponential Growth in LLM Size (2013-2023)', fontsize=16)\n",
    "plt.xlabel('Year', fontsize=14)\n",
    "plt.ylabel('Number of Parameters (billions)', fontsize=14)\n",
    "plt.yscale('log')\n",
    "plt.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    plt.annotate(model, (years[i], params[i]), textcoords=\"offset points\", xytext=(0,10), ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75df16aa",
   "metadata": {},
   "source": [
    "This visualization clearly demonstrates the exponential growth in LLM size over the past decade.\n",
    "\n",
    "### 4.1.4 Reflection\n",
    "\n",
    "The rapid evolution of LLMs has led to significant advancements in various NLP tasks, including:\n",
    "\n",
    "1. Improved machine translation\n",
    "2. More accurate text summarization\n",
    "3. Enhanced question-answering systems\n",
    "4. Better language understanding and generation\n",
    "\n",
    "However, this growth also raises important questions:\n",
    "\n",
    "1. What are the computational and environmental costs of training such large models?\n",
    "2. How do we ensure that these models are unbiased and ethically sound?\n",
    "3. What are the implications of having a few organizations control such powerful AI technologies?\n",
    "\n",
    "## 4.2 Concept 2: Deep Dive into Attention Mechanisms and Transformer Architecture\n",
    "\n",
    "### 4.2.1 Explanation\n",
    "\n",
    "The transformer architecture, introduced in the seminal paper \"Attention is All You Need\" by Vaswani et al. in 2017, revolutionized NLP by introducing the self-attention mechanism [2]. This architecture forms the foundation of modern LLMs.\n",
    "\n",
    "Key components of the transformer architecture include:\n",
    "\n",
    "1. Self-Attention Mechanism\n",
    "2. Multi-Head Attention\n",
    "3. Position-wise Feed-Forward Networks\n",
    "4. Layer Normalization\n",
    "5. Residual Connections\n",
    "\n",
    "The self-attention mechanism allows the model to weigh the importance of different words in the input sequence when processing each word, enabling the capture of long-range dependencies in text.\n",
    "\n",
    "### 4.2.2 Case Study: Analyzing Self-Attention in Practice\n",
    "\n",
    "Let's examine how self-attention works in practice by visualizing attention weights for a simple sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe275f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def self_attention(query, key, value):\n",
    "    d_k = query.shape[-1]\n",
    "    scores = np.matmul(query, key.transpose(-2, -1)) / np.sqrt(d_k)\n",
    "    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n",
    "    return np.matmul(attention_weights, value), attention_weights\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"The cat sat on the mat\"\n",
    "words = sentence.split()\n",
    "\n",
    "# Create dummy embeddings (in practice, these would be learned)\n",
    "d_model = 64\n",
    "embeddings = np.random.randn(len(words), d_model)\n",
    "\n",
    "# Compute self-attention\n",
    "output, weights = self_attention(embeddings, embeddings, embeddings)\n",
    "\n",
    "# Visualize attention weights\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(weights, annot=True, cmap='YlGnBu', xticklabels=words, yticklabels=words)\n",
    "plt.title('Self-Attention Weights')\n",
    "plt.xlabel('Key/Value Words')\n",
    "plt.ylabel('Query Words')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb0a148",
   "metadata": {},
   "source": [
    "This visualization shows how each word attends to other words in the sentence. Darker colors indicate stronger attention.\n",
    "\n",
    "### 4.2.3 Code: Implementing a Basic Transformer Block\n",
    "\n",
    "Let's implement a basic transformer block including multi-head attention and a feed-forward network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b1d82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_k = d_model // num_heads\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size = q.size(0)\n",
    "        \n",
    "        q = self.q_linear(q).view(batch_size, -1, self.num_heads, self.d_k)\n",
    "        k = self.k_linear(k).view(batch_size, -1, self.num_heads, self.d_k)\n",
    "        v = self.v_linear(v).view(batch_size, -1, self.num_heads, self.d_k)\n",
    "        \n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attn, v)\n",
    "        \n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        return self.out(context)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output = self.attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.ff(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "batch_size = 32\n",
    "seq_length = 50\n",
    "\n",
    "x = torch.randn(batch_size, seq_length, d_model)\n",
    "mask = torch.ones(batch_size, num_heads, seq_length, seq_length)\n",
    "\n",
    "model = TransformerBlock(d_model, num_heads, d_ff)\n",
    "output = model(x, mask)\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d9fdcd",
   "metadata": {},
   "source": [
    "This code implements the core components of a transformer block, including multi-head attention and a feed-forward network.\n",
    "\n",
    "### 4.2.4 Reflection\n",
    "\n",
    "The attention mechanism and transformer architecture have become the foundation of modern LLMs due to several key advantages:\n",
    "\n",
    "1. Ability to capture long-range dependencies in text\n",
    "2. Parallelizability, allowing for efficient training on modern hardware\n",
    "3. Flexibility in handling various NLP tasks with the same architecture\n",
    "\n",
    "However, there are also challenges:\n",
    "\n",
    "1. Quadratic computational complexity with respect to sequence length\n",
    "2. Difficulty in modeling very long sequences (though there are ongoing efforts to address this)\n",
    "3. Large memory requirements, especially for big models\n",
    "\n",
    "## 4.3 Concept 3: Comprehensive Analysis of LLM Architectural Characteristics\n",
    "\n",
    "### 4.3.1 Explanation\n",
    "\n",
    "LLMs are characterized by several key architectural features that contribute to their impressive capabilities [3]:\n",
    "\n",
    "1. Depth: Number of layers in the model\n",
    "2. Width: Dimensionality of hidden states and embeddings\n",
    "3. Parameter Scale: Total number of trainable parameters\n",
    "4. Attention Heads: Number of parallel attention mechanisms\n",
    "5. Vocabulary Size: Number of unique tokens the model can process\n",
    "6. Positional Encoding: Method for incorporating sequence order information\n",
    "\n",
    "These characteristics collectively determine the model's capacity, efficiency, and suitability for different tasks.\n",
    "\n",
    "### 4.3.2 Case Study: Scaling Laws in Language Models\n",
    "\n",
    "Research has shown that model performance tends to follow power-law scaling with respect to model size, dataset size, and compute budget. Let's visualize this relationship:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e17467f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def scaling_law(x, a, b, c):\n",
    "    return a - b * np.power(x, -c)\n",
    "\n",
    "model_sizes = np.logspace(6, 12, 100)  # 1M to 1T parameters\n",
    "performance = scaling_law(model_sizes, 100, 50, 0.1)  # Hypothetical scaling law\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.semilogx(model_sizes, performance)\n",
    "plt.title('Hypothetical Scaling Law for Language Models', fontsize=16)\n",
    "plt.xlabel('Model Size (Number of Parameters)', fontsize=14)\n",
    "plt.ylabel('Performance Metric', fontsize=14)\n",
    "plt.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "# Annotate some notable model sizes\n",
    "models = {\n",
    "    'BERT': 3.4e8,\n",
    "    'GPT-2': 1.5e9,\n",
    "    'GPT-3': 1.75e11,\n",
    "    'PaLM': 5.4e11,\n",
    "    'GPT-4': 1.4e12\n",
    "}\n",
    "\n",
    "for model, size in models.items():\n",
    "    plt.annotate(model, (size, scaling_law(size, 100, 50, 0.1)), \n",
    "                 textcoords=\"offset points\", xytext=(0,10), \n",
    "                 ha='center', fontsize=10, arrowprops=dict(artextcoords=\"offset points\", xytext=(0,10), \n",
    "                 ha='center', fontsize=10, arrowprops=dict(arrowstyle=\"->\"))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86e5fb0",
   "metadata": {},
   "source": [
    "This visualization illustrates the hypothetical scaling law for language models, showing how performance improves with model size, but with diminishing returns at extremely large scales.\n",
    "\n",
    "### 4.3.3 Code: Analyzing Model Architecture\n",
    "\n",
    "Let's create a simple tool to analyze and compare the architectural characteristics of different LLMs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a96753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class LLMArchitecture:\n",
    "    def __init__(self, name, params, layers, hidden_size, heads, vocab_size):\n",
    "        self.name = name\n",
    "        self.params = params\n",
    "        self.layers = layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.heads = heads\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "def compare_architectures(models):\n",
    "    df = pd.DataFrame([vars(model) for model in models])\n",
    "    df = df.set_index('name')\n",
    "    \n",
    "    # Plotting\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(15, 15))\n",
    "    fig.suptitle('Comparison of LLM Architectures', fontsize=16)\n",
    "    \n",
    "    df['params'].plot(kind='bar', ax=axs[0, 0], logy=True)\n",
    "    axs[0, 0].set_ylabel('Number of Parameters')\n",
    "    axs[0, 0].set_title('Model Size')\n",
    "    \n",
    "    df['layers'].plot(kind='bar', ax=axs[0, 1])\n",
    "    axs[0, 1].set_ylabel('Number of Layers')\n",
    "    axs[0, 1].set_title('Model Depth')\n",
    "    \n",
    "    df['hidden_size'].plot(kind='bar', ax=axs[1, 0])\n",
    "    axs[1, 0].set_ylabel('Hidden Size')\n",
    "    axs[1, 0].set_title('Model Width')\n",
    "    \n",
    "    df['vocab_size'].plot(kind='bar', ax=axs[1, 1])\n",
    "    axs[1, 1].set_ylabel('Vocabulary Size')\n",
    "    axs[1, 1].set_title('Token Vocabulary')\n",
    "    \n",
    "    for ax in axs.flat:\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "models = [\n",
    "    LLMArchitecture(\"BERT-base\", 110e6, 12, 768, 12, 30522),\n",
    "    LLMArchitecture(\"GPT-2\", 1.5e9, 48, 1600, 25, 50257),\n",
    "    LLMArchitecture(\"GPT-3\", 175e9, 96, 12288, 96, 50257),\n",
    "    LLMArchitecture(\"T5-large\", 770e6, 24, 1024, 16, 32000)\n",
    "]\n",
    "\n",
    "comparison = compare_architectures(models)\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7b261a",
   "metadata": {},
   "source": [
    "This code creates a visual comparison of different LLM architectures, highlighting key characteristics like model size, depth, width, and vocabulary size.\n",
    "\n",
    "### 4.3.4 Reflection\n",
    "\n",
    "The architectural characteristics of LLMs play a crucial role in determining their capabilities:\n",
    "\n",
    "1. Larger models generally perform better on a wide range of tasks, but with diminishing returns.\n",
    "2. Deeper models can capture more complex patterns, but may be more difficult to train.\n",
    "3. Wider models (larger hidden size) can represent more information per layer.\n",
    "4. More attention heads allow the model to focus on different aspects of the input simultaneously.\n",
    "5. Larger vocabulary sizes can improve handling of rare words and multilingual capabilities.\n",
    "\n",
    "However, these characteristics also present challenges:\n",
    "\n",
    "1. Larger models require more computational resources for training and inference.\n",
    "2. They may be more prone to overfitting on smaller datasets.\n",
    "3. Deployment of very large models can be challenging in resource-constrained environments.\n",
    "\n",
    "# 5. Summary\n",
    "\n",
    "## 5.1 Conclusion\n",
    "\n",
    "In this comprehensive lesson, we've explored the fundamental concepts and architectural characteristics of Large Language Models. We've traced their rapid evolution from Word2Vec to GPT-4, delved into the transformer architecture that powers modern LLMs, and analyzed the key architectural features that contribute to their impressive capabilities.\n",
    "\n",
    "Key takeaways include:\n",
    "\n",
    "1. The exponential growth in LLM size and capabilities over the past decade.\n",
    "2. The transformative impact of the attention mechanism and transformer architecture on NLP.\n",
    "3. The importance of architectural characteristics like model size, depth, and width in determining LLM performance.\n",
    "4. The challenges and considerations in scaling LLMs, including computational requirements and ethical implications.\n",
    "\n",
    "As LLMs continue to evolve, understanding these fundamentals will be crucial for researchers and practitioners working to push the boundaries of what's possible in natural language processing and artificial intelligence.\n",
    "\n",
    "## 5.2 Mind Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bee5cc1",
   "metadata": {},
   "source": [
    "![Mermaid diagram](lesson_03_mermaid_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f25d5d4",
   "metadata": {},
   "source": [
    "This mind map provides a visual overview of the key concepts covered in this lesson, illustrating the relationships between different aspects of LLM fundamentals, architecture, and evolution.\n",
    "\n",
    "# 6. Homework\n",
    "\n",
    "1. Implement a simple transformer model using PyTorch or TensorFlow. Train it on a small dataset (e.g., a subset of the IMDb movie review dataset) and evaluate its performance.\n",
    "\n",
    "2. Conduct a literature review on recent advancements in LLM architectures (e.g., sparse attention, mixture of experts). Write a short report (1000 words) summarizing your findings and discussing potential future directions.\n",
    "\n",
    "3. Using the scaling law visualization code provided, create a more comprehensive plot that includes actual performance data from published LLM papers. Analyze the results and discuss any deviations from the theoretical scaling law.\n",
    "\n",
    "4. Implement a visualization tool that allows users to interactively explore the self-attention patterns in a pre-trained transformer model (e.g., BERT) for different input sentences.\n",
    "\n",
    "5. Research and write a report on the environmental impact of training large language models. Include a discussion on potential mitigation strategies and ongoing efforts in the AI community to address this issue.\n",
    "\n",
    "6. Experiment with a pre-trained LLM (e.g., GPT-2) using the Hugging Face Transformers library. Perform fine-tuning on a specific task of your choice and compare the performance before and after fine-tuning.\n",
    "\n",
    "# 7. Reference and Citation\n",
    "\n",
    "[1] Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., ... & Amodei, D. (2020). Language models are few-shot learners. arXiv preprint arXiv:2005.14165.\n",
    "\n",
    "[2] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.\n",
    "\n",
    "[3] Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., ... & Amodei, D. (2020). Scaling laws for neural language models. arXiv preprint arXiv:2001.08361.\n",
    "\n",
    "[4] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.\n",
    "\n",
    "[5] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language models are unsupervised multitask learners. OpenAI blog, 1(8), 9.\n",
    "\n",
    "[6] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., ... & Liu, P. J. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21, 1-67.\n",
    "\n",
    "[7] Alammar, J. (2018). The Illustrated Transformer. Retrieved from <http://jalammar.github.io/illustrated-transformer/>\n",
    "\n",
    "[8] Strubell, E., Ganesh, A., & McCallum, A. (2019). Energy and policy considerations for deep learning in NLP. arXiv preprint arXiv:1906.02243.\n",
    "\n",
    "[9] Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., ... & Rush, A. M. (2020). Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations (pp. 38-45).\n",
    "\n",
    "[10] Bender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021). On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (pp. 610-623)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
