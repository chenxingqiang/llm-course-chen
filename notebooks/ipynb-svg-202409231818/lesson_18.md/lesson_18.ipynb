{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe3dd3d7",
   "metadata": {},
   "source": [
    "# Lesson 18  LLM Model Deployment and Backend Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba979e1",
   "metadata": {},
   "source": [
    "<img src=\"data:image/svg+xml;base64,<svg aria-roledescription="gantt" role="graphics-document document" style="max-width: 1184px;" viewBox="0 0 1184 580" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" width="100%" id="mermaid-1727086733159"><style>#mermaid-1727086733159{font-family:"trebuchet ms",verdana,arial,sans-serif;font-size:16px;fill:#333;}#mermaid-1727086733159 .error-icon{fill:#552222;}#mermaid-1727086733159 .error-text{fill:#552222;stroke:#552222;}#mermaid-1727086733159 .edge-thickness-normal{stroke-width:1px;}#mermaid-1727086733159 .edge-thickness-thick{stroke-width:3.5px;}#mermaid-1727086733159 .edge-pattern-solid{stroke-dasharray:0;}#mermaid-1727086733159 .edge-thickness-invisible{stroke-width:0;fill:none;}#mermaid-1727086733159 .edge-pattern-dashed{stroke-dasharray:3;}#mermaid-1727086733159 .edge-pattern-dotted{stroke-dasharray:2;}#mermaid-1727086733159 .marker{fill:#333333;stroke:#333333;}#mermaid-1727086733159 .marker.cross{stroke:#333333;}#mermaid-1727086733159 svg{font-family:"trebuchet ms",verdana,arial,sans-serif;font-size:16px;}#mermaid-1727086733159 p{margin:0;}#mermaid-1727086733159 .mermaid-main-font{font-family:var(--mermaid-font-family, "trebuchet ms", verdana, arial, sans-serif);}#mermaid-1727086733159 .exclude-range{fill:#eeeeee;}#mermaid-1727086733159 .section{stroke:none;opacity:0.2;}#mermaid-1727086733159 .section0{fill:rgba(102, 102, 255, 0.49);}#mermaid-1727086733159 .section2{fill:#fff400;}#mermaid-1727086733159 .section1,#mermaid-1727086733159 .section3{fill:white;opacity:0.2;}#mermaid-1727086733159 .sectionTitle0{fill:#333;}#mermaid-1727086733159 .sectionTitle1{fill:#333;}#mermaid-1727086733159 .sectionTitle2{fill:#333;}#mermaid-1727086733159 .sectionTitle3{fill:#333;}#mermaid-1727086733159 .sectionTitle{text-anchor:start;font-family:var(--mermaid-font-family, "trebuchet ms", verdana, arial, sans-serif);}#mermaid-1727086733159 .grid .tick{stroke:lightgrey;opacity:0.8;shape-rendering:crispEdges;}#mermaid-1727086733159 .grid .tick text{font-family:"trebuchet ms",verdana,arial,sans-serif;fill:#333;}#mermaid-1727086733159 .grid path{stroke-width:0;}#mermaid-1727086733159 .today{fill:none;stroke:red;stroke-width:2px;}#mermaid-1727086733159 .task{stroke-width:2;}#mermaid-1727086733159 .taskText{text-anchor:middle;font-family:var(--mermaid-font-family, "trebuchet ms", verdana, arial, sans-serif);}#mermaid-1727086733159 .taskTextOutsideRight{fill:black;text-anchor:start;font-family:var(--mermaid-font-family, "trebuchet ms", verdana, arial, sans-serif);}#mermaid-1727086733159 .taskTextOutsideLeft{fill:black;text-anchor:end;}#mermaid-1727086733159 .task.clickable{cursor:pointer;}#mermaid-1727086733159 .taskText.clickable{cursor:pointer;fill:#003163!important;font-weight:bold;}#mermaid-1727086733159 .taskTextOutsideLeft.clickable{cursor:pointer;fill:#003163!important;font-weight:bold;}#mermaid-1727086733159 .taskTextOutsideRight.clickable{cursor:pointer;fill:#003163!important;font-weight:bold;}#mermaid-1727086733159 .taskText0,#mermaid-1727086733159 .taskText1,#mermaid-1727086733159 .taskText2,#mermaid-1727086733159 .taskText3{fill:white;}#mermaid-1727086733159 .task0,#mermaid-1727086733159 .task1,#mermaid-1727086733159 .task2,#mermaid-1727086733159 .task3{fill:#8a90dd;stroke:#534fbc;}#mermaid-1727086733159 .taskTextOutside0,#mermaid-1727086733159 .taskTextOutside2{fill:black;}#mermaid-1727086733159 .taskTextOutside1,#mermaid-1727086733159 .taskTextOutside3{fill:black;}#mermaid-1727086733159 .active0,#mermaid-1727086733159 .active1,#mermaid-1727086733159 .active2,#mermaid-1727086733159 .active3{fill:#bfc7ff;stroke:#534fbc;}#mermaid-1727086733159 .activeText0,#mermaid-1727086733159 .activeText1,#mermaid-1727086733159 .activeText2,#mermaid-1727086733159 .activeText3{fill:black!important;}#mermaid-1727086733159 .done0,#mermaid-1727086733159 .done1,#mermaid-1727086733159 .done2,#mermaid-1727086733159 .done3{stroke:grey;fill:lightgrey;stroke-width:2;}#mermaid-1727086733159 .doneText0,#mermaid-1727086733159 .doneText1,#mermaid-1727086733159 .doneText2,#mermaid-1727086733159 .doneText3{fill:black!important;}#mermaid-1727086733159 .crit0,#mermaid-1727086733159 .crit1,#mermaid-1727086733159 .crit2,#mermaid-1727086733159 .crit3{stroke:#ff8888;fill:red;stroke-width:2;}#mermaid-1727086733159 .activeCrit0,#mermaid-1727086733159 .activeCrit1,#mermaid-1727086733159 .activeCrit2,#mermaid-1727086733159 .activeCrit3{stroke:#ff8888;fill:#bfc7ff;stroke-width:2;}#mermaid-1727086733159 .doneCrit0,#mermaid-1727086733159 .doneCrit1,#mermaid-1727086733159 .doneCrit2,#mermaid-1727086733159 .doneCrit3{stroke:#ff8888;fill:lightgrey;stroke-width:2;cursor:pointer;shape-rendering:crispEdges;}#mermaid-1727086733159 .milestone{transform:rotate(45deg) scale(0.8,0.8);}#mermaid-1727086733159 .milestoneText{font-style:italic;}#mermaid-1727086733159 .doneCritText0,#mermaid-1727086733159 .doneCritText1,#mermaid-1727086733159 .doneCritText2,#mermaid-1727086733159 .doneCritText3{fill:black!important;}#mermaid-1727086733159 .activeCritText0,#mermaid-1727086733159 .activeCritText1,#mermaid-1727086733159 .activeCritText2,#mermaid-1727086733159 .activeCritText3{fill:black!important;}#mermaid-1727086733159 .titleText{text-anchor:middle;font-size:18px;fill:#333;font-family:var(--mermaid-font-family, "trebuchet ms", verdana, arial, sans-serif);}#mermaid-1727086733159 :root{--mermaid-font-family:"trebuchet ms",verdana,arial,sans-serif;}</style><g></g><g text-anchor="middle" font-family="sans-serif" font-size="10" fill="none" transform="translate(75, 530)" class="grid"><path d="M0,-495V0H1034V-495" stroke="currentColor" class="domain"></path><g transform="translate(69,0)" opacity="1" class="tick"><line y2="-495" stroke="currentColor"></line><text style="text-anchor: middle;" font-size="10" stroke="none" dy="1em" y="3" fill="#000">02</text></g><g transform="translate(172,0)" opacity="1" class="tick"><line y2="-495" stroke="currentColor"></line><text style="text-anchor: middle;" font-size="10" stroke="none" dy="1em" y="3" fill="#000">03</text></g><g transform="translate(276,0)" opacity="1" class="tick"><line y2="-495" stroke="currentColor"></line><text style="text-anchor: middle;" font-size="10" stroke="none" dy="1em" y="3" fill="#000">04</text></g><g transform="translate(379,0)" opacity="1" class="tick"><line y2="-495" stroke="currentColor"></line><text style="text-anchor: middle;" font-size="10" stroke="none" dy="1em" y="3" fill="#000">05</text></g><g transform="translate(483,0)" opacity="1" class="tick"><line y2="-495" stroke="currentColor"></line><text style="text-anchor: middle;" font-size="10" stroke="none" dy="1em" y="3" fill="#000">06</text></g><g transform="translate(586,0)" opacity="1" class="tick"><line y2="-495" stroke="currentColor"></line><text style="text-anchor: middle;" font-size="10" stroke="none" dy="1em" y="3" fill="#000">07</text></g><g transform="translate(689,0)" opacity="1" class="tick"><line y2="-495" stroke="currentColor"></line><text style="text-anchor: middle;" font-size="10" stroke="none" dy="1em" y="3" fill="#000">08</text></g><g transform="translate(793,0)" opacity="1" class="tick"><line y2="-495" stroke="currentColor"></line><text style="text-anchor: middle;" font-size="10" stroke="none" dy="1em" y="3" fill="#000">09</text></g><g transform="translate(896,0)" opacity="1" class="tick"><line y2="-495" stroke="currentColor"></line><text style="text-anchor: middle;" font-size="10" stroke="none" dy="1em" y="3" fill="#000">10</text></g><g transform="translate(1000,0)" opacity="1" class="tick"><line y2="-495" stroke="currentColor"></line><text style="text-anchor: middle;" font-size="10" stroke="none" dy="1em" y="3" fill="#000">11</text></g></g><g><rect class="section section0" height="24" width="1146.5" y="48" x="0"></rect><rect class="section section1" height="24" width="1146.5" y="288" x="0"></rect><rect class="section section0" height="24" width="1146.5" y="72" x="0"></rect><rect class="section section1" height="24" width="1146.5" y="312" x="0"></rect><rect class="section section0" height="24" width="1146.5" y="96" x="0"></rect><rect class="section section1" height="24" width="1146.5" y="336" x="0"></rect><rect class="section section0" height="24" width="1146.5" y="120" x="0"></rect><rect class="section section1" height="24" width="1146.5" y="360" x="0"></rect><rect class="section section0" height="24" width="1146.5" y="144" x="0"></rect><rect class="section section1" height="24" width="1146.5" y="384" x="0"></rect><rect class="section section0" height="24" width="1146.5" y="168" x="0"></rect><rect class="section section1" height="24" width="1146.5" y="408" x="0"></rect><rect class="section section0" height="24" width="1146.5" y="192" x="0"></rect><rect class="section section1" height="24" width="1146.5" y="432" x="0"></rect><rect class="section section0" height="24" width="1146.5" y="216" x="0"></rect><rect class="section section1" height="24" width="1146.5" y="456" x="0"></rect><rect class="section section0" height="24" width="1146.5" y="240" x="0"></rect><rect class="section section1" height="24" width="1146.5" y="480" x="0"></rect><rect class="section section0" height="24" width="1146.5" y="264" x="0"></rect><rect class="section section1" height="24" width="1146.5" y="504" x="0"></rect></g><g><rect class="task task0" transform-origin="126.5px 60px" height="20" width="103" y="50" x="75" ry="3" rx="3" id="a14"></rect><rect class="task task1" transform-origin="126.5px 300px" height="20" width="103" y="290" x="75" ry="3" rx="3" id="l14"></rect><rect class="task task0" transform-origin="230px 84px" height="20" width="104" y="74" x="178" ry="3" rx="3" id="a15"></rect><rect class="task task1" transform-origin="230px 324px" height="20" width="104" y="314" x="178" ry="3" rx="3" id="l15"></rect><rect class="task task0" transform-origin="333.5px 108px" height="20" width="103" y="98" x="282" ry="3" rx="3" id="a16"></rect><rect class="task task1" transform-origin="333.5px 348px" height="20" width="103" y="338" x="282" ry="3" rx="3" id="l16"></rect><rect class="task task0" transform-origin="437px 132px" height="20" width="104" y="122" x="385" ry="3" rx="3" id="a17"></rect><rect class="task task1" transform-origin="437px 372px" height="20" width="104" y="362" x="385" ry="3" rx="3" id="l17"></rect><rect class="task active0" transform-origin="540.5px 156px" height="20" width="103" y="146" x="489" ry="3" rx="3" id="a18"></rect><rect class="task active1" transform-origin="540.5px 396px" height="20" width="103" y="386" x="489" ry="3" rx="3" id="l18"></rect><rect class="task task0" transform-origin="643.5px 180px" height="20" width="103" y="170" x="592" ry="3" rx="3" id="a19"></rect><rect class="task task1" transform-origin="643.5px 420px" height="20" width="103" y="410" x="592" ry="3" rx="3" id="l19"></rect><rect class="task task0" transform-origin="747px 204px" height="20" width="104" y="194" x="695" ry="3" rx="3" id="a20"></rect><rect class="task task1" transform-origin="747px 444px" height="20" width="104" y="434" x="695" ry="3" rx="3" id="l20"></rect><rect class="task task0" transform-origin="850.5px 228px" height="20" width="103" y="218" x="799" ry="3" rx="3" id="a21"></rect><rect class="task task1" transform-origin="850.5px 468px" height="20" width="103" y="458" x="799" ry="3" rx="3" id="l21"></rect><rect class="task task0" transform-origin="954px 252px" height="20" width="104" y="242" x="902" ry="3" rx="3" id="a22"></rect><rect class="task task1" transform-origin="954px 492px" height="20" width="104" y="482" x="902" ry="3" rx="3" id="l22"></rect><rect class="task task0" transform-origin="1057.5px 276px" height="20" width="103" y="266" x="1006" ry="3" rx="3" id="a23"></rect><rect class="task task1" transform-origin="1057.5px 516px" height="20" width="103" y="506" x="1006" ry="3" rx="3" id="l23"></rect><text class="taskTextOutsideRight taskTextOutside0  width-152.8671875" y="63.5" x="183" font-size="11" id="a14-text">Model Quantization Techniques                      </text><text class="taskText taskText1  width-44.9140625" y="303.5" x="126.5" font-size="11" id="l14-text">lesson 14 </text><text class="taskTextOutsideRight taskTextOutside0  width-155.578125" y="87.5" x="287" font-size="11" id="a15-text">Introduction to Chatbot Project                    </text><text class="taskText taskText1  width-44.9140625" y="327.5" x="230" font-size="11" id="l15-text">lesson 15 </text><text class="taskTextOutsideRight taskTextOutside0  width-222.2109375" y="111.5" x="390" font-size="11" id="a16-text">Test Dataset Collection and Model Evaluation       </text><text class="taskText taskText1  width-44.9140625" y="351.5" x="333.5" font-size="11" id="l16-text">lesson 16 </text><text class="taskTextOutsideRight taskTextOutside0  width-298.875" y="135.5" x="494" font-size="11" id="a17-text">Designing input and output formats for chatbot with context </text><text class="taskText taskText1  width-44.9140625" y="375.5" x="437" font-size="11" id="l17-text">lesson 17 </text><text class="taskTextOutsideRight taskTextOutside0 activeText0 width-225.25" y="159.5" x="597" font-size="11" id="a18-text">Model Deployment and Backend Development           </text><text class="taskText taskText1 activeText1 width-44.9140625" y="399.5" x="540.5" font-size="11" id="l18-text">lesson 18 </text><text class="taskTextOutsideRight taskTextOutside0  width-148.125" y="183.5" x="700" font-size="11" id="a19-text">Frontend web page debugging                        </text><text class="taskText taskText1  width-44.9140625" y="423.5" x="643.5" font-size="11" id="l19-text">lesson 19 </text><text class="taskTextOutsideRight taskTextOutside0  width-155.7734375" y="207.5" x="804" font-size="11" id="a20-text">System Testing and Deployment                      </text><text class="taskText taskText1  width-44.9140625" y="447.5" x="747" font-size="11" id="l20-text">lesson 20 </text><text class="taskText taskText0  width-84.2578125" y="231.5" x="850.5" font-size="11" id="a21-text">RAG Introduction                                   </text><text class="taskText taskText1  width-44.9140625" y="471.5" x="850.5" font-size="11" id="l21-text">lesson 21 </text><text class="taskTextOutsideLeft taskTextOutside0" y="255.5" x="897" font-size="11" id="a22-text">RAG Frameworks - Introduction and use of Llamaindex and LangChain </text><text class="taskText taskText1  width-44.9140625" y="495.5" x="954" font-size="11" id="l22-text">lesson 22 </text><text class="taskTextOutsideLeft taskTextOutside0" y="279.5" x="1001" font-size="11" id="a23-text">RAG embedding model                                </text><text class="taskText taskText1  width-44.9140625" y="519.5" x="1057.5" font-size="11" id="l23-text">lesson 23 </text></g><g><text class="sectionTitle sectionTitle0" font-size="11" y="170" x="10" dy="0em"><tspan x="10" alignment-baseline="central">Course Content</tspan></text><text class="sectionTitle sectionTitle1" font-size="11" y="410" x="10" dy="0em"><tspan x="10" alignment-baseline="central">Lessons</tspan></text></g><g class="today"><line class="today" y2="555" y1="25" x2="2066982" x1="2066982"></line></g><text class="titleText" y="25" x="592">LLM Course Timeline</text></svg>\" alt=\"Mermaid diagram 1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5798b982",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In the rapidly evolving landscape of artificial intelligence, Large Language Models (LLMs) have emerged as powerful tools capable of understanding and generating human-like text. However, the journey from a trained model to a production-ready application is fraught with challenges. This lesson delves into the critical processes of deploying LLMs and developing robust backend systems to support them, bridging the gap between experimental success and real-world application.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this comprehensive lesson, you will be able to:\n",
    "\n",
    "1. Convert and optimize LLMs for deployment using formats like ONNX and TensorRT\n",
    "2. Implement effective model deployment strategies, including containerization and server-side deployment\n",
    "3. Develop efficient model inference pipelines for real-time text generation\n",
    "4. Design and implement scalable backend systems to support LLM-based applications\n",
    "5. Apply best practices in security, performance optimization, and error handling in LLM deployments\n",
    "\n",
    "## Model Conversion and Optimization\n",
    "\n",
    "### The Necessity of Model Conversion\n",
    "\n",
    "LLMs, often trained using frameworks like PyTorch or TensorFlow, may not be directly deployable in various production environments. Converting these models to standardized formats ensures compatibility and optimizes performance across different platforms.\n",
    "\n",
    "### ONNX: Open Neural Network Exchange\n",
    "\n",
    "ONNX has emerged as a popular choice for model interoperability. It allows models trained in one framework to be easily deployed in another, providing flexibility in deployment options.\n",
    "\n",
    "Let's examine a practical example of converting a PyTorch-based GPT-2 model to ONNX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471aa059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def convert_to_onnx(model_name, output_path):\n",
    "    # Load the pre-trained model and tokenizer\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Prepare a sample input\n",
    "    sample_text = \"The future of AI is\"\n",
    "    input_ids = tokenizer.encode(sample_text, return_tensors=\"pt\")\n",
    "\n",
    "    # Export the model to ONNX\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        input_ids,\n",
    "        output_path,\n",
    "        input_names=['input_ids'],\n",
    "        output_names=['output'],\n",
    "        dynamic_axes={\n",
    "            'input_ids': {0: 'batch_size', 1: 'sequence'},\n",
    "            'output': {0: 'batch_size', 1: 'sequence'}\n",
    "        },\n",
    "        opset_version=11\n",
    "    )\n",
    "    print(f\"Model successfully converted to ONNX and saved at {output_path}\")\n",
    "\n",
    "# Usage\n",
    "convert_to_onnx(\"gpt2\", \"gpt2_model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a3b879",
   "metadata": {},
   "source": [
    "This script loads a pre-trained GPT-2 model, prepares a sample input, and exports the model to ONNX format. The `dynamic_axes` parameter allows for flexibility in input sizes, crucial for handling varying text lengths in production.\n",
    "\n",
    "### TensorRT: High-Performance Inference\n",
    "\n",
    "For deployments targeting NVIDIA GPUs, TensorRT offers significant performance improvements through optimizations like layer fusion and precision calibration.\n",
    "\n",
    "Here's an example of how to optimize an ONNX model using TensorRT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc839944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "import numpy as np\n",
    "\n",
    "def build_tensorrt_engine(onnx_path, engine_path):\n",
    "    logger = trt.Logger(trt.Logger.WARNING)\n",
    "    builder = trt.Builder(logger)\n",
    "    network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n",
    "    parser = trt.OnnxParser(network, logger)\n",
    "    \n",
    "    with open(onnx_path, 'rb') as model:\n",
    "        if not parser.parse(model.read()):\n",
    "            print('ERROR: Failed to parse the ONNX file.')\n",
    "            for error in range(parser.num_errors):\n",
    "                print(parser.get_error(error))\n",
    "            return None\n",
    "    \n",
    "    config = builder.create_builder_config()\n",
    "    config.max_workspace_size = 1 << 30  # 1GB\n",
    "    profile = builder.create_optimization_profile()\n",
    "    profile.set_shape(\"input_ids\", (1, 1), (1, 128), (1, 256))\n",
    "    config.add_optimization_profile(profile)\n",
    "    \n",
    "    engine = builder.build_engine(network, config)\n",
    "    \n",
    "    with open(engine_path, \"wb\") as f:\n",
    "        f.write(engine.serialize())\n",
    "    \n",
    "    print(f\"TensorRT engine built and saved at {engine_path}\")\n",
    "\n",
    "# Usage\n",
    "build_tensorrt_engine(\"gpt2_model.onnx\", \"gpt2_model.trt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98efb3af",
   "metadata": {},
   "source": [
    "This script takes an ONNX model, applies TensorRT optimizations, and produces a serialized engine file. The optimization profile allows for dynamic input shapes, essential for handling variable-length inputs in text generation tasks.\n",
    "\n",
    "### Benchmarking and Validation\n",
    "\n",
    "After conversion, it's crucial to benchmark the optimized models against the original to ensure performance gains and output consistency. Here's a simple benchmarking script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764d85e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "\n",
    "def benchmark_models(input_text, tokenizer, pytorch_model, onnx_path):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    \n",
    "    # PyTorch inference\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        pytorch_output = pytorch_model.generate(input_ids, max_length=50)\n",
    "    pytorch_time = time.time() - start_time\n",
    "    \n",
    "    # ONNX inference\n",
    "    ort_session = onnxruntime.InferenceSession(onnx_path)\n",
    "    ort_inputs = {ort_session.get_inputs()[0].name: input_ids.numpy()}\n",
    "    start_time = time.time()\n",
    "    onnx_output = ort_session.run(None, ort_inputs)\n",
    "    onnx_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"PyTorch inference time: {pytorch_time:.4f} seconds\")\n",
    "    print(f\"ONNX inference time: {onnx_time:.4f} seconds\")\n",
    "    print(f\"Speedup: {pytorch_time / onnx_time:.2f}x\")\n",
    "    \n",
    "    # Validate outputs\n",
    "    pytorch_text = tokenizer.decode(pytorch_output[0], skip_special_tokens=True)\n",
    "    onnx_text = tokenizer.decode(onnx_output[0][0], skip_special_tokens=True)\n",
    "    print(f\"PyTorch output: {pytorch_text}\")\n",
    "    print(f\"ONNX output: {onnx_text}\")\n",
    "    print(f\"Outputs match: {pytorch_text == onnx_text}\")\n",
    "\n",
    "# Usage\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "pytorch_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "input_text = \"The future of AI is\"\n",
    "\n",
    "benchmark_models(input_text, tokenizer, pytorch_model, \"gpt2_model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3406d799",
   "metadata": {},
   "source": [
    "This benchmarking script compares inference time between PyTorch and ONNX models and validates output consistency, providing crucial insights into the effectiveness of the optimization process.\n",
    "\n",
    "By mastering these conversion and optimization techniques, you lay the groundwork for efficient LLM deployment. In the next section, we'll explore strategies for deploying these optimized models in production environments.\n",
    "\n",
    "## Model Deployment Strategies\n",
    "\n",
    "After optimizing your LLM, the next crucial step is deploying it in a production environment. This section explores two primary deployment strategies: containerization with Docker and direct server deployment. Each approach has its merits, and the choice often depends on your specific infrastructure and scalability requirements.\n",
    "\n",
    "### Containerized Deployment with Docker\n",
    "\n",
    "Containerization has revolutionized application deployment by providing consistency across different environments. Docker, in particular, has become the de facto standard for containerization in the AI/ML world.\n",
    "\n",
    "### Benefits of Docker for LLM Deployment\n",
    "\n",
    "1. **Consistency**: Docker ensures that your model runs in the same environment, regardless of where it's deployed.\n",
    "2. **Isolation**: Containers isolate the application and its dependencies, reducing conflicts with other system components.\n",
    "3. **Scalability**: Docker containers can be easily scaled horizontally to handle increased load.\n",
    "4. **Versioning**: Docker images can be versioned, allowing for easy rollbacks and A/B testing of different model versions.\n",
    "\n",
    "### Implementing Docker Deployment\n",
    "\n",
    "Let's walk through the process of containerizing our LLM inference server:\n",
    "\n",
    "1. First, create a `Dockerfile`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9f6136",
   "metadata": {},
   "source": [
    "```dockerfile\n",
    "# Use an official Python runtime as the base image\n",
    "FROM python:3.8-slim-buster\n",
    "\n",
    "# Set the working directory in the container\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy the requirements file into the container\n",
    "COPY requirements.txt .\n",
    "\n",
    "# Install the required packages\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy the model files and server code into the container\n",
    "COPY model/ /app/model/\n",
    "COPY server.py .\n",
    "\n",
    "# Make port 8000 available to the world outside this container\n",
    "EXPOSE 8000\n",
    "\n",
    "# Run the server when the container launches\n",
    "CMD [\"uvicorn\", \"server:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16db4bf8",
   "metadata": {},
   "source": [
    "2. Create a `requirements.txt` file listing all necessary Python packages:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4a6b67",
   "metadata": {},
   "source": [
    "```\n",
    "fastapi==0.68.0\n",
    "uvicorn==0.15.0\n",
    "torch==1.9.0\n",
    "transformers==4.9.2\n",
    "onnxruntime==1.8.1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879f79c8",
   "metadata": {},
   "source": [
    "3. Implement the server code in `server.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da1c8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "import onnxruntime\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Load the ONNX model\n",
    "session = onnxruntime.InferenceSession(\"model/gpt2_model.onnx\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "@app.post(\"/generate/\")\n",
    "async def generate_text(prompt: str):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"np\")\n",
    "    output = session.run(None, {\"input_ids\": input_ids})\n",
    "    generated_text = tokenizer.decode(output[0][0], skip_special_tokens=True)\n",
    "    return {\"generated_text\": generated_text}\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    return {\"status\": \"healthy\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b9c2e7",
   "metadata": {},
   "source": [
    "4. Build and run the Docker container:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e0e4b6",
   "metadata": {},
   "source": [
    "```bash\n",
    "docker build -t llm-server .\n",
    "docker run -p 8000:8000 llm-server\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952015b5",
   "metadata": {},
   "source": [
    "This setup creates a lightweight, portable container that includes our LLM, the inference server, and all necessary dependencies.\n",
    "\n",
    "### Direct Server Deployment\n",
    "\n",
    "While containerization offers many advantages, there are scenarios where direct deployment on a server might be preferred, such as when you need fine-grained control over the hosting environment or when working with legacy systems.\n",
    "\n",
    "### Implementing Direct Server Deployment\n",
    "\n",
    "For direct deployment, we'll use FastAPI to create a robust, high-performance inference server. Here's an enhanced version of our server code that includes logging, error handling, and a simple caching mechanism:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa77d207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import onnxruntime\n",
    "from transformers import AutoTokenizer\n",
    "from functools import lru_cache\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Load the ONNX model\n",
    "session = onnxruntime.InferenceSession(\"model/gpt2_model.onnx\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "class GenerationRequest(BaseModel):\n",
    "    prompt: str\n",
    "    max_length: int = 50\n",
    "\n",
    "@lru_cache(maxsize=100)\n",
    "def generate_cached(prompt: str, max_length: int):\n",
    "    try:\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors=\"np\")\n",
    "        output = session.run(None, {\"input_ids\": input_ids})\n",
    "        return tokenizer.decode(output[0][0][:max_length], skip_special_tokens=True)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating text: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Internal server error\")\n",
    "\n",
    "@app.post(\"/generate/\")\n",
    "async def generate_text(request: GenerationRequest):\n",
    "    logger.info(f\"Received generation request: {request.prompt[:50]}...\")\n",
    "    generated_text = generate_cached(request.prompt, request.max_length)\n",
    "    return {\"generated_text\": generated_text}\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    return {\"status\": \"healthy\"}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed1935b",
   "metadata": {},
   "source": [
    "To deploy this server:\n",
    "\n",
    "1. Install the required packages:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cf381f",
   "metadata": {},
   "source": [
    "```\n",
    "pip install fastapi uvicorn onnxruntime transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fdb050",
   "metadata": {},
   "source": [
    "2. Run the server:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd72e212",
   "metadata": {},
   "source": [
    "```\n",
    "python server.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecf3ec7",
   "metadata": {},
   "source": [
    "This setup provides a robust inference server with error handling, logging, and basic caching to improve performance for repeated requests.\n",
    "\n",
    "### Deployment Considerations\n",
    "\n",
    "When deciding between containerized and direct deployment, consider the following factors:\n",
    "\n",
    "1. **Scalability Requirements**: If you need to scale your application quickly and efficiently, containerization often provides an edge.\n",
    "\n",
    "2. **Infrastructure**: Consider your existing infrastructure. If you're already using container orchestration platforms like Kubernetes, Docker deployment might be more suitable.\n",
    "\n",
    "3. **Performance**: While containers add a slight overhead, the difference is usually negligible for LLM inference. However, if you need bare-metal performance, direct deployment might be preferable.\n",
    "\n",
    "4. **Deployment Frequency**: If you update your model frequently, containers can simplify the deployment process.\n",
    "\n",
    "5. **Team Expertise**: Consider your team's familiarity with containerization technologies.\n",
    "\n",
    "### Monitoring and Maintenance\n",
    "\n",
    "Regardless of the deployment method, implementing robust monitoring is crucial. Consider using tools like Prometheus for metrics collection and Grafana for visualization. Key metrics to monitor include:\n",
    "\n",
    "- Inference latency\n",
    "- Request throughput\n",
    "- Error rates\n",
    "- Resource utilization (CPU, GPU, memory)\n",
    "\n",
    "Regular maintenance tasks should include:\n",
    "\n",
    "- Updating the model with new versions\n",
    "- Monitoring and optimizing resource usage\n",
    "- Reviewing and analyzing logs for potential issues or areas of improvement\n",
    "\n",
    "By carefully considering these deployment strategies and implementing proper monitoring and maintenance procedures, you can ensure that your LLM performs reliably and efficiently in a production environment.\n",
    "\n",
    "In the next section, we'll delve into the intricacies of building a scalable backend to support your deployed LLM, focusing on handling high traffic loads and ensuring system reliability.\n",
    "\n",
    "## Backend Development for LLM Applications\n",
    "\n",
    "Developing a robust backend is crucial for supporting LLM-based applications in production environments. This section will explore key aspects of backend development, including scalability, performance optimization, and handling real-time requests.\n",
    "\n",
    "### Designing a Scalable Architecture\n",
    "\n",
    "When designing a backend for LLM applications, scalability is paramount. The architecture should be able to handle varying loads and scale horizontally to accommodate growing user bases.\n",
    "\n",
    "### Microservices Architecture\n",
    "\n",
    "A microservices architecture can provide the flexibility and scalability needed for LLM applications. Let's design a basic microservices setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22e2d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# api_gateway.py\n",
    "from fastapi import FastAPI, HTTPException\n",
    "import httpx\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "MODEL_SERVICE_URL = \"http://model-service:8000\"\n",
    "CACHE_SERVICE_URL = \"http://cache-service:8001\"\n",
    "\n",
    "@app.post(\"/generate/\")\n",
    "async def generate_text(request: dict):\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        # Check cache first\n",
    "        cache_response = await client.get(f\"{CACHE_SERVICE_URL}/get/{request['prompt']}\")\n",
    "        if cache_response.status_code == 200:\n",
    "            return cache_response.json()\n",
    "        \n",
    "        # If not in cache, call model service\n",
    "        model_response = await client.post(f\"{MODEL_SERVICE_URL}/generate/\", json=request)\n",
    "        if model_response.status_code != 200:\n",
    "            raise HTTPException(status_code=model_response.status_code, detail=\"Model service error\")\n",
    "        \n",
    "        # Store result in cache\n",
    "        await client.post(f\"{CACHE_SERVICE_URL}/set/\", json={\n",
    "            \"key\": request['prompt'],\n",
    "            \"value\": model_response.json()[\"generated_text\"]\n",
    "        })\n",
    "        \n",
    "        return model_response.json()\n",
    "\n",
    "# model_service.py\n",
    "# (Use the ONNX inference server from the previous section)\n",
    "\n",
    "# cache_service.py\n",
    "from fastapi import FastAPI\n",
    "import redis\n",
    "\n",
    "app = FastAPI()\n",
    "redis_client = redis.Redis(host='redis', port=6379, db=0)\n",
    "\n",
    "@app.get(\"/get/{key}\")\n",
    "async def get_cache(key: str):\n",
    "    value = redis_client.get(key)\n",
    "    if value:\n",
    "        return {\"value\": value.decode()}\n",
    "    return {\"value\": None}\n",
    "\n",
    "@app.post(\"/set/\")\n",
    "async def set_cache(item: dict):\n",
    "    redis_client.setex(item[\"key\"], 3600, item[\"value\"])  # Cache for 1 hour\n",
    "    return {\"status\": \"success\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846c3381",
   "metadata": {},
   "source": [
    "This setup separates concerns into three services:\n",
    "\n",
    "1. API Gateway: Handles incoming requests and orchestrates between services.\n",
    "2. Model Service: Manages LLM inference.\n",
    "3. Cache Service: Provides caching to reduce load on the model service.\n",
    "\n",
    "### Performance Optimization\n",
    "\n",
    "Optimizing backend performance is crucial for providing a responsive user experience.\n",
    "\n",
    "### Asynchronous Processing\n",
    "\n",
    "For long-running tasks like text generation, consider implementing asynchronous processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3106d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# api_gateway.py\n",
    "from fastapi import FastAPI, BackgroundTasks\n",
    "import httpx\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/generate_async/\")\n",
    "async def generate_text_async(request: dict, background_tasks: BackgroundTasks):\n",
    "    task_id = generate_unique_id()\n",
    "    background_tasks.add_task(process_generation, task_id, request)\n",
    "    return {\"task_id\": task_id}\n",
    "\n",
    "@app.get(\"/result/{task_id}\")\n",
    "async def get_result(task_id: str):\n",
    "    # Check result status and return if available\n",
    "    pass\n",
    "\n",
    "async def process_generation(task_id: str, request: dict):\n",
    "    # Perform generation and store result\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865b2e51",
   "metadata": {},
   "source": [
    "This approach allows the server to handle more concurrent requests by offloading the generation task to the background.\n",
    "\n",
    "### Load Balancing\n",
    "\n",
    "Implement load balancing to distribute requests across multiple instances of your model service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77bcdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nginx.conf\n",
    "http {\n",
    "    upstream model_servers {\n",
    "        server model_service_1:8000;\n",
    "        server model_service_2:8000;\n",
    "        server model_service_3:8000;\n",
    "    }\n",
    "\n",
    "    server {\n",
    "        listen 80;\n",
    "\n",
    "        location /generate/ {\n",
    "            proxy_pass http://model_servers;\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251da7b3",
   "metadata": {},
   "source": [
    "This NGINX configuration distributes requests across three model service instances.\n",
    "\n",
    "### Handling Real-time Requests\n",
    "\n",
    "For applications requiring real-time interaction, consider implementing WebSocket support:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bf84d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# websocket_server.py\n",
    "from fastapi import FastAPI, WebSocket\n",
    "from fastapi.websockets import WebSocketDisconnect\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class ConnectionManager:\n",
    "    def __init__(self):\n",
    "        self.active_connections = []\n",
    "\n",
    "    async def connect(self, websocket: WebSocket):\n",
    "        await websocket.accept()\n",
    "        self.active_connections.append(websocket)\n",
    "\n",
    "    def disconnect(self, websocket: WebSocket):\n",
    "        self.active_connections.remove(websocket)\n",
    "\n",
    "    async def send_message(self, message: str, websocket: WebSocket):\n",
    "        await websocket.send_text(message)\n",
    "\n",
    "manager = ConnectionManager()\n",
    "\n",
    "@app.websocket(\"/ws/{client_id}\")\n",
    "async def websocket_endpoint(websocket: WebSocket, client_id: int):\n",
    "    await manager.connect(websocket)\n",
    "    try:\n",
    "        while True:\n",
    "            data = await websocket.receive_text()\n",
    "            # Process the received data (e.g., generate text)\n",
    "            response = await generate_text(data)\n",
    "            await manager.send_message(response, websocket)\n",
    "    except WebSocketDisconnect:\n",
    "        manager.disconnect(websocket)\n",
    "\n",
    "async def generate_text(prompt: str):\n",
    "    # Implement text generation logic here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0838d8",
   "metadata": {},
   "source": [
    "This WebSocket server allows for real-time, bidirectional communication between the client and server, which can be particularly useful for interactive AI applications.\n",
    "\n",
    "### Error Handling and Logging\n",
    "\n",
    "Implementing comprehensive error handling and logging is crucial for maintaining and debugging your backend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c5152a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi.exceptions import RequestValidationError\n",
    "from fastapi.responses import JSONResponse\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@app.exception_handler(RequestValidationError)\n",
    "async def validation_exception_handler(request, exc):\n",
    "    logger.error(f\"Validation error: {str(exc)}\")\n",
    "    return JSONResponse(\n",
    "        status_code=400,\n",
    "        content={\"message\": \"Invalid request\", \"details\": str(exc)}\n",
    "    )\n",
    "\n",
    "@app.exception_handler(HTTPException)\n",
    "async def http_exception_handler(request, exc):\n",
    "    logger.error(f\"HTTP error {exc.status_code}: {exc.detail}\")\n",
    "    return JSONResponse(\n",
    "        status_code=exc.status_code,\n",
    "        content={\"message\": exc.detail}\n",
    "    )\n",
    "\n",
    "@app.exception_handler(Exception)\n",
    "async def general_exception_handler(request, exc):\n",
    "    logger.error(f\"Unexpected error: {str(exc)}\", exc_info=True)\n",
    "    return JSONResponse(\n",
    "        status_code=500,\n",
    "        content={\"message\": \"An unexpected error occurred\"}\n",
    "    )\n",
    "\n",
    "# Your route handlers go here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee50c9ea",
   "metadata": {},
   "source": [
    "This setup provides structured error responses and logs detailed error information, facilitating easier debugging and maintenance.\n",
    "\n",
    "### Security Considerations\n",
    "\n",
    "When developing backends for LLM applications, security should be a top priority:\n",
    "\n",
    "1. **Input Validation**: Carefully validate and sanitize all user inputs to prevent injection attacks.\n",
    "2. **Rate Limiting**: Implement rate limiting to prevent abuse and ensure fair usage.\n",
    "3. **Authentication and Authorization**: Use robust authentication mechanisms and implement proper authorization checks.\n",
    "4. **HTTPS**: Always use HTTPS in production to encrypt data in transit.\n",
    "5. **API Keys**: For public APIs, implement API key authentication and management.\n",
    "\n",
    "Here's a basic example of implementing rate limiting and API key authentication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0428e507",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, Depends, HTTPException\n",
    "from fastapi.security import APIKeyHeader\n",
    "from slowapi import Limiter, _rate_limit_exceeded_handler\n",
    "from slowapi.util import get_remote_address\n",
    "from slowapi.errors import RateLimitExceeded\n",
    "\n",
    "app = FastAPI()\n",
    "limiter = Limiter(key_func=get_remote_address)\n",
    "app.state.limiter = limiter\n",
    "app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)\n",
    "\n",
    "API_KEY_NAME = \"X-API-Key\"\n",
    "API_KEY = \"your-secret-api-key\"\n",
    "api_key_header = APIKeyHeader(name=API_KEY_NAME, auto_error=False)\n",
    "\n",
    "async def get_api_key(api_key_header: str = Depends(api_key_header)):\n",
    "    if api_key_header == API_KEY:\n",
    "        return api_key_header\n",
    "    raise HTTPException(status_code=403, detail=\"Invalid API Key\")\n",
    "\n",
    "@app.post(\"/generate/\")\n",
    "@limiter.limit(\"5/minute\")\n",
    "async def generate_text(request: dict, api_key: str = Depends(get_api_key)):\n",
    "    # Your text generation logic here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d9dd14",
   "metadata": {},
   "source": [
    "This example implements a rate limit of 5 requests per minute and requires a valid API key for access.\n",
    "\n",
    "By implementing these backend development strategies, you can create a robust, scalable, and secure infrastructure for your LLM-based application. Remember that backend development is an iterative process, and you should continuously monitor, test, and optimize your system based on real-world usage patterns and requirements.\n",
    "\n",
    "## Summary and Key Takeaways\n",
    "\n",
    "Throughout this comprehensive lesson on LLM Model Deployment and Backend Development, we've covered a wide range of crucial topics. Let's recap the key points:\n",
    "\n",
    "1. **Model Conversion and Optimization**\n",
    "   - Converting models to standardized formats like ONNX ensures compatibility across platforms.\n",
    "   - Optimization techniques such as TensorRT can significantly improve inference performance.\n",
    "   - Benchmarking is essential to validate the effectiveness of optimization efforts.\n",
    "\n",
    "2. **Deployment Strategies**\n",
    "   - Containerization with Docker offers consistency and scalability.\n",
    "   - Direct server deployment provides fine-grained control over the hosting environment.\n",
    "   - The choice between strategies depends on factors like scalability requirements and team expertise.\n",
    "\n",
    "3. **Backend Development**\n",
    "   - A microservices architecture can enhance flexibility and scalability.\n",
    "   - Performance optimization techniques include asynchronous processing and load balancing.\n",
    "   - Real-time interactions can be facilitated through WebSocket implementations.\n",
    "   - Robust error handling and logging are crucial for maintaining and debugging the system.\n",
    "   - Security considerations, including input validation, rate limiting, and authentication, are paramount.\n",
    "\n",
    "The successful deployment and operation of LLM-based applications require a holistic approach that considers all these aspects. As the field of AI continues to evolve, staying updated with the latest best practices and technologies will be crucial for building efficient, scalable, and secure LLM applications.\n",
    "\n",
    "## Hands-on Exercise: Building a Scalable LLM-powered Chat Application\n",
    "\n",
    "To solidify your understanding of the concepts covered in this lesson, let's work through a practical exercise. You'll build a simple but scalable chat application powered by an LLM.\n",
    "\n",
    "### Exercise Objectives\n",
    "\n",
    "1. Deploy an optimized LLM using ONNX runtime\n",
    "2. Implement a FastAPI backend with WebSocket support\n",
    "3. Create a basic frontend for user interaction\n",
    "4. Implement rate limiting and basic authentication\n",
    "\n",
    "### Step 1: Optimize and Deploy the Model\n",
    "\n",
    "First, optimize the model using ONNX (assuming you have a trained model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43c4800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Export the model to ONNX\n",
    "dummy_input = tokenizer(\"Hello, world!\", return_tensors=\"pt\").input_ids\n",
    "torch.onnx.export(model, dummy_input, \"gpt2_model.onnx\", \n",
    "                  input_names=['input_ids'], \n",
    "                  output_names=['output'], \n",
    "                  dynamic_axes={'input_ids': {0: 'batch_size', 1: 'sequence'}, \n",
    "                                'output': {0: 'batch_size', 1: 'sequence'}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4751879b",
   "metadata": {},
   "source": [
    "### Step 2: Implement the Backend\n",
    "\n",
    "Create a FastAPI application with WebSocket support and ONNX runtime inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27e3391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import onnxruntime\n",
    "from fastapi import FastAPI, WebSocket, Depends, HTTPException\n",
    "from fastapi.security import APIKeyHeader\n",
    "from slowapi import Limiter, _rate_limit_exceeded_handler\n",
    "from slowapi.util import get_remote_address\n",
    "from slowapi.errors import RateLimitExceeded\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "app = FastAPI()\n",
    "limiter = Limiter(key_func=get_remote_address)\n",
    "app.state.limiter = limiter\n",
    "app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)\n",
    "\n",
    "# Load the ONNX model\n",
    "session = onnxruntime.InferenceSession(\"gpt2_model.onnx\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Set up API key authentication\n",
    "API_KEY = \"your-secret-api-key\"\n",
    "api_key_header = APIKeyHeader(name=\"X-API-Key\", auto_error=False)\n",
    "\n",
    "async def get_api_key(api_key_header: str = Depends(api_key_header)):\n",
    "    if api_key_header == API_KEY:\n",
    "        return api_key_header\n",
    "    raise HTTPException(status_code=403, detail=\"Invalid API Key\")\n",
    "\n",
    "@app.websocket(\"/chat\")\n",
    "async def websocket_endpoint(websocket: WebSocket):\n",
    "    await websocket.accept()\n",
    "    try:\n",
    "        while True:\n",
    "            data = await websocket.receive_text()\n",
    "            response = generate_text(data)\n",
    "            await websocket.send_text(response)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        await websocket.close()\n",
    "\n",
    "@limiter.limit(\"5/minute\")\n",
    "def generate_text(prompt: str):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"np\")\n",
    "    output = session.run(None, {\"input_ids\": input_ids})\n",
    "    return tokenizer.decode(output[0][0], skip_special_tokens=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ad66f2",
   "metadata": {},
   "source": [
    "### Step 3: Create a Simple Frontend\n",
    "\n",
    "Create an `index.html` file with basic chat functionality:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c12b258",
   "metadata": {},
   "source": [
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>LLM Chat</title>\n",
    "    <script>\n",
    "        let socket = new WebSocket(\"ws://localhost:8000/chat\");\n",
    "        \n",
    "        socket.onmessage = function(event) {\n",
    "            let chatBox = document.getElementById(\"chat-box\");\n",
    "            chatBox.innerHTML += `<p><strong>AI:</strong> ${event.data}</p>`;\n",
    "        };\n",
    "\n",
    "        function sendMessage() {\n",
    "            let messageInput = document.getElementById(\"message-input\");\n",
    "            let message = messageInput.value;\n",
    "            let chatBox = document.getElementById(\"chat-box\");\n",
    "            \n",
    "            chatBox.innerHTML += `<p><strong>You:</strong> ${message}</p>`;\n",
    "            socket.send(message);\n",
    "            messageInput.value = \"\";\n",
    "        }\n",
    "    </script>\n",
    "</head>\n",
    "<body>\n",
    "    <div id=\"chat-box\" style=\"height: 300px; overflow-y: scroll; border: 1px solid #ccc; padding: 10px;\"></div>\n",
    "    <input type=\"text\" id=\"message-input\" placeholder=\"Type your message...\">\n",
    "    <button onclick=\"sendMessage()\">Send</button>\n",
    "</body>\n",
    "</html>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb55fd80",
   "metadata": {},
   "source": [
    "### Step 4: Run and Test\n",
    "\n",
    "1. Start the backend server:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d3d545",
   "metadata": {},
   "source": [
    "```\n",
    "python backend.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e5d87a",
   "metadata": {},
   "source": [
    "2. Open the `index.html` file in a web browser.\n",
    "\n",
    "3. Start chatting with the LLM!\n",
    "\n",
    "### Extension Ideas\n",
    "\n",
    "- Implement proper error handling and logging in the backend.\n",
    "- Add user authentication to the chat application.\n",
    "- Implement a caching mechanism to store and retrieve common responses.\n",
    "- Explore deploying the application using Docker for easier scaling.\n",
    "\n",
    "By completing this exercise, you'll have hands-on experience with deploying an optimized LLM, creating a scalable backend, and building a basic frontend for user interaction. This practical application reinforces the concepts covered throughout the lesson and provides a foundation for more complex LLM-powered applications.\n",
    "\n",
    "Remember, building production-ready AI applications requires continuous learning, testing, and refinement. Keep exploring new techniques and best practices to enhance your skills in LLM deployment and backend development.\n",
    "\n",
    "## Advanced Topics and Future Trends in LLM Deployment and Backend Development\n",
    "\n",
    "As the field of AI and specifically Large Language Models continues to evolve rapidly, it's crucial to stay informed about advanced topics and emerging trends. This section explores cutting-edge developments and future directions in LLM deployment and backend development.\n",
    "\n",
    "### Federated Learning for LLMs\n",
    "\n",
    "Federated Learning allows for training models across decentralized devices or servers holding local data samples, without exchanging them. This approach addresses privacy concerns and enables personalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1deffb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_federated as tff\n",
    "\n",
    "# Define a simple model\n",
    "def create_keras_model():\n",
    "    return tf.keras.models.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=(784,)),\n",
    "        tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "    ])\n",
    "\n",
    "# Wrap the model for federated learning\n",
    "def model_fn():\n",
    "    keras_model = create_keras_model()\n",
    "    return tff.learning.from_keras_model(\n",
    "        keras_model,\n",
    "        input_spec=preprocessed_example_dataset.element_spec,\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "    )\n",
    "\n",
    "# Create a federated learning process\n",
    "iterative_process = tff.learning.build_federated_averaging_process(\n",
    "    model_fn,\n",
    "    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02),\n",
    "    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0)\n",
    ")\n",
    "\n",
    "# Run federated learning (simplified)\n",
    "state = iterative_process.initialize()\n",
    "for round_num in range(5):\n",
    "    state, metrics = iterative_process.next(state, federated_train_data)\n",
    "    print('round {:2d}, metrics={}'.format(round_num, metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d5cdc9",
   "metadata": {},
   "source": [
    "This example demonstrates a basic setup for federated learning, which could be adapted for LLMs to enable privacy-preserving, distributed model improvements.\n",
    "\n",
    "### Efficient Fine-tuning Techniques\n",
    "\n",
    "As LLMs grow larger, efficient fine-tuning becomes crucial. Techniques like Parameter-Efficient Fine-Tuning (PEFT) are gaining traction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecfab3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "print(f\"Trainable params: {peft_model.print_trainable_parameters()}\")\n",
    "\n",
    "# Fine-tuning code would follow..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bfa8c7",
   "metadata": {},
   "source": [
    "This example sets up a model for fine-tuning using the LoRA (Low-Rank Adaptation) technique, which significantly reduces the number of trainable parameters.\n",
    "\n",
    "### Multimodal LLMs\n",
    "\n",
    "Future LLM applications will increasingly integrate multiple modalities, such as text, images, and audio. Backend systems will need to handle these diverse data types efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e55044",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "max_length = 16\n",
    "num_beams = 4\n",
    "gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}\n",
    "\n",
    "def predict_step(image_paths):\n",
    "    images = []\n",
    "    for image_path in image_paths:\n",
    "        i_image = Image.open(image_path)\n",
    "        if i_image.mode != \"RGB\":\n",
    "            i_image = i_image.convert(mode=\"RGB\")\n",
    "        images.append(i_image)\n",
    "\n",
    "    pixel_values = feature_extractor(images=images, return_tensors=\"pt\").pixel_values\n",
    "    pixel_values = pixel_values.to(device)\n",
    "\n",
    "    output_ids = model.generate(pixel_values, **gen_kwargs)\n",
    "\n",
    "    preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    return preds\n",
    "\n",
    "# Example usage\n",
    "image_paths = [\"path/to/image1.jpg\", \"path/to/image2.jpg\"]\n",
    "predictions = predict_step(image_paths)\n",
    "for pred in predictions:\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f4df7f",
   "metadata": {},
   "source": [
    "This example demonstrates a multimodal model that generates text captions for images, showcasing how future backends might need to handle diverse input types.\n",
    "\n",
    "### Continuous Learning and Model Updates\n",
    "\n",
    "Future LLM systems will likely implement continuous learning capabilities, allowing models to update themselves based on new data and interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e9d11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import serve\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "@serve.deployment\n",
    "class ContinuousLearningLLM:\n",
    "    def __init__(self):\n",
    "        self.model_name = \"gpt2\"\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(self.model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-5)\n",
    "\n",
    "    def generate(self, prompt):\n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "        output = self.model.generate(input_ids, max_length=50)\n",
    "        return self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    def learn(self, prompt, target):\n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "        target_ids = self.tokenizer.encode(target, return_tensors=\"pt\")\n",
    "        \n",
    "        outputs = self.model(input_ids, labels=target_ids)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "    async def __call__(self, request):\n",
    "        if request.method == \"GET\":\n",
    "            prompt = request.query_params[\"prompt\"]\n",
    "            return self.generate(prompt)\n",
    "        elif request.method == \"POST\":\n",
    "            data = await request.json()\n",
    "            loss = self.learn(data[\"prompt\"], data[\"target\"])\n",
    "            return {\"loss\": loss}\n",
    "\n",
    "deployment = ContinuousLearningLLM.bind()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91760769",
   "metadata": {},
   "source": [
    "This example outlines a basic structure for a continuously learning LLM using Ray Serve, allowing for both inference and on-the-fly learning through API calls.\n",
    "\n",
    "### Ethical AI and Responsible Deployment\n",
    "\n",
    "As LLMs become more prevalent, ethical considerations and responsible deployment practices will be increasingly important. Future backend systems will need to incorporate robust fairness, accountability, transparency, and ethics (FATE) measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d531dfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "def ethical_content_filter(text):\n",
    "    # Toxicity check\n",
    "    classifier = pipeline(\"text-classification\", model=\"unitary/toxic-bert\")\n",
    "    result = classifier(text)[0]\n",
    "    if result['label'] == 'toxic' and result['score'] > 0.7:\n",
    "        return False, \"Content flagged as potentially toxic.\"\n",
    "\n",
    "    # Sentiment analysis\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment = sia.polarity_scores(text)\n",
    "    if sentiment['compound'] < -0.5:\n",
    "        return False, \"Content flagged for extremely negative sentiment.\"\n",
    "\n",
    "    # Add more ethical checks as needed...\n",
    "\n",
    "    return True, \"Content passed ethical checks.\"\n",
    "\n",
    "# Example usage in the backend\n",
    "@app.post(\"/generate/\")\n",
    "async def generate_text(request: dict):\n",
    "    generated_text = llm_generate(request['prompt'])  # Your LLM generation function\n",
    "    is_ethical, message = ethical_content_filter(generated_text)\n",
    "    if not is_ethical:\n",
    "        return {\"error\": message}\n",
    "    return {\"generated_text\": generated_text}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0bb762",
   "metadata": {},
   "source": [
    "This example demonstrates a basic ethical filter that could be integrated into an LLM backend to ensure responsible content generation.\n",
    "\n",
    "### Conclusion and Future Outlook\n",
    "\n",
    "The field of LLM deployment and backend development is rapidly evolving. Key areas to watch include:\n",
    "\n",
    "1. More efficient model architectures and training techniques\n",
    "2. Advanced deployment strategies for edge and mobile devices\n",
    "3. Improved integration of LLMs with other AI systems and traditional software\n",
    "4. Enhanced privacy-preserving techniques for model training and inference\n",
    "5. Development of industry-specific LLMs and deployment patterns\n",
    "6. Standardization of ethical AI practices in LLM deployments\n",
    "\n",
    "As an AI engineer or researcher, staying informed about these trends and continuously experimenting with new techniques will be crucial for building the next generation of AI-powered applications.\n",
    "\n",
    "By exploring these advanced topics and future trends, you're better equipped to anticipate and prepare for the evolving landscape of LLM deployment and backend development. Remember, the field of AI is dynamic, and continuous learning is key to staying at the forefront of these exciting developments."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
