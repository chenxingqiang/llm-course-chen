{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97d41b9e",
   "metadata": {},
   "source": [
    "# 1.Course Title: Comprehensive NLP Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93a3749",
   "metadata": {},
   "source": [
    "![Mermaid diagram](lesson_02_mermaid_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d6b523",
   "metadata": {},
   "source": [
    "Advanced Natural Language Processing: From Fundamentals to State-of-the-Art\n",
    "\n",
    "## 2. Learning Objectives\n",
    "\n",
    "By the end of this comprehensive lesson, students will be able to:\n",
    "\n",
    "- 2.1 Thoroughly understand and explain the importance of NLP in modern AI applications\n",
    "- 2.2 Recognize and categorize the main types of NLP tasks, with the ability to provide relevant examples\n",
    "- 2.3 Comprehend the evolution of NLP algorithms and models, from traditional approaches to neural networks\n",
    "- 2.4 Gain deep insights into state-of-the-art NLP models and their applications\n",
    "- 2.5 Implement basic NLP tasks using popular libraries and frameworks\n",
    "- 2.6 Critically evaluate the strengths and limitations of different NLP approaches\n",
    "\n",
    "## 3. Overview\n",
    "\n",
    "This in-depth lesson covers four key concepts, providing a comprehensive exploration of Natural Language Processing (NLP) fundamentals:\n",
    "\n",
    "- 3.1 The Importance and Wide-ranging Applications of NLP\n",
    "- 3.2 Comprehensive Overview of NLP Tasks and Their Real-world Applications\n",
    "- 3.3 The Evolution of NLP: From Rule-based Systems to Transformers\n",
    "- 3.4 State-of-the-Art NLP Models and Their Groundbreaking Capabilities\n",
    "\n",
    "## 4. Detailed Content\n",
    "\n",
    "### 4.1 Concept 1: The Importance and Wide-ranging Applications of NLP\n",
    "\n",
    "#### 4.1.1 Explanation\n",
    "Natural Language Processing (NLP) is a crucial subfield of artificial intelligence that focuses on the interaction between computers and humans using natural language. Its importance stems from several key factors:\n",
    "\n",
    "1. Improving Human-Computer Interaction: NLP enables more natural and intuitive interfaces.\n",
    "2. Information Extraction: It allows machines to extract meaningful information from unstructured text.\n",
    "3. Automation of Language-related Tasks: NLP can automate translation, summarization, and other language tasks.\n",
    "4. Enhancing Accessibility: It helps in creating tools for people with disabilities.\n",
    "5. Data-driven Decision Making: NLP can analyze large volumes of text data to provide insights.\n",
    "\n",
    "The applications of NLP are vast and growing, including:\n",
    "- Virtual Assistants (e.g., Siri, Alexa)\n",
    "- Machine Translation (e.g., Google Translate)\n",
    "- Sentiment Analysis for Social Media Monitoring\n",
    "- Chatbots for Customer Service\n",
    "- Content Recommendation Systems\n",
    "- Automatic Text Summarization\n",
    "- Speech Recognition Systems\n",
    "\n",
    "#### 4.1.2 Case Study: NLP in Healthcare\n",
    "Let's consider the application of NLP in healthcare. Electronic Health Records (EHRs) contain vast amounts of unstructured text data. NLP can help in:\n",
    "\n",
    "1. Extracting relevant medical information from clinical notes\n",
    "2. Identifying potential drug interactions\n",
    "3. Assisting in clinical decision support systems\n",
    "4. Automating medical coding for billing purposes\n",
    "\n",
    "For example, a study by Liao et al. (2015) showed that NLP could identify patients with heart failure from clinical notes with high accuracy, potentially improving early diagnosis and treatment.\n",
    "\n",
    "#### 4.1.3 Code: Advanced Sentiment Analysis with VADER\n",
    "\n",
    "Let's implement a more sophisticated sentiment analysis using VADER (Valence Aware Dictionary and sEntiment Reasoner), which is particularly attuned to sentiments expressed in social media:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4808613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    return sia.polarity_scores(text)\n",
    "\n",
    "texts = [\n",
    "    \"I absolutely love this NLP course! It's incredibly informative.\",\n",
    "    \"The content is okay, but I find some parts confusing.\",\n",
    "    \"This is the worst course I've ever taken. Total waste of time.\",\n",
    "    \"While the course has some interesting points, it could be better organized.\",\n",
    "    \"I'm amazed by how much I've learned about NLP in such a short time!\"\n",
    "]\n",
    "\n",
    "sentiments = [analyze_sentiment(text) for text in texts]\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = range(len(texts))\n",
    "ax.bar([i-0.2 for i in x], [s['pos'] for s in sentiments], width=0.2, align='center', label='Positive', color='green')\n",
    "ax.bar([i for i in x], [s['neu'] for s in sentiments], width=0.2, align='center', label='Neutral', color='gray')\n",
    "ax.bar([i+0.2 for i in x], [s['neg'] for s in sentiments], width=0.2, align='center', label='Negative', color='red')\n",
    "ax.set_ylabel('Sentiment Score')\n",
    "ax.set_title('Sentiment Analysis of Course Feedback')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f'Text {i+1}' for i in x], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "for i, (text, sentiment) in enumerate(zip(texts, sentiments)):\n",
    "    print(f\"Text {i+1}: {text}\")\n",
    "    print(f\"Sentiment: {sentiment}\")\n",
    "    print(f\"Overall: {'Positive' if sentiment['compound'] > 0 else 'Negative' if sentiment['compound'] < 0 else 'Neutral'}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927cd27b",
   "metadata": {},
   "source": [
    "This code performs sentiment analysis on multiple pieces of text feedback about a course, visualizing the results and providing a more nuanced understanding of sentiment.\n",
    "\n",
    "#### 4.1.4 Reflection\n",
    "\n",
    "The wide-ranging applications of NLP demonstrate its crucial role in modern AI. From improving user experiences to enabling data-driven decision making in complex fields like healthcare, NLP is transforming how we interact with and extract value from textual data. However, it's important to consider the ethical implications, such as privacy concerns in healthcare applications or potential biases in sentiment analysis systems.\n",
    "\n",
    "### 4.2 Concept 2: Comprehensive Overview of NLP Tasks and Their Real-world Applications\n",
    "\n",
    "#### 4.2.1 Explanation\n",
    "\n",
    "NLP encompasses a wide range of tasks, which can be broadly categorized into three main types:\n",
    "\n",
    "1. Classification Tasks: Assigning predefined categories to text.\n",
    "   - Examples: Sentiment Analysis, Topic Classification, Spam Detection\n",
    "\n",
    "2. Extraction Tasks: Identifying and extracting specific information from text.\n",
    "   - Examples: Named Entity Recognition (NER), Keyword Extraction, Relationship Extraction\n",
    "\n",
    "3. Generation Tasks: Creating human-like text based on input.\n",
    "   - Examples: Machine Translation, Text Summarization, Question Answering\n",
    "\n",
    "Each of these task types serves different purposes in processing and understanding natural language, often working together in complex NLP systems.\n",
    "\n",
    "#### 4.2.2 Case Study: Multilingual Customer Support System\n",
    "\n",
    "Imagine developing a multilingual customer support system for a global e-commerce platform. This system would incorporate multiple NLP tasks:\n",
    "\n",
    "1. Language Detection (Classification): Identify the language of incoming customer queries.\n",
    "2. Intent Classification (Classification): Determine the purpose of the customer's message (e.g., return request, product inquiry).\n",
    "3. Named Entity Recognition (Extraction): Identify product names, order numbers, or customer names in the query.\n",
    "4. Sentiment Analysis (Classification): Gauge the customer's mood from their message.\n",
    "5. Machine Translation (Generation): Translate the customer's message if it's not in the support agent's language.\n",
    "6. Response Generation (Generation): Automatically generate appropriate responses based on the query's intent and extracted information.\n",
    "\n",
    "This case study demonstrates how different NLP tasks can be combined to create a sophisticated, real-world application.\n",
    "\n",
    "#### 4.2.3 Code: Multi-task NLP Pipeline\n",
    "\n",
    "Let's implement a simplified version of the multilingual customer support system described above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e7c208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from transformers import pipeline\n",
    "\n",
    "# Ensure necessary NLTK data is downloaded\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Initialize models\n",
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-ROMANCE\")\n",
    "\n",
    "def process_query(query):\n",
    "    # Language Detection\n",
    "    lang = detect(query)\n",
    "    \n",
    "    # Translate to English if not already in English\n",
    "    if lang != 'en':\n",
    "        translation = translator(query, max_length=512)[0]['translation_text']\n",
    "        print(f\"Translated query: {translation}\")\n",
    "        query = translation\n",
    "    \n",
    "    # Tokenization and NER\n",
    "    tokens = word_tokenize(query)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    named_entities = ne_chunk(pos_tags)\n",
    "    \n",
    "    # Extract product names (simplified)\n",
    "    products = [word for word, pos in pos_tags if pos == 'NN']\n",
    "    \n",
    "    # Sentiment Analysis\n",
    "    sentiment = sentiment_analyzer.polarity_scores(query)\n",
    "    \n",
    "    # Intent Classification (simplified)\n",
    "    intent = \"product_inquiry\" if \"how\" in query.lower() or \"what\" in query.lower() else \"general_query\"\n",
    "    \n",
    "    return {\n",
    "        \"original_language\": lang,\n",
    "        \"named_entities\": named_entities,\n",
    "        \"products\": products,\n",
    "        \"sentiment\": sentiment,\n",
    "        \"intent\": intent\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "queries = [\n",
    "    \"How do I return the faulty smartphone I bought last week?\",\n",
    "    \"J'adore votre nouveau produit! Quand sera-t-il disponible en France ?\",\n",
    "    \"This laptop is terrible. I want a refund immediately!\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\nProcessing query: {query}\")\n",
    "    result = process_query(query)\n",
    "    print(f\"Original Language: {result['original_language']}\")\n",
    "    print(f\"Named Entities: {result['named_entities']}\")\n",
    "    print(f\"Products mentioned: {result['products']}\")\n",
    "    print(f\"Sentiment: {result['sentiment']}\")\n",
    "    print(f\"Detected Intent: {result['intent']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7ec7dc",
   "metadata": {},
   "source": [
    "This code demonstrates a multi-task NLP pipeline that processes customer queries, handling language detection, translation, named entity recognition, sentiment analysis, and a simplified form of intent classification.\n",
    "\n",
    "#### 4.2.4 Reflection\n",
    "\n",
    "The diversity of NLP tasks allows for the creation of sophisticated systems that can understand and generate human language in nuanced ways. However, each task comes with its own challenges:\n",
    "\n",
    "1. Classification tasks may struggle with context and nuance.\n",
    "2. Extraction tasks can be sensitive to variations in text structure and vocabulary.\n",
    "3. Generation tasks might produce fluent but inaccurate or biased text.\n",
    "\n",
    "Understanding these challenges is crucial for developing robust NLP systems and choosing the right approaches for specific applications.\n",
    "\n",
    "### 4.3 Concept 3: The Evolution of NLP: From Rule-based Systems to Transformers\n",
    "\n",
    "#### 4.3.1 Explanation\n",
    "\n",
    "The field of NLP has seen significant advancements over the years, evolving from simple rule-based systems to sophisticated neural network-based approaches. This evolution can be broadly categorized into several stages:\n",
    "\n",
    "1. Rule-based Systems (1950s-1980s): Hand-crafted rules for language processing.\n",
    "2. Statistical Methods (1980s-2000s): Probabilistic models like n-grams and Hidden Markov Models.\n",
    "3. Machine Learning Approaches (2000s-2010s): Including Naive Bayes, SVMs, and Decision Trees.\n",
    "4. Deep Learning Revolution (2010s-present): Neural networks, particularly sequence models like RNNs and LSTMs.\n",
    "5. Transformer Era (2017-present): Attention-based models revolutionizing NLP tasks.\n",
    "\n",
    "Each stage brought significant improvements in NLP capabilities, with the most recent advancements enabling human-like language understanding and generation.\n",
    "\n",
    "#### 4.3.2 Case Study: Evolution of Machine Translation\n",
    "\n",
    "Let's trace the evolution of machine translation as an example:\n",
    "\n",
    "1. Rule-based MT (e.g., Systran, 1970s): Used hand-crafted linguistic rules.\n",
    "   - Pros: Worked well for closely related languages.\n",
    "   - Cons: Struggled with exceptions and idioms.\n",
    "\n",
    "2. Statistical MT (e.g., Google Translate, 2000s): Used statistical models trained on parallel corpora.\n",
    "   - Pros: Better handling of language variations.\n",
    "   - Cons: Required large parallel corpora, struggled with long-range dependencies.\n",
    "\n",
    "3. Neural MT (e.g., Google's GNMT, 2016): Used sequence-to-sequence models with attention.\n",
    "   - Pros: Significant improvement in fluency and accuracy.\n",
    "   - Cons: Computationally intensive, potential for hallucination.\n",
    "\n",
    "4. Transformer-based MT (e.g., DeepL, 2017): Used the transformer architecture.\n",
    "   - Pros: State-of-the-art performance, better handling of context.\n",
    "   - Cons: Requires large amounts of data and computational resources.\n",
    "\n",
    "#### 4.3.3 Code: Comparing Different Text Classification Approaches\n",
    "\n",
    "Let's implement and compare different text classification approaches to see the evolution in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f277a777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data\n",
    "texts = [\n",
    "    \"I love this movie\",\n",
    "    \"Great film, highly recommended\",\n",
    "    \"Terrible movie, waste of time\",\n",
    "    \"I hated every minute of it\",\n",
    "    \"Awesome acting and plot\",\n",
    "    \"Boring and predictable\",\n",
    "    \"Excellent cinematography\",\n",
    "    \"Poor direction and script\"\n",
    "]\n",
    "labels = [1, 1, 0, 0, 1, 0, 1, 0]  # 1 for positive, 0 for negative\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# 1. Naive Bayes with Bag of Words\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "X_test_bow = vectorizer.transform(X_test)\n",
    "\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train_bow, y_train)\n",
    "nb_predictions = nb_classifier.predict(X_test_bow)\n",
    "nb_accuracy = accuracy_score(y_test, nb_predictions)\n",
    "\n",
    "# 2. SVM with Bag of Words\n",
    "svm_classifier = SVC(kernel='linear')\n",
    "svm_classifier.fit(X_train_bow, y_train)\n",
    "svm_predictions = svm_classifier.predict(X_test_bow)\n",
    "svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
    "\n",
    "# 3. LSTM Neural Network\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "max_length = max([len(x) for x in X_train_seq + X_test_seq])\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_length)\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(1000, 16Embedding(1000, 16, input_length=max_length),\n",
    "    LSTM(32),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(X_train_pad, np.array(y_train), epochs=10, validation_split=0.2, verbose=0)\n",
    "\n",
    "lstm_predictions = (model.predict(X_test_pad) > 0.5).astype(int).flatten()\n",
    "lstm_accuracy = accuracy_score(y_test, lstm_predictions)\n",
    "\n",
    "# Plotting results\n",
    "models = ['Naive Bayes', 'SVM', 'LSTM']\n",
    "accuracies = [nb_accuracy, svm_accuracy, lstm_accuracy]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(models, accuracies)\n",
    "plt.title('Accuracy Comparison of Different Text Classification Models')\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "for i, v in enumerate(accuracies):\n",
    "    plt.text(i, v + 0.01, f'{v:.2f}', ha='center')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Naive Bayes Accuracy: {nb_accuracy:.2f}\")\n",
    "print(f\"SVM Accuracy: {svm_accuracy:.2f}\")\n",
    "print(f\"LSTM Accuracy: {lstm_accuracy:.2f}\")\n",
    "\n",
    "# Plot LSTM training history\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('LSTM Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca00926",
   "metadata": {},
   "source": [
    "This code demonstrates the evolution of text classification methods, from traditional machine learning approaches (Naive Bayes and SVM) to deep learning (LSTM). It visualizes the performance comparison and the training process of the LSTM model.\n",
    "\n",
    "#### 4.3.4 Reflection\n",
    "\n",
    "The evolution of NLP techniques has led to significant improvements in performance and capabilities:\n",
    "\n",
    "1. Rule-based systems were limited by the complexity of language rules and exceptions.\n",
    "2. Statistical methods improved handling of language variations but required large datasets.\n",
    "3. Machine learning approaches enhanced adaptability to different domains and tasks.\n",
    "4. Deep learning, especially transformers, revolutionized NLP with context-aware processing and transfer learning capabilities.\n",
    "\n",
    "However, this evolution also brings challenges:\n",
    "\n",
    "- Increased computational requirements\n",
    "- Need for larger datasets\n",
    "- Reduced interpretability in more complex models\n",
    "- Potential for amplifying biases present in training data\n",
    "\n",
    "Understanding this evolution helps in choosing the right approach for specific NLP tasks and understanding the trade-offs involved.\n",
    "\n",
    "### 4.4 Concept 4: State-of-the-Art NLP Models and Their Groundbreaking Capabilities\n",
    "\n",
    "#### 4.4.1 Explanation\n",
    "\n",
    "Modern NLP is dominated by large pre-trained models based on the Transformer architecture. These models have set new benchmarks in various NLP tasks and can be fine-tuned for specific applications. Key models include:\n",
    "\n",
    "1. BERT (Bidirectional Encoder Representations from Transformers)\n",
    "   - Introduced by Google in 2018\n",
    "   - Bidirectional context understanding\n",
    "   - Excels in tasks like question answering and sentiment analysis\n",
    "\n",
    "2. GPT (Generative Pre-trained Transformer) series\n",
    "   - Developed by OpenAI\n",
    "   - Powerful text generation capabilities\n",
    "   - GPT-3 (2020) showed impressive few-shot learning abilities\n",
    "\n",
    "3. T5 (Text-to-Text Transfer Transformer)\n",
    "   - Introduced by Google in 2019\n",
    "   - Unified framework for multiple NLP tasks\n",
    "   - Strong performance across various benchmarks\n",
    "\n",
    "4. XLNet\n",
    "   - Combines strengths of autoregressive and autoencoding models\n",
    "   - Overcomes limitations of BERT in certain tasks\n",
    "\n",
    "These models have dramatically expanded the possibilities of what can be achieved with NLP, from more accurate language understanding to human-like text generation.\n",
    "\n",
    "#### 4.4.2 Case Study: GPT-3 in Creative Writing\n",
    "\n",
    "GPT-3 has shown remarkable capabilities in creative writing tasks. For example, it has been used to:\n",
    "\n",
    "- Generate poetry that captures specific styles and themes\n",
    "- Write short stories with coherent plots\n",
    "- Create dialogues for characters with distinct personalities\n",
    "\n",
    "Gwern Branwen's experiments with GPT-3 for poetry and creative writing showcase its ability to understand and emulate various literary styles and forms.\n",
    "\n",
    "#### 4.4.3 Code: Fine-tuning BERT for Sentiment Analysis\n",
    "\n",
    "Let's implement a more advanced sentiment analysis by fine-tuning a pre-trained BERT model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284cb315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data (in a real scenario, you'd use a larger dataset)\n",
    "texts = [\n",
    "    \"I love this product! It's amazing.\",\n",
    "    \"Terrible experience, never buying again.\",\n",
    "    \"Neutral opinion, neither good nor bad.\",\n",
    "    \"Absolutely fantastic, exceeded my expectations!\",\n",
    "    \"Disappointing quality, not worth the price.\",\n",
    "    \"Okay product, does the job.\",\n",
    "    \"Mind-blowing performance, highly recommended!\",\n",
    "    \"Waste of money, don't buy it.\"\n",
    "]\n",
    "labels = [1, 0, 2, 1, 0, 2, 1, 0]  # 0: Negative, 1: Positive, 2: Neutral\n",
    "\n",
    "# Split data\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "\n",
    "# Tokenize and encode sequences\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=64)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=64)\n",
    "\n",
    "# Convert to PyTorch datasets\n",
    "train_dataset = TensorDataset(\n",
    "    torch.tensor(train_encodings['input_ids']),\n",
    "    torch.tensor(train_encodings['attention_mask']),\n",
    "    torch.tensor(train_labels)\n",
    ")\n",
    "val_dataset = TensorDataset(\n",
    "    torch.tensor(val_encodings['input_ids']),\n",
    "    torch.tensor(val_encodings['attention_mask']),\n",
    "    torch.tensor(val_labels)\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2)\n",
    "\n",
    "# Set up optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 5\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            total_val_loss += outputs.loss.item()\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_epochs+1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, num_epochs+1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Function to predict sentiment\n",
    "def predict_sentiment(text):\n",
    "    encoding = tokenizer(text, truncation=True, padding=True, max_length=64, return_tensors='pt')\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        prediction = torch.argmax(outputs.logits, dim=1).item()\n",
    "    \n",
    "    return ['Negative', 'Positive', 'Neutral'][prediction]\n",
    "\n",
    "# Test the model\n",
    "test_texts = [\n",
    "    \"This product is fantastic!\",\n",
    "    \"I'm very disappointed with the quality.\",\n",
    "    \"It's an average product, nothing special.\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    sentiment = predict_sentiment(text)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Predicted sentiment: {sentiment}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65719907",
   "metadata": {},
   "source": [
    "This code demonstrates how to fine-tune a pre-trained BERT model for sentiment analysis, showcasing the power of transfer learning in modern NLP.\n",
    "\n",
    "#### 4.4.4 Reflection\n",
    "\n",
    "State-of-the-art NLP models have dramatically expanded the possibilities of what can be achieved with natural language processing:\n",
    "\n",
    "1. They can understand context and nuance in language much better than previous models.\n",
    "2. Their transfer learning capabilities allow for impressive performance on various tasks with minimal fine-tuning.\n",
    "3. They can generate human-like text, opening up new applications in creative writing, content generation, and more.\n",
    "\n",
    "However, these models also raise important ethical considerations:\n",
    "\n",
    "- Potential for generating convincing misinformation\n",
    "- Privacy concerns related to the vast amount of data used in training\n",
    "- Environmental impact of training large models\n",
    "- Potential for amplifying biases present in training data\n",
    "\n",
    "As NLP practitioners, it's crucial to consider these ethical implications alongside the technical capabilities of these models.\n",
    "\n",
    "## 5. Summary\n",
    "\n",
    "### 5.1 Conclusion\n",
    "\n",
    "In this comprehensive lesson on NLP fundamentals, we've explored the importance and wide-ranging applications of NLP, delved into various NLP tasks, traced the evolution of NLP techniques, and examined state-of-the-art models that are pushing the boundaries of what's possible in language processing and generation.\n",
    "\n",
    "Key takeaways include:\n",
    "\n",
    "1. The pervasive importance of NLP in modern AI applications, from virtual assistants to healthcare.\n",
    "2. The diversity of NLP tasks, including classification, extraction, and generation, each with its own challenges and applications.\n",
    "3. The rapid evolution of NLP techniques, from rule-based systems to transformer-based models, dramatically improving performance across various tasks.\n",
    "4. The groundbreaking capabilities of state-of-the-art models like BERT and GPT, which have opened up new possibilities in NLP.\n",
    "5. The ethical considerations that come with these powerful new technologies, including privacy concerns and the potential for bias.\n",
    "\n",
    "As NLP continues to advance, understanding these fundamentals will be crucial for developing and applying NLP solutions effectively and responsibly.\n",
    "\n",
    "### 5.2 Mind Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0975eb1",
   "metadata": {},
   "source": [
    "![Mermaid diagram](lesson_02_mermaid_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f986f8",
   "metadata": {},
   "source": [
    "This mind map provides a visual overview of the key concepts covered in this lesson, illustrating the relationships between different aspects of NLP fundamentals, tasks, evolution, and state-of-the-art models.\n",
    "\n",
    "## 6. Homework\n",
    "\n",
    "1. Implement a text classification system using three different approaches: a traditional machine learning method (e.g., Naive Bayes or SVM), a recurrent neural network (LSTM or GRU), and a fine-tuned BERT model. Compare their performance on a dataset of your choice (e.g., movie reviews, news articles).\n",
    "\n",
    "2. Develop a named entity recognition (NER) system using spaCy. Train it on a custom dataset related to a specific domain (e.g., medical texts, legal documents) and evaluate its performance.\n",
    "\n",
    "3. Experiment with the GPT-2 model (or GPT-3 if you have access) to generate text in different styles (e.g., news articles, poetry, technical documentation). Analyze the strengths and weaknesses of the generated text and discuss potential applications and ethical considerations.\n",
    "\n",
    "4. Implement a simple chatbot using a combination of rule-based and machine learning techniques. The chatbot should be able to handle basic queries in a specific domain (e.g., customer support for an e-commerce platform).\n",
    "\n",
    "5. Conduct a literature review on recent advancements in multilingual NLP models. Write a report (1500-2000 words) discussing the challenges in multilingual NLP and how recent models address these challenges.\n",
    "\n",
    "6. Develop a simple machine translation system using sequence-to-sequence learning with attention. Train it on a small parallel corpus (e.g., English-French) and compare its performance with popular online translation services.\n",
    "\n",
    "## 7. Reference and Citation\n",
    "\n",
    "[1] Jurafsky, D., & Martin, J. H. (2020). Speech and Language Processing (3rd ed. draft). Retrieved from <https://web.stanford.edu/~jurafsky/slp3/>\n",
    "\n",
    "[2] Goldberg, Y. (2017). Neural Network Methods for Natural Language Processing. Synthesis Lectures on Human Language Technologies, 10(1), 1-309.\n",
    "\n",
    "[3] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.\n",
    "\n",
    "[4] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv preprint arXiv:1810.04805.\n",
    "\n",
    "[5] Brown, T. B., Mann, B., Ryder,[5] Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., ... & Amodei, D. (2020). Language Models are Few-Shot Learners. arXiv preprint arXiv:2005.14165.\n",
    "\n",
    "[6] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language models are unsupervised multitask learners. OpenAI Blog, 1(8), 9.\n",
    "\n",
    "[7] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., ... & Liu, P. J. (2020). Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of Machine Learning Research, 21, 1-67.\n",
    "\n",
    "[8] Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R., & Le, Q. V. (2019). XLNet: Generalized Autoregressive Pretraining for Language Understanding. arXiv preprint arXiv:1906.08237.\n",
    "\n",
    "[9] Liao, K. P., Cai, T., Savova, G. K., Murphy, S. N., Karlson, E. W., Ananthakrishnan, A. N., ... & Kohane, I. S. (2015). Development of phenotype algorithms using electronic medical records and incorporating natural language processing. bmj, 350.\n",
    "\n",
    "[10] Hutto, C. J., & Gilbert, E. (2014). Vader: A parsimonious rule-based model for sentiment analysis of social media text. In Eighth international AAAI conference on weblogs and social media.\n",
    "\n",
    "[11] Honnibal, M., & Montani, I. (2017). spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing.\n",
    "\n",
    "[12] Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., ... & Rush, A. M. (2020). Transformers: State-of-the-art Natural Language Processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations (pp. 38-45).\n",
    "\n",
    "[13] Bahdanau, D., Cho, K., & Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.\n",
    "\n",
    "[14] Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., & Zettlemoyer, L. (2018). Deep contextualized word representations. arXiv preprint arXiv:1802.05365.\n",
    "\n",
    "[15] Conneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G., Guzm√°n, F., ... & Stoyanov, V. (2020). Unsupervised Cross-lingual Representation Learning at Scale. arXiv preprint arXiv:1911.02116."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
