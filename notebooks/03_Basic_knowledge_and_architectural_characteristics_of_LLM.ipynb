{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1441cac5",
   "metadata": {},
   "source": [
    "# Lesson 3: Basic Knowledge and Architectural Characteristics of LLMs\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Large Language Models (LLMs) have revolutionized the field of Natural Language Processing (NLP). In this lesson, we'll explore the development history of LLMs, delve into the key concepts of attention and transformers, and examine the architectural characteristics that make LLMs so powerful.\n",
    "\n",
    "## Lesson Objectives\n",
    "\n",
    "By the end of this lesson, you will:\n",
    "1. Understand the development history of LLMs and their impact on NLP\n",
    "2. Grasp the concepts of attention and transformer architecture\n",
    "3. Recognize the key architectural characteristics of LLMs\n",
    "\n",
    "## 1. Development History of LLMs and Their Applications in NLP\n",
    "\n",
    "The history of LLMs is closely tied to the evolution of neural network architectures and the increasing availability of computational resources and data.\n",
    "\n",
    "### Key Milestones:\n",
    "\n",
    "1. **2013: Word2Vec** - Although not an LLM, Word2Vec introduced the concept of word embeddings, laying the groundwork for future developments.\n",
    "\n",
    "2. **2014: Sequence-to-Sequence Models** - These models, using LSTMs, showed promise in machine translation tasks.\n",
    "\n",
    "3. **2017: Transformer Architecture** - Introduced in the \"Attention Is All You Need\" paper, this architecture became the foundation for modern LLMs.\n",
    "\n",
    "4. **2018: BERT** - Google's BERT model demonstrated the power of bidirectional training and transfer learning in NLP.\n",
    "\n",
    "5. **2019: GPT-2** - OpenAI's GPT-2 showcased impressive text generation capabilities.\n",
    "\n",
    "6. **2020: GPT-3** - With 175 billion parameters, GPT-3 demonstrated remarkable few-shot learning abilities.\n",
    "\n",
    "7. **2022: ChatGPT** - Based on GPT-3.5, it showed human-like conversational abilities.\n",
    "\n",
    "8. **2023: GPT-4** - Multimodal capabilities and even more advanced language understanding and generation.\n",
    "\n",
    "### Applications in NLP:\n",
    "\n",
    "LLMs have found applications in various NLP tasks, including:\n",
    "- Machine Translation\n",
    "- Text Summarization\n",
    "- Question Answering\n",
    "- Text Generation\n",
    "- Sentiment Analysis\n",
    "- Named Entity Recognition\n",
    "- Dialogue Systems\n",
    "\n",
    "Let's visualize the growth in model size over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51d2ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "models = ['BERT', 'GPT-2', 'GPT-3', 'GPT-4']\n",
    "params = [0.34, 1.5, 175, 1000]  # in billions\n",
    "years = [2018, 2019, 2020, 2023]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(years, params, marker='o')\n",
    "plt.title('Growth in LLM Size (Parameters)')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Parameters (billions)')\n",
    "plt.yscale('log')\n",
    "plt.grid(True)\n",
    "for i, model in enumerate(models):\n",
    "    plt.annotate(model, (years[i], params[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcc2738",
   "metadata": {},
   "source": [
    "[Image Placeholder: Graph showing the exponential growth in LLM size over time]\n",
    "\n",
    "## 2. Attention and Transformer Introduction\n",
    "\n",
    "The transformer architecture, introduced in 2017, revolutionized NLP by introducing the self-attention mechanism.\n",
    "\n",
    "### Attention Mechanism:\n",
    "\n",
    "Attention allows a model to focus on different parts of the input when producing each part of the output. In the context of NLP, this means the model can weigh the importance of different words in a sentence when processing or generating text.\n",
    "\n",
    "Key components of attention:\n",
    "- Query (Q): The current word we're focusing on\n",
    "- Key (K): All words we're comparing against\n",
    "- Value (V): The actual content we're extracting information from\n",
    "\n",
    "The attention weight is computed as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6addac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Attention(Q, K, V) = softmax((QK^T) / âˆšd_k) V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373f3091",
   "metadata": {},
   "source": [
    "Here's a simplified implementation of self-attention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c551fed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def self_attention(query, key, value):\n",
    "    d_k = query.shape[-1]\n",
    "    scores = np.matmul(query, key.transpose(-2, -1)) / np.sqrt(d_k)\n",
    "    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n",
    "    return np.matmul(attention_weights, value)\n",
    "\n",
    "# Example usage\n",
    "seq_length, d_model = 4, 64\n",
    "query = np.random.randn(seq_length, d_model)\n",
    "key = np.random.randn(seq_length, d_model)\n",
    "value = np.random.randn(seq_length, d_model)\n",
    "\n",
    "output = self_attention(query, key, value)\n",
    "print(\"Self-attention output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a717fb",
   "metadata": {},
   "source": [
    "### Transformer Architecture:\n",
    "\n",
    "The transformer architecture consists of an encoder and a decoder, each composed of multiple layers. Each layer contains:\n",
    "1. Multi-Head Attention\n",
    "2. Feed-Forward Neural Network\n",
    "3. Layer Normalization\n",
    "4. Residual Connections\n",
    "\n",
    "Here's a high-level visualization of the transformer architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa6b6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "dot = Digraph(comment='Transformer Architecture')\n",
    "dot.attr(rankdir='TB', size='8,8')\n",
    "\n",
    "# Input\n",
    "dot.node('A', 'Input')\n",
    "\n",
    "# Encoder\n",
    "with dot.subgraph(name='cluster_0') as c:\n",
    "    c.attr(label='Encoder')\n",
    "    c.node('B', 'Self-Attention')\n",
    "    c.node('C', 'Feed Forward')\n",
    "    c.edge('B', 'C')\n",
    "\n",
    "# Decoder\n",
    "with dot.subgraph(name='cluster_1') as c:\n",
    "    c.attr(label='Decoder')\n",
    "    c.node('D', 'Masked\\nSelf-Attention')\n",
    "    c.node('E', 'Encoder-Decoder\\nAttention')\n",
    "    c.node('F', 'Feed Forward')\n",
    "    c.edge('D', 'E')\n",
    "    c.edge('E', 'F')\n",
    "\n",
    "# Output\n",
    "dot.node('G', 'Output')\n",
    "\n",
    "# Connections\n",
    "dot.edge('A', 'B')\n",
    "dot.edge('C', 'E')\n",
    "dot.edge('F', 'G')\n",
    "\n",
    "dot.render('transformer_architecture', format='png', cleanup=True)\n",
    "dot.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500ba9f3",
   "metadata": {},
   "source": [
    "[Image Placeholder: Diagram of the Transformer Architecture]\n",
    "\n",
    "## 3. Architectural Characteristics of LLMs\n",
    "\n",
    "LLMs are characterized by several key architectural features:\n",
    "\n",
    "### Depth:\n",
    "LLMs typically have many layers, allowing them to learn hierarchical representations of language. For example:\n",
    "- BERT-base: 12 layers\n",
    "- GPT-3: 96 layers\n",
    "\n",
    "### Width:\n",
    "The width refers to the dimensionality of the hidden states. Larger widths allow for more expressive representations:\n",
    "- BERT-base: 768-dimensional hidden states\n",
    "- GPT-3: 12,288-dimensional hidden states\n",
    "\n",
    "### Parameter Scale:\n",
    "The total number of trainable parameters in the model. This has been increasing dramatically:\n",
    "- BERT-base: 110 million parameters\n",
    "- GPT-3: 175 billion parameters\n",
    "- GPT-4: estimated over 1 trillion parameters\n",
    "\n",
    "### Ability to Handle Natural Language Tasks:\n",
    "LLMs exhibit several key capabilities:\n",
    "1. Transfer Learning: Pre-trained on large corpora, they can be fine-tuned for specific tasks.\n",
    "2. Few-shot Learning: They can perform new tasks with just a few examples.\n",
    "3. Zero-shot Learning: They can attempt tasks they weren't explicitly trained on.\n",
    "4. Multi-task Learning: A single model can perform various NLP tasks.\n",
    "\n",
    "Let's visualize the relationship between model size and performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639c3dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "model_sizes = [0.1, 1, 10, 100, 1000]  # billion parameters\n",
    "performance = [60, 70, 80, 85, 90]  # hypothetical performance metric\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogx(model_sizes, performance, marker='o')\n",
    "plt.title('LLM Size vs Performance')\n",
    "plt.xlabel('Model Size (billion parameters)')\n",
    "plt.ylabel('Performance Metric')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e05e2a0",
   "metadata": {},
   "source": [
    "[Image Placeholder: Graph showing the relationship between LLM size and performance]\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this lesson, we've explored the development history of Large Language Models, delved into the key concepts of attention and transformers that underpin their architecture, and examined the crucial characteristics that define LLMs. \n",
    "\n",
    "As we've seen, LLMs have grown dramatically in size and capability over the past few years, revolutionizing various NLP tasks. Their ability to understand and generate human-like text has opened up new possibilities in AI and continues to push the boundaries of what's possible in natural language processing.\n",
    "\n",
    "In the next lesson, we'll dive deeper into the practical aspects of working with LLMs, including training, fine-tuning, and deployment strategies.\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "1. \"Attention Is All You Need\" paper: https://arxiv.org/abs/1706.03762\n",
    "2. \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" paper: https://arxiv.org/abs/1810.04805\n",
    "3. \"Language Models are Few-Shot Learners\" (GPT-3 paper): https://arxiv.org/abs/2005.14165\n",
    "4. The Illustrated Transformer by Jay Alammar: http://jalammar.github.io/illustrated-transformer/\n",
    "5. \"A Survey of Large Language Models\" by Wei et al.: https://arxiv.org/abs/2303.18223\n",
    "\n",
    "Remember, the field of LLMs is rapidly evolving, with new models and techniques emerging regularly. Stay curious and keep exploring!"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
