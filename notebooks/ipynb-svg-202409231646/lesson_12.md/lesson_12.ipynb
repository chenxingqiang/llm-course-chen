{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ccf9eb7",
   "metadata": {},
   "source": [
    "# Lesson 12: Advanced Model Inference and Function Calling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de68e0ad",
   "metadata": {},
   "source": [
    "<img src=\"data:image/svg+xml;base64,<svg aria-roledescription="gantt" role="graphics-document document" style="max-width: 1184px;" viewBox="0 0 1184 580" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" width="100%" id="mermaid-1727081260879"><style>#mermaid-1727081260879{font-family:"trebuchet ms",verdana,arial,sans-serif;font-size:16px;fill:#333;}#mermaid-1727081260879 .error-icon{fill:#552222;}#mermaid-1727081260879 .error-text{fill:#552222;stroke:#552222;}#mermaid-1727081260879 .edge-thickness-normal{stroke-width:1px;}#mermaid-1727081260879 .edge-thickness-thick{stroke-width:3.5px;}#mermaid-1727081260879 .edge-pattern-solid{stroke-dasharray:0;}#mermaid-1727081260879 .edge-thickness-invisible{stroke-width:0;fill:none;}#mermaid-1727081260879 .edge-pattern-dashed{stroke-dasharray:3;}#mermaid-1727081260879 .edge-pattern-dotted{stroke-dasharray:2;}#mermaid-1727081260879 .marker{fill:#333333;stroke:#333333;}#mermaid-1727081260879 .marker.cross{stroke:#333333;}#mermaid-1727081260879 svg{font-family:"trebuchet ms",verdana,arial,sans-serif;font-size:16px;}#mermaid-1727081260879 p{margin:0;}#mermaid-1727081260879 .mermaid-main-font{font-family:var(--mermaid-font-family, "trebuchet ms", verdana, arial, sans-serif);}#mermaid-1727081260879 .exclude-range{fill:#eeeeee;}#mermaid-1727081260879 .section{stroke:none;opacity:0.2;}#mermaid-1727081260879 .section0{fill:rgba(102, 102, 255, 0.49);}#mermaid-1727081260879 .section2{fill:#fff400;}#mermaid-1727081260879 .section1,#mermaid-1727081260879 .section3{fill:white;opacity:0.2;}#mermaid-1727081260879 .sectionTitle0{fill:#333;}#mermaid-1727081260879 .sectionTitle1{fill:#333;}#mermaid-1727081260879 .sectionTitle2{fill:#333;}#mermaid-1727081260879 .sectionTitle3{fill:#333;}#mermaid-1727081260879 .sectionTitle{text-anchor:start;font-family:var(--mermaid-font-family, "trebuchet ms", verdana, arial, sans-serif);}#mermaid-1727081260879 .grid .tick{stroke:lightgrey;opacity:0.8;shape-rendering:crispEdges;}#mermaid-1727081260879 .grid .tick text{font-family:"trebuchet ms",verdana,arial,sans-serif;fill:#333;}#mermaid-1727081260879 .grid path{stroke-width:0;}#mermaid-1727081260879 .today{fill:none;stroke:red;stroke-width:2px;}#mermaid-1727081260879 .task{stroke-width:2;}#mermaid-1727081260879 .taskText{text-anchor:middle;font-family:var(--mermaid-font-family, "trebuchet ms", verdana, arial, sans-serif);}#mermaid-1727081260879 .taskTextOutsideRight{fill:black;text-anchor:start;font-family:var(--mermaid-font-family, "trebuchet ms", verdana, arial, sans-serif);}#mermaid-1727081260879 .taskTextOutsideLeft{fill:black;text-anchor:end;}#mermaid-1727081260879 .task.clickable{cursor:pointer;}#mermaid-1727081260879 .taskText.clickable{cursor:pointer;fill:#003163!important;font-weight:bold;}#mermaid-1727081260879 .taskTextOutsideLeft.clickable{cursor:pointer;fill:#003163!important;font-weight:bold;}#mermaid-1727081260879 .taskTextOutsideRight.clickable{cursor:pointer;fill:#003163!important;font-weight:bold;}#mermaid-1727081260879 .taskText0,#mermaid-1727081260879 .taskText1,#mermaid-1727081260879 .taskText2,#mermaid-1727081260879 .taskText3{fill:white;}#mermaid-1727081260879 .task0,#mermaid-1727081260879 .task1,#mermaid-1727081260879 .task2,#mermaid-1727081260879 .task3{fill:#8a90dd;stroke:#534fbc;}#mermaid-1727081260879 .taskTextOutside0,#mermaid-1727081260879 .taskTextOutside2{fill:black;}#mermaid-1727081260879 .taskTextOutside1,#mermaid-1727081260879 .taskTextOutside3{fill:black;}#mermaid-1727081260879 .active0,#mermaid-1727081260879 .active1,#mermaid-1727081260879 .active2,#mermaid-1727081260879 .active3{fill:#bfc7ff;stroke:#534fbc;}#mermaid-1727081260879 .activeText0,#mermaid-1727081260879 .activeText1,#mermaid-1727081260879 .activeText2,#mermaid-1727081260879 .activeText3{fill:black!important;}#mermaid-1727081260879 .done0,#mermaid-1727081260879 .done1,#mermaid-1727081260879 .done2,#mermaid-1727081260879 .done3{stroke:grey;fill:lightgrey;stroke-width:2;}#mermaid-1727081260879 .doneText0,#mermaid-1727081260879 .doneText1,#mermaid-1727081260879 .doneText2,#mermaid-1727081260879 .doneText3{fill:black!important;}#mermaid-1727081260879 .crit0,#mermaid-1727081260879 .crit1,#mermaid-1727081260879 .crit2,#mermaid-1727081260879 .crit3{stroke:#ff8888;fill:red;stroke-width:2;}#mermaid-1727081260879 .activeCrit0,#mermaid-1727081260879 .activeCrit1,#mermaid-1727081260879 .activeCrit2,#mermaid-1727081260879 .activeCrit3{stroke:#ff8888;fill:#bfc7ff;stroke-width:2;}#mermaid-1727081260879 .doneCrit0,#mermaid-1727081260879 .doneCrit1,#mermaid-1727081260879 .doneCrit2,#mermaid-1727081260879 .doneCrit3{stroke:#ff8888;fill:lightgrey;stroke-width:2;cursor:pointer;shape-rendering:crispEdges;}#mermaid-1727081260879 .milestone{transform:rotate(45deg) scale(0.8,0.8);}#mermaid-1727081260879 .milestoneText{font-style:italic;}#mermaid-1727081260879 .doneCritText0,#mermaid-1727081260879 .doneCritText1,#mermaid-1727081260879 .doneCritText2,#mermaid-1727081260879 .doneCritText3{fill:black!important;}#mermaid-1727081260879 .activeCritText0,#mermaid-1727081260879 .activeCritText1,#mermaid-1727081260879 .activeCritText2,#mermaid-1727081260879 .activeCritText3{fill:black!important;}#mermaid-1727081260879 .titleText{text-anchor:middle;font-size:18px;fill:#333;font-family:var(--mermaid-font-family, "trebuchet ms", verdana, arial, sans-serif);}#mermaid-1727081260879 :root{--mermaid-font-family:"trebuchet ms",verdana,arial,sans-serif;}</style><g></g><g text-anchor="middle" font-family="sans-serif" font-size="10" fill="none" transform="translate(75, 530)" class="grid"><path d="M0,-495V0H1034V-495" stroke="currentColor" class="domain"></path><g transform="translate(69,0)" opacity="1" class="tick"><line y2="-495" stroke="currentColor"></line><text style="text-anchor: middle;" font-size="10" stroke="none" dy="1em" y="3" fill="#000">02</text></g><g transform="translate(172,0)" opacity="1" class="tick"><line y2="-495" stroke="currentColor"></line><text style="text-anchor: middle;" font-size="10" stroke="none" dy="1em" y="3" fill="#000">03</text></g><g transform="translate(276,0)" opacity="1" class="tick"><line y2="-495" stroke="currentColor"></line><text style="text-anchor: middle;" font-size="10" stroke="none" dy="1em" y="3" fill="#000">04</text></g><g transform="translate(379,0)" opacity="1" class="tick"><line y2="-495" stroke="currentColor"></line><text style="text-anchor: middle;" font-size="10" stroke="none" dy="1em" y="3" fill="#000">05</text></g><g transform="translate(483,0)" opacity="1" class="tick"><line y2="-495" stroke="currentColor"></line><text style="text-anchor: middle;" font-size="10" stroke="none" dy="1em" y="3" fill="#000">06</text></g><g transform="translate(586,0)" opacity="1" class="tick"><line y2="-495" stroke="currentColor"></line><text style="text-anchor: middle;" font-size="10" stroke="none" dy="1em" y="3" fill="#000">07</text></g><g transform="translate(689,0)" opacity="1" class="tick"><line y2="-495" stroke="currentColor"></line><text style="text-anchor: middle;" font-size="10" stroke="none" dy="1em" y="3" fill="#000">08</text></g><g transform="translate(793,0)" opacity="1" class="tick"><line y2="-495" stroke="currentColor"></line><text style="text-anchor: middle;" font-size="10" stroke="none" dy="1em" y="3" fill="#000">09</text></g><g transform="translate(896,0)" opacity="1" class="tick"><line y2="-495" stroke="currentColor"></line><text style="text-anchor: middle;" font-size="10" stroke="none" dy="1em" y="3" fill="#000">10</text></g><g transform="translate(1000,0)" opacity="1" class="tick"><line y2="-495" stroke="currentColor"></line><text style="text-anchor: middle;" font-size="10" stroke="none" dy="1em" y="3" fill="#000">11</text></g></g><g><rect class="section section0" height="24" width="1146.5" y="48" x="0"></rect><rect class="section section1" height="24" width="1146.5" y="288" x="0"></rect><rect class="section section0" height="24" width="1146.5" y="72" x="0"></rect><rect class="section section1" height="24" width="1146.5" y="312" x="0"></rect><rect class="section section0" height="24" width="1146.5" y="96" x="0"></rect><rect class="section section1" height="24" width="1146.5" y="336" x="0"></rect><rect class="section section0" height="24" width="1146.5" y="120" x="0"></rect><rect class="section section1" height="24" width="1146.5" y="360" x="0"></rect><rect class="section section0" height="24" width="1146.5" y="144" x="0"></rect><rect class="section section1" height="24" width="1146.5" y="384" x="0"></rect><rect class="section section0" height="24" width="1146.5" y="168" x="0"></rect><rect class="section section1" height="24" width="1146.5" y="408" x="0"></rect><rect class="section section0" height="24" width="1146.5" y="192" x="0"></rect><rect class="section section1" height="24" width="1146.5" y="432" x="0"></rect><rect class="section section0" height="24" width="1146.5" y="216" x="0"></rect><rect class="section section1" height="24" width="1146.5" y="456" x="0"></rect><rect class="section section0" height="24" width="1146.5" y="240" x="0"></rect><rect class="section section1" height="24" width="1146.5" y="480" x="0"></rect><rect class="section section0" height="24" width="1146.5" y="264" x="0"></rect><rect class="section section1" height="24" width="1146.5" y="504" x="0"></rect></g><g><rect class="task task0" transform-origin="126.5px 60px" height="20" width="103" y="50" x="75" ry="3" rx="3" id="a8"></rect><rect class="task task1" transform-origin="126.5px 300px" height="20" width="103" y="290" x="75" ry="3" rx="3" id="l8"></rect><rect class="task task0" transform-origin="230px 84px" height="20" width="104" y="74" x="178" ry="3" rx="3" id="a9"></rect><rect class="task task1" transform-origin="230px 324px" height="20" width="104" y="314" x="178" ry="3" rx="3" id="l9"></rect><rect class="task task0" transform-origin="333.5px 108px" height="20" width="103" y="98" x="282" ry="3" rx="3" id="a10"></rect><rect class="task task1" transform-origin="333.5px 348px" height="20" width="103" y="338" x="282" ry="3" rx="3" id="l10"></rect><rect class="task task0" transform-origin="437px 132px" height="20" width="104" y="122" x="385" ry="3" rx="3" id="a11"></rect><rect class="task task1" transform-origin="437px 372px" height="20" width="104" y="362" x="385" ry="3" rx="3" id="l11"></rect><rect class="task active0" transform-origin="540.5px 156px" height="20" width="103" y="146" x="489" ry="3" rx="3" id="a12"></rect><rect class="task active1" transform-origin="540.5px 396px" height="20" width="103" y="386" x="489" ry="3" rx="3" id="l12"></rect><rect class="task task0" transform-origin="643.5px 180px" height="20" width="103" y="170" x="592" ry="3" rx="3" id="a13"></rect><rect class="task task1" transform-origin="643.5px 420px" height="20" width="103" y="410" x="592" ry="3" rx="3" id="l13"></rect><rect class="task task0" transform-origin="747px 204px" height="20" width="104" y="194" x="695" ry="3" rx="3" id="a14"></rect><rect class="task task1" transform-origin="747px 444px" height="20" width="104" y="434" x="695" ry="3" rx="3" id="l14"></rect><rect class="task task0" transform-origin="850.5px 228px" height="20" width="103" y="218" x="799" ry="3" rx="3" id="a15"></rect><rect class="task task1" transform-origin="850.5px 468px" height="20" width="103" y="458" x="799" ry="3" rx="3" id="l15"></rect><rect class="task task0" transform-origin="954px 252px" height="20" width="104" y="242" x="902" ry="3" rx="3" id="a16"></rect><rect class="task task1" transform-origin="954px 492px" height="20" width="104" y="482" x="902" ry="3" rx="3" id="l16"></rect><rect class="task task0" transform-origin="1057.5px 276px" height="20" width="103" y="266" x="1006" ry="3" rx="3" id="a17"></rect><rect class="task task1" transform-origin="1057.5px 516px" height="20" width="103" y="506" x="1006" ry="3" rx="3" id="l17"></rect><text class="taskTextOutsideRight taskTextOutside0  width-127.171875" y="63.5" x="183" font-size="11" id="a8-text">LLM training - Fine-tuning                         </text><text class="taskText taskText1  width-39.1484375" y="303.5" x="126.5" font-size="11" id="l8-text">lesson 8  </text><text class="taskTextOutsideRight taskTextOutside0  width-320.8046875" y="87.5" x="287" font-size="11" id="a9-text">LLM training - Reward Modeling and Proximal Policy Optimization </text><text class="taskText taskText1  width-39.1484375" y="327.5" x="230" font-size="11" id="l9-text">lesson 9  </text><text class="taskTextOutsideRight taskTextOutside0  width-203.046875" y="111.5" x="390" font-size="11" id="a10-text">Famous SOTA LLM models and JAIS model              </text><text class="taskText taskText1  width-44.9140625" y="351.5" x="333.5" font-size="11" id="l10-text">lesson 10 </text><text class="taskTextOutsideRight taskTextOutside0  width-205.453125" y="135.5" x="494" font-size="11" id="a11-text">Methods and Metrics for Model Evaluation           </text><text class="taskText taskText1  width-44.9140625" y="375.5" x="437" font-size="11" id="l11-text">lesson 11 </text><text class="taskTextOutsideRight taskTextOutside0 activeText0 width-182.15625" y="159.5" x="597" font-size="11" id="a12-text">Model Inference and Function calling               </text><text class="taskText taskText1 activeText1 width-44.9140625" y="399.5" x="540.5" font-size="11" id="l12-text">lesson 12 </text><text class="taskTextOutsideRight taskTextOutside0  width-249.0703125" y="183.5" x="700" font-size="11" id="a13-text">Prompt engineering - ChatGPT Prompt Engineering    </text><text class="taskText taskText1  width-44.9140625" y="423.5" x="643.5" font-size="11" id="l13-text">lesson 13 </text><text class="taskTextOutsideRight taskTextOutside0  width-152.8671875" y="207.5" x="804" font-size="11" id="a14-text">Model Quantization Techniques                      </text><text class="taskText taskText1  width-44.9140625" y="447.5" x="747" font-size="11" id="l14-text">lesson 14 </text><text class="taskTextOutsideRight taskTextOutside0  width-155.578125" y="231.5" x="907" font-size="11" id="a15-text">Introduction to Chatbot Project                    </text><text class="taskText taskText1  width-44.9140625" y="471.5" x="850.5" font-size="11" id="l15-text">lesson 15 </text><text class="taskTextOutsideLeft taskTextOutside0" y="255.5" x="897" font-size="11" id="a16-text">Test Dataset Collection and Model Evaluation       </text><text class="taskText taskText1  width-44.9140625" y="495.5" x="954" font-size="11" id="l16-text">lesson 16 </text><text class="taskTextOutsideLeft taskTextOutside0" y="279.5" x="1001" font-size="11" id="a17-text">Designing input and output formats for chatbot with context </text><text class="taskText taskText1  width-44.9140625" y="519.5" x="1057.5" font-size="11" id="l17-text">lesson 17 </text></g><g><text class="sectionTitle sectionTitle0" font-size="11" y="170" x="10" dy="0em"><tspan x="10" alignment-baseline="central">Course Content</tspan></text><text class="sectionTitle sectionTitle1" font-size="11" y="410" x="10" dy="0em"><tspan x="10" alignment-baseline="central">Lessons</tspan></text></g><g class="today"><line class="today" y2="555" y1="25" x2="2066975" x1="2066975"></line></g><text class="titleText" y="25" x="592">LLM Course Timeline</text></svg>\" alt=\"Mermaid diagram 1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa61340d",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "In this lesson, we delve into advanced techniques for model inference and function calling, exploring the spectrum from local deployment to cloud-based services and specialized models. This knowledge is crucial for leveraging the full potential of Large Language Models (LLMs) in various applications.\n",
    "\n",
    "## 2. Local Model Deployment with PyTorch and Hugging Face\n",
    "\n",
    "### 2.1 Explanation\n",
    "\n",
    "Local model deployment offers control and customization but requires careful resource management. PyTorch and Hugging Face's Transformers library provide powerful tools for working with LLMs locally.\n",
    "\n",
    "### 2.2 Code Example: Advanced Local Model Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5801d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "class AdvancedLocalModel:\n",
    "    def __init__(self, model_name):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def generate_text(self, prompt, max_length=100):\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        outputs = self.model.generate(**inputs, max_length=max_length)\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    def fine_tune(self, train_file, output_dir, num_train_epochs=3):\n",
    "        dataset = TextDataset(tokenizer=self.tokenizer, file_path=train_file, block_size=128)\n",
    "        data_collator = DataCollatorForLanguageModeling(tokenizer=self.tokenizer, mlm=False)\n",
    "        \n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            overwrite_output_dir=True,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            per_device_train_batch_size=4,\n",
    "            save_steps=10_000,\n",
    "            save_total_limit=2,\n",
    "        )\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            data_collator=data_collator,\n",
    "            train_dataset=dataset,\n",
    "        )\n",
    "        \n",
    "        trainer.train()\n",
    "        self.model = trainer.model\n",
    "        self.tokenizer.save_pretrained(output_dir)\n",
    "        self.model.save_pretrained(output_dir)\n",
    "\n",
    "    def estimate_model_size(self):\n",
    "        return sum(p.numel() for p in self.model.parameters()) * 4 / (1024 ** 3)  # Size in GB\n",
    "\n",
    "# Usage example\n",
    "model = AdvancedLocalModel(\"gpt2\")\n",
    "print(f\"Model size: {model.estimate_model_size():.2f} GB\")\n",
    "generated_text = model.generate_text(\"The future of AI is\")\n",
    "print(f\"Generated text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958145a6",
   "metadata": {},
   "source": [
    "This code demonstrates loading, using, and fine-tuning a local model, as well as estimating its size for resource management.\n",
    "\n",
    "## 3. Advanced GPU Resource Management\n",
    "\n",
    "### 3.1 Explanation\n",
    "\n",
    "Efficient GPU resource management is crucial for optimal performance when working with large models locally. This involves techniques for memory optimization and dynamic resource allocation.\n",
    "\n",
    "### 3.2 Code Example: GPU Resource Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f13713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import nvidia_smi\n",
    "import gc\n",
    "\n",
    "class GPUResourceManager:\n",
    "    def __init__(self):\n",
    "        nvidia_smi.nvmlInit()\n",
    "        self.handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
    "        self.models = {}\n",
    "\n",
    "    def load_model(self, model_name):\n",
    "        if model_name not in self.models:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "            model.to('cuda')\n",
    "            self.models[model_name] = (model, tokenizer)\n",
    "        return self.models[model_name]\n",
    "\n",
    "    def unload_model(self, model_name):\n",
    "        if model_name in self.models:\n",
    "            del self.models[model_name]\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    def get_gpu_utilization(self):\n",
    "        res = nvidia_smi.nvmlDeviceGetUtilizationRates(self.handle)\n",
    "        return res.gpu\n",
    "\n",
    "    def get_gpu_memory_usage(self):\n",
    "        info = nvidia_smi.nvmlDeviceGetMemoryInfo(self.handle)\n",
    "        return info.used / info.total * 100\n",
    "\n",
    "    def optimize_batch_size(self, model_name, input_text, target_utilization=90):\n",
    "        model, tokenizer = self.load_model(model_name)\n",
    "        batch_size = 1\n",
    "        while True:\n",
    "            try:\n",
    "                inputs = tokenizer([input_text] * batch_size, return_tensors=\"pt\", padding=True, truncation=True).to('cuda')\n",
    "                _ = model.generate(**inputs)\n",
    "                gpu_util = self.get_gpu_utilization()\n",
    "                if gpu_util > target_utilization:\n",
    "                    batch_size -= 1\n",
    "                    break\n",
    "                batch_size *= 2\n",
    "            except RuntimeError:  # Out of memory\n",
    "                batch_size = max(1, batch_size // 2)\n",
    "                break\n",
    "        return batch_size\n",
    "\n",
    "# Usage example\n",
    "manager = GPUResourceManager()\n",
    "print(f\"Initial GPU Memory Usage: {manager.get_gpu_memory_usage():.2f}%\")\n",
    "optimal_batch_size = manager.optimize_batch_size(\"gpt2\", \"The future of AI is\")\n",
    "print(f\"Optimal batch size for gpt2: {optimal_batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8d36a4",
   "metadata": {},
   "source": [
    "This class demonstrates advanced GPU resource management techniques, including model loading/unloading and batch size optimization.\n",
    "\n",
    "## 4. Leveraging Cloud-Based LLM Services\n",
    "\n",
    "### 4.1 Explanation\n",
    "\n",
    "Cloud-based LLM services, like the OpenAI API, offer access to powerful models without the need for local resources. Effective use requires understanding API integration, prompt engineering, and resource management.\n",
    "\n",
    "### 4.2 Code Example: Advanced OpenAI API Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e7da80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import asyncio\n",
    "import aiohttp\n",
    "\n",
    "class AdvancedOpenAIClient:\n",
    "    def __init__(self, api_key):\n",
    "        openai.api_key = api_key\n",
    "        self.base_url = \"https://api.openai.com/v1/engines/\"\n",
    "        self.session = None\n",
    "\n",
    "    async def __aenter__(self):\n",
    "        self.session = aiohttp.ClientSession()\n",
    "        return self\n",
    "\n",
    "    async def __aexit__(self, exc_type, exc, tb):\n",
    "        await self.session.close()\n",
    "\n",
    "    async def generate_text_async(self, prompt, model=\"text-davinci-002\", max_tokens=100, temperature=0.7):\n",
    "        url = f\"{self.base_url}{model}/completions\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {openai.api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        data = {\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"temperature\": temperature,\n",
    "        }\n",
    "        \n",
    "        async with self.session.post(url, headers=headers, json=data) as response:\n",
    "            if response.status == 200:\n",
    "                result = await response.json()\n",
    "                return result['choices'][0]['text'].strip()\n",
    "            else:\n",
    "                raise Exception(f\"Error {response.status}: {await response.text()}\")\n",
    "\n",
    "    async def batch_generate(self, prompts, model=\"text-davinci-002\", max_tokens=100, temperature=0.7):\n",
    "        tasks = [self.generate_text_async(prompt, model, max_tokens, temperature) for prompt in prompts]\n",
    "        return await asyncio.gather(*tasks)\n",
    "\n",
    "# Usage example\n",
    "async def main():\n",
    "    api_key = \"your-api-key-here\"\n",
    "    async with AdvancedOpenAIClient(api_key) as client:\n",
    "        prompts = [\n",
    "            \"Write a tweet about the future of AI\",\n",
    "            \"Create a short product description for a smart home device\",\n",
    "            \"Compose a haiku about machine learning\"\n",
    "        ]\n",
    "        results = await client.batch_generate(prompts)\n",
    "        for i, result in enumerate(results):\n",
    "            print(f\"Result {i+1}: {result}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd929d93",
   "metadata": {},
   "source": [
    "This example demonstrates advanced usage of the OpenAI API, including asynchronous requests and batch processing.\n",
    "\n",
    "## 5. Exploring Specialized Models: JAIS\n",
    "\n",
    "### 5.1 Explanation\n",
    "\n",
    "Specialized models like JAIS (Juelich AI Supercomputer) focus on specific domains, offering advantages in tasks like scientific text analysis. Understanding their capabilities is crucial for domain-specific applications.\n",
    "\n",
    "### 5.2 Code Example: JAIS Model Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1febb829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "class JAISModel:\n",
    "    def __init__(self, model_name):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        nltk.download('punkt')\n",
    "\n",
    "    def generate_text(self, prompt, max_length=200):\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        outputs = self.model.generate(**inputs, max_length=max_length)\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    def summarize_paper(self, abstract, max_length=150):\n",
    "        prompt = f\"Summarize the following scientific abstract:\\n\\n{abstract}\\n\\nSummary:\"\n",
    "        return self.generate_text(prompt, max_length)\n",
    "\n",
    "    def extract_key_findings(self, paper_text, num_findings=3):\n",
    "        prompt = f\"Extract {num_findings} key findings from the following scientific text:\\n\\n{paper_text}\\n\\nKey Findings:\"\n",
    "        findings = self.generate_text(prompt, max_length=200)\n",
    "        return sent_tokenize(findings)[:num_findings]\n",
    "\n",
    "# Usage example\n",
    "model = JAISModel(\"jais-model-name\")  # Replace with actual JAIS model name\n",
    "abstract = \"Recent advancements in quantum computing have shown promising results in solving complex optimization problems...\"\n",
    "summary = model.summarize_paper(abstract)\n",
    "print(f\"Summary: {summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be4fe78",
   "metadata": {},
   "source": [
    "This example demonstrates how to use a specialized model like JAIS for scientific text analysis tasks.\n",
    "\n",
    "## 6. Implementing Custom Function Calling\n",
    "\n",
    "### 6.1 Explanation\n",
    "\n",
    "Custom function calling allows LLMs to interact with external systems, expanding their capabilities beyond text generation. This involves defining functions, parsing LLM outputs, and executing external actions.\n",
    "\n",
    "### 6.2 Code Example: Custom Function Calling System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b2f23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import openai\n",
    "import datetime\n",
    "\n",
    "class AIAssistant:\n",
    "    def __init__(self, api_key):\n",
    "        openai.api_key = api_key\n",
    "        self.functions = {\n",
    "            \"schedule_appointment\": self.schedule_appointment,\n",
    "            \"check_weather\": self.check_weather\n",
    "        }\n",
    "\n",
    "    def parse_function_call(self, text):\n",
    "        match = re.search(r'FUNCTION_CALL\\((.*?)\\)', text)\n",
    "        if match:\n",
    "            try:\n",
    "                return json.loads(match.group(1).replace(\"'\", '\"'))\n",
    "            except json.JSONDecodeError:\n",
    "                return None\n",
    "        return None\n",
    "\n",
    "    def execute_function(self, func_name, **kwargs):\n",
    "        if func_name in self.functions:\n",
    "            return self.functions[func_name](**kwargs)\n",
    "        return f\"Error: Function '{func_name}' not found.\"\n",
    "\n",
    "    def generate_response(self, user_input):\n",
    "        prompt = f\"\"\"\n",
    "        You are an AI assistant that can perform various tasks. If you need to perform a specific action, \n",
    "        use the FUNCTION_CALL format like this: FUNCTION_CALL({{\"name\": \"function_name\", \"args\": {{\"arg1\": \"value1\", \"arg2\": \"value2\"}}}})\n",
    "        \n",
    "        Available functions:\n",
    "        - schedule_appointment(date, time, description)\n",
    "        - check_weather(city, date)\n",
    "\n",
    "        User: {user_input}\n",
    "        Assistant: \"\"\"\n",
    "\n",
    "        response = openai.Completion.create(\n",
    "            engine=\"text-davinci-002\",\n",
    "            prompt=prompt,\n",
    "            max_tokens=150\n",
    "        )\n",
    "\n",
    "        ai_response = response.choices[0].text.strip()\n",
    "        function_call = self.parse_function_call(ai_response)\n",
    "\n",
    "        if function_call:\n",
    "            result = self.execute_function(function_call['name'], **function_call['args'])\n",
    "            return f\"{ai_response}\\n\\nFunction result: {result}\"\n",
    "        else:\n",
    "            return ai_response\n",
    "\n",
    "    def schedule_appointment(self, date, time, description):\n",
    "        # This would interact with a calendar API in a real implementation\n",
    "        appointment_datetime = datetime.datetime.strptime(f\"{date} {time}\", \"%Y-%m-%d %H:%M\")\n",
    "        return f\"Appointment scheduled for {appointment_datetime} - {description}\"\n",
    "\n",
    "    def check_weather(self, city, date):\n",
    "        # This would use a weather API in a real implementation\n",
    "        return f\"Weather forecast for {city} on {date}: Sunny, 25°C\"\n",
    "\n",
    "# Usage example\n",
    "assistant = AIAssistant(\"your-openai-api-key\")\n",
    "response = assistant.generate_response(\"Schedule a meeting with John for tomorrow at 2 PM to discuss the project proposal.\")\n",
    "print(f\"Assistant: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c18c3bc",
   "metadata": {},
   "source": [
    "This example demonstrates a custom function calling system that allows an AI assistant to perform tasks like scheduling appointments and checking weather.\n",
    "\n",
    "## 7. Comparative Analysis\n",
    "\n",
    "### 7.1 Explanation\n",
    "\n",
    "Understanding the strengths and weaknesses of different inference approaches is crucial for choosing the right solution for a given application. This involves comparing local models, cloud-based services, and specialized models across various dimensions.\n",
    "\n",
    "### 7.2 Code Example: Comparative Analysis Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05615004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import openai\n",
    "\n",
    "class ModelComparisonFramework:\n",
    "    def __init__(self, local_model_name, openai_api_key, specialized_model_name):\n",
    "        self.local_model = self.load_local_model(local_model_name)\n",
    "        self.openai_api_key = openai_api_key\n",
    "        self.specialized_model = self.load_local_model(specialized_model_name)\n",
    "\n",
    "    def load_local_model(self, model_name):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "        return (model, tokenizer)\n",
    "\n",
    "    def cloud_inference(self, prompt, max_tokens=100):\n",
    "        openai.api_key = self.openai_api_key\n",
    "        start_time = time.time()\n",
    "        response = openai.Completion.create(\n",
    "            engine=\"text-davinci-002\",\n",
    "            prompt=prompt,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        inference_time = time.time() - start_time\n",
    "        return response.choices[0].text.strip(), inference_time\n",
    "\n",
    "    def specialized_inference(self, prompt, max_length=100):\n",
    "        model, tokenizer = self.specialized_model\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        start_time = time.time()\n",
    "        outputs = model.generate(**inputs, max_length=max_length)\n",
    "        inference_time = time.time() - start_time\n",
    "        return tokenizer.decode(outputs[0], skip_special_tokens=True), inference_time\n",
    "\n",
    "    def run_comparison(self, prompts, task_types):\n",
    "        results = []\n",
    "        for prompt, task_type in zip(prompts, task_types):\n",
    "            local_result, local_time = self.local_inference(prompt)\n",
    "            cloud_result, cloud_time = self.cloud_inference(prompt)\n",
    "            specialized_result, specialized_time = self.specialized_inference(prompt)\n",
    "\n",
    "            results.append({\n",
    "                \"task_type\": task_type,\n",
    "                \"prompt\": prompt,\n",
    "                \"local\": {\"result\": local_result, \"time\": local_time},\n",
    "                \"cloud\": {\"result\": cloud_result, \"time\": cloud_time},\n",
    "                \"specialized\": {\"result\": specialized_result, \"time\": specialized_time}\n",
    "            })\n",
    "\n",
    "        return results\n",
    "\n",
    "    def analyze_results(self, results):\n",
    "        analysis = {\n",
    "            \"average_times\": {\"local\": 0, \"cloud\": 0, \"specialized\": 0},\n",
    "            \"task_type_performance\": {}\n",
    "        }\n",
    "\n",
    "        for result in results:\n",
    "            task_type = result[\"task_type\"]\n",
    "            if task_type not in analysis[\"task_type_performance\"]:\n",
    "                analysis[\"task_type_performance\"][task_type] = {\"local\": 0, \"cloud\": 0, \"specialized\": 0}\n",
    "            \n",
    "            for model_type in [\"local\", \"cloud\", \"specialized\"]:\n",
    "                analysis[\"average_times\"][model_type] += result[model_type][\"time\"]\n",
    "                analysis[\"task_type_performance\"][task_type][model_type] += result[model_type][\"time\"]\n",
    "\n",
    "        num_results = len(results)\n",
    "        for model_type in [\"local\", \"cloud\", \"specialized\"]:\n",
    "            analysis[\"average_times\"][model_type] /= num_results\n",
    "\n",
    "        for task_type in analysis[\"task_type_performance\"]:\n",
    "            for model_type in [\"local\", \"cloud\", \"specialized\"]:\n",
    "                analysis[\"task_type_performance\"][task_type][model_type] /= sum(1 for r in results if r[\"task_type\"] == task_type)\n",
    "\n",
    "        return analysis\n",
    "\n",
    "# Usage example\n",
    "framework = ModelComparisonFramework(\n",
    "    local_model_name=\"gpt2\",\n",
    "    openai_api_key=\"your-openai-api-key\",\n",
    "    specialized_model_name=\"jais-model-name\"  # Replace with actual JAIS model name\n",
    ")\n",
    "\n",
    "prompts = [\n",
    "    \"Summarize the key features of quantum computing.\",\n",
    "    \"Write a short story about a robot learning to paint.\",\n",
    "    \"Explain the process of photosynthesis in simple terms.\",\n",
    "    \"Generate a hypothesis for the impact of climate change on coral reefs.\"\n",
    "]\n",
    "\n",
    "task_types = [\"scientific_explanation\", \"creative_writing\", \"science_simplification\", \"scientific_hypothesis\"]\n",
    "\n",
    "comparison_results = framework.run_comparison(prompts, task_types)\n",
    "analysis = framework.analyze_results(comparison_results)\n",
    "\n",
    "print(\"Comparison Results:\")\n",
    "for result in comparison_results:\n",
    "    print(f\"\\nTask Type: {result['task_type']}\")\n",
    "    print(f\"Prompt: {result['prompt']}\")\n",
    "    for model_type in [\"local\", \"cloud\", \"specialized\"]:\n",
    "        print(f\"{model_type.capitalize()} Model:\")\n",
    "        print(f\"  Result: {result[model_type]['result'][:100]}...\")  # Truncated for brevity\n",
    "        print(f\"  Inference Time: {result[model_type]['time']:.4f} seconds\")\n",
    "\n",
    "print(\"\\nAnalysis:\")\n",
    "print(f\"Average Inference Times:\")\n",
    "for model_type, avg_time in analysis[\"average_times\"].items():\n",
    "    print(f\"  {model_type.capitalize()} Model: {avg_time:.4f} seconds\")\n",
    "\n",
    "print(\"\\nTask Type Performance (Average Inference Time):\")\n",
    "for task_type, performance in analysis[\"task_type_performance\"].items():\n",
    "    print(f\"  {task_type}:\")\n",
    "    for model_type, avg_time in performance.items():\n",
    "        print(f\"    {model_type.capitalize()} Model: {avg_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2056bf",
   "metadata": {},
   "source": [
    "This comprehensive framework allows for direct comparison of local, cloud-based, and specialized models across various task types, providing insights into their relative strengths and performance characteristics.\n",
    "\n",
    "## 8. Conclusion\n",
    "\n",
    "In this lesson, we've explored advanced techniques for model inference and function calling, covering a wide range of approaches from local deployment to cloud-based services and specialized models. Key takeaways include:\n",
    "\n",
    "1. Local model deployment offers control and customization but requires careful resource management.\n",
    "2. GPU resource management is crucial for optimal performance when working with large models locally.\n",
    "3. Cloud-based LLM services provide access to powerful models without local resource constraints but require effective API integration and prompt engineering.\n",
    "4. Specialized models like JAIS offer advantages in domain-specific tasks, particularly in scientific and technical fields.\n",
    "5. Custom function calling expands LLM capabilities by enabling interaction with external systems and data sources.\n",
    "6. Comparative analysis is essential for choosing the right approach for specific use cases, considering factors like performance, cost, and scalability.\n",
    "\n",
    "As the field of AI continues to evolve, the ability to effectively implement and combine these various approaches will be crucial for developing sophisticated, efficient, and powerful AI applications.\n",
    "\n",
    "## 9. Further Reading and Resources\n",
    "\n",
    "To deepen your understanding of the topics covered in this lesson, consider exploring the following resources:\n",
    "\n",
    "1. \"Deep Learning\" by Ian Goodfellow, Yoshua Bengio, and Aaron Courville\n",
    "2. PyTorch documentation: <https://pytorch.org/docs/stable/index.html>\n",
    "3. Hugging Face Transformers library documentation: <https://huggingface.co/transformers/>\n",
    "4. OpenAI API documentation: <https://beta.openai.com/docs/>\n",
    "5. \"Designing Machine Learning Systems\" by Chip Huyen\n",
    "\n",
    "## 10. Homework Assignments\n",
    "\n",
    "1. Implement a local model using PyTorch and Hugging Face for sentiment analysis. Optimize its performance and analyze its resource usage on different hardware configurations.\n",
    "\n",
    "2. Develop a cloud-based application using the OpenAI API that acts as a creative writing assistant, capable of generating story outlines, character descriptions, and dialogue.\n",
    "\n",
    "3. Research and write a report on the current state and future potential of specialized language models like JAIS, including potential applications in fields such as bioinformatics and climate modeling.\n",
    "\n",
    "4. Design and implement a custom function calling system for an AI assistant that can perform at least five different real-world tasks (e.g., scheduling, data analysis, web scraping).\n",
    "\n",
    "5. Conduct a comparative analysis of local, cloud-based, and specialized model approaches for a language translation application. Consider factors such as translation quality, speed, cost, and scalability in your analysis.\n",
    "\n",
    "By completing these assignments, you'll gain practical experience in implementing and evaluating different approaches to model inference and function calling, preparing you for real-world AI development challenges."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
