{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41ff377f",
   "metadata": {},
   "source": [
    "# Model Quantization Techniques: A Comprehensive Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98942280",
   "metadata": {},
   "source": [
    "<img src=\"data:image/svg+xml;base64,<svg aria-roledescription="gantt" role="graphics-document document" style="max-width: 1184px;" viewBox="0 0 1184 580" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" width="100%" id="mermaid-1727081265004"><style>#mermaid-1727081265004{font-family:"trebuchet ms",verdana,arial,sans-serif;font-size:16px;fill:#333;}#mermaid-1727081265004 .error-icon{fill:#552222;}#mermaid-1727081265004 .error-text{fill:#552222;stroke:#552222;}#mermaid-1727081265004 .edge-thickness-normal{stroke-width:1px;}#mermaid-1727081265004 .edge-thickness-thick{stroke-width:3.5px;}#mermaid-1727081265004 .edge-pattern-solid{stroke-dasharray:0;}#mermaid-1727081265004 .edge-thickness-invisible{stroke-width:0;fill:none;}#mermaid-1727081265004 .edge-pattern-dashed{stroke-dasharray:3;}#mermaid-1727081265004 .edge-pattern-dotted{stroke-dasharray:2;}#mermaid-1727081265004 .marker{fill:#333333;stroke:#333333;}#mermaid-1727081265004 .marker.cross{stroke:#333333;}#mermaid-1727081265004 svg{font-family:"trebuchet ms",verdana,arial,sans-serif;font-size:16px;}#mermaid-1727081265004 p{margin:0;}#mermaid-1727081265004 .mermaid-main-font{font-family:var(--mermaid-font-family, "trebuchet ms", verdana, arial, sans-serif);}#mermaid-1727081265004 .exclude-range{fill:#eeeeee;}#mermaid-1727081265004 .section{stroke:none;opacity:0.2;}#mermaid-1727081265004 .section0{fill:rgba(102, 102, 255, 0.49);}#mermaid-1727081265004 .section2{fill:#fff400;}#mermaid-1727081265004 .section1,#mermaid-1727081265004 .section3{fill:white;opacity:0.2;}#mermaid-1727081265004 .sectionTitle0{fill:#333;}#mermaid-1727081265004 .sectionTitle1{fill:#333;}#mermaid-1727081265004 .sectionTitle2{fill:#333;}#mermaid-1727081265004 .sectionTitle3{fill:#333;}#mermaid-1727081265004 .sectionTitle{text-anchor:start;font-family:var(--mermaid-font-family, "trebuchet ms", verdana, arial, sans-serif);}#mermaid-1727081265004 .grid .tick{stroke:lightgrey;opacity:0.8;shape-rendering:crispEdges;}#mermaid-1727081265004 .grid .tick text{font-family:"trebuchet ms",verdana,arial,sans-serif;fill:#333;}#mermaid-1727081265004 .grid path{stroke-width:0;}#mermaid-1727081265004 .today{fill:none;stroke:red;stroke-width:2px;}#mermaid-1727081265004 .task{stroke-width:2;}#mermaid-1727081265004 .taskText{text-anchor:middle;font-family:var(--mermaid-font-family, "trebuchet ms", verdana, arial, sans-serif);}#mermaid-1727081265004 .taskTextOutsideRight{fill:black;text-anchor:start;font-family:var(--mermaid-font-family, "trebuchet ms", verdana, arial, sans-serif);}#mermaid-1727081265004 .taskTextOutsideLeft{fill:black;text-anchor:end;}#mermaid-1727081265004 .task.clickable{cursor:pointer;}#mermaid-1727081265004 .taskText.clickable{cursor:pointer;fill:#003163!important;font-weight:bold;}#mermaid-1727081265004 .taskTextOutsideLeft.clickable{cursor:pointer;fill:#003163!important;font-weight:bold;}#mermaid-1727081265004 .taskTextOutsideRight.clickable{cursor:pointer;fill:#003163!important;font-weight:bold;}#mermaid-1727081265004 .taskText0,#mermaid-1727081265004 .taskText1,#mermaid-1727081265004 .taskText2,#mermaid-1727081265004 .taskText3{fill:white;}#mermaid-1727081265004 .task0,#mermaid-1727081265004 .task1,#mermaid-1727081265004 .task2,#mermaid-1727081265004 .task3{fill:#8a90dd;stroke:#534fbc;}#mermaid-1727081265004 .taskTextOutside0,#mermaid-1727081265004 .taskTextOutside2{fill:black;}#mermaid-1727081265004 .taskTextOutside1,#mermaid-1727081265004 .taskTextOutside3{fill:black;}#mermaid-1727081265004 .active0,#mermaid-1727081265004 .active1,#mermaid-1727081265004 .active2,#mermaid-1727081265004 .active3{fill:#bfc7ff;stroke:#534fbc;}#mermaid-1727081265004 .activeText0,#mermaid-1727081265004 .activeText1,#mermaid-1727081265004 .activeText2,#mermaid-1727081265004 .activeText3{fill:black!important;}#mermaid-1727081265004 .done0,#mermaid-1727081265004 .done1,#mermaid-1727081265004 .done2,#mermaid-1727081265004 .done3{stroke:grey;fill:lightgrey;stroke-width:2;}#mermaid-1727081265004 .doneText0,#mermaid-1727081265004 .doneText1,#mermaid-1727081265004 .doneText2,#mermaid-1727081265004 .doneText3{fill:black!important;}#mermaid-1727081265004 .crit0,#mermaid-1727081265004 .crit1,#mermaid-1727081265004 .crit2,#mermaid-1727081265004 .crit3{stroke:#ff8888;fill:red;stroke-width:2;}#mermaid-1727081265004 .activeCrit0,#mermaid-1727081265004 .activeCrit1,#mermaid-1727081265004 .activeCrit2,#mermaid-1727081265004 .activeCrit3{stroke:#ff8888;fill:#bfc7ff;stroke-width:2;}#mermaid-1727081265004 .doneCrit0,#mermaid-1727081265004 .doneCrit1,#mermaid-1727081265004 .doneCrit2,#mermaid-1727081265004 .doneCrit3{stroke:#ff8888;fill:lightgrey;stroke-width:2;cursor:pointer;shape-rendering:crispEdges;}#mermaid-1727081265004 .milestone{transform:rotate(45deg) scale(0.8,0.8);}#mermaid-1727081265004 .milestoneText{font-style:italic;}#mermaid-1727081265004 .doneCritText0,#mermaid-1727081265004 .doneCritText1,#mermaid-1727081265004 .doneCritText2,#mermaid-1727081265004 .doneCritText3{fill:black!important;}#mermaid-1727081265004 .activeCritText0,#mermaid-1727081265004 .activeCritText1,#mermaid-1727081265004 .activeCritText2,#mermaid-1727081265004 .activeCritText3{fill:black!important;}#mermaid-1727081265004 .titleText{text-anchor:middle;font-size:18px;fill:#333;font-family:var(--mermaid-font-family, "trebuchet ms", verdana, arial, sans-serif);}#mermaid-1727081265004 :root{--mermaid-font-family:"trebuchet ms",verdana,arial,sans-serif;}</style><g></g><g text-anchor="middle" font-family="sans-serif" font-size="10" fill="none" transform="translate(75, 530)" class="grid"><path d="M0,-495V0H1034V-495" stroke="currentColor" class="domain"></path><g transform="translate(69,0)" opacity="1" class="tick"><line y2="-495" stroke="currentColor"></line><text style="text-anchor: middle;" font-size="10" stroke="none" dy="1em" y="3" fill="#000">02</text></g><g transform="translate(172,0)" opacity="1" class="tick"><line y2="-495" stroke="currentColor"></line><text style="text-anchor: middle;" font-size="10" stroke="none" dy="1em" y="3" fill="#000">03</text></g><g transform="translate(276,0)" opacity="1" class="tick"><line y2="-495" stroke="currentColor"></line><text style="text-anchor: middle;" font-size="10" stroke="none" dy="1em" y="3" fill="#000">04</text></g><g transform="translate(379,0)" opacity="1" class="tick"><line y2="-495" stroke="currentColor"></line><text style="text-anchor: middle;" font-size="10" stroke="none" dy="1em" y="3" fill="#000">05</text></g><g transform="translate(483,0)" opacity="1" class="tick"><line y2="-495" stroke="currentColor"></line><text style="text-anchor: middle;" font-size="10" stroke="none" dy="1em" y="3" fill="#000">06</text></g><g transform="translate(586,0)" opacity="1" class="tick"><line y2="-495" stroke="currentColor"></line><text style="text-anchor: middle;" font-size="10" stroke="none" dy="1em" y="3" fill="#000">07</text></g><g transform="translate(689,0)" opacity="1" class="tick"><line y2="-495" stroke="currentColor"></line><text style="text-anchor: middle;" font-size="10" stroke="none" dy="1em" y="3" fill="#000">08</text></g><g transform="translate(793,0)" opacity="1" class="tick"><line y2="-495" stroke="currentColor"></line><text style="text-anchor: middle;" font-size="10" stroke="none" dy="1em" y="3" fill="#000">09</text></g><g transform="translate(896,0)" opacity="1" class="tick"><line y2="-495" stroke="currentColor"></line><text style="text-anchor: middle;" font-size="10" stroke="none" dy="1em" y="3" fill="#000">10</text></g><g transform="translate(1000,0)" opacity="1" class="tick"><line y2="-495" stroke="currentColor"></line><text style="text-anchor: middle;" font-size="10" stroke="none" dy="1em" y="3" fill="#000">11</text></g></g><g><rect class="section section0" height="24" width="1146.5" y="48" x="0"></rect><rect class="section section1" height="24" width="1146.5" y="288" x="0"></rect><rect class="section section0" height="24" width="1146.5" y="72" x="0"></rect><rect class="section section1" height="24" width="1146.5" y="312" x="0"></rect><rect class="section section0" height="24" width="1146.5" y="96" x="0"></rect><rect class="section section1" height="24" width="1146.5" y="336" x="0"></rect><rect class="section section0" height="24" width="1146.5" y="120" x="0"></rect><rect class="section section1" height="24" width="1146.5" y="360" x="0"></rect><rect class="section section0" height="24" width="1146.5" y="144" x="0"></rect><rect class="section section1" height="24" width="1146.5" y="384" x="0"></rect><rect class="section section0" height="24" width="1146.5" y="168" x="0"></rect><rect class="section section1" height="24" width="1146.5" y="408" x="0"></rect><rect class="section section0" height="24" width="1146.5" y="192" x="0"></rect><rect class="section section1" height="24" width="1146.5" y="432" x="0"></rect><rect class="section section0" height="24" width="1146.5" y="216" x="0"></rect><rect class="section section1" height="24" width="1146.5" y="456" x="0"></rect><rect class="section section0" height="24" width="1146.5" y="240" x="0"></rect><rect class="section section1" height="24" width="1146.5" y="480" x="0"></rect><rect class="section section0" height="24" width="1146.5" y="264" x="0"></rect><rect class="section section1" height="24" width="1146.5" y="504" x="0"></rect></g><g><rect class="task task0" transform-origin="126.5px 60px" height="20" width="103" y="50" x="75" ry="3" rx="3" id="a10"></rect><rect class="task task1" transform-origin="126.5px 300px" height="20" width="103" y="290" x="75" ry="3" rx="3" id="l10"></rect><rect class="task task0" transform-origin="230px 84px" height="20" width="104" y="74" x="178" ry="3" rx="3" id="a11"></rect><rect class="task task1" transform-origin="230px 324px" height="20" width="104" y="314" x="178" ry="3" rx="3" id="l11"></rect><rect class="task task0" transform-origin="333.5px 108px" height="20" width="103" y="98" x="282" ry="3" rx="3" id="a12"></rect><rect class="task task1" transform-origin="333.5px 348px" height="20" width="103" y="338" x="282" ry="3" rx="3" id="l12"></rect><rect class="task task0" transform-origin="437px 132px" height="20" width="104" y="122" x="385" ry="3" rx="3" id="a13"></rect><rect class="task task1" transform-origin="437px 372px" height="20" width="104" y="362" x="385" ry="3" rx="3" id="l13"></rect><rect class="task active0" transform-origin="540.5px 156px" height="20" width="103" y="146" x="489" ry="3" rx="3" id="a14"></rect><rect class="task active1" transform-origin="540.5px 396px" height="20" width="103" y="386" x="489" ry="3" rx="3" id="l14"></rect><rect class="task task0" transform-origin="643.5px 180px" height="20" width="103" y="170" x="592" ry="3" rx="3" id="a15"></rect><rect class="task task1" transform-origin="643.5px 420px" height="20" width="103" y="410" x="592" ry="3" rx="3" id="l15"></rect><rect class="task task0" transform-origin="747px 204px" height="20" width="104" y="194" x="695" ry="3" rx="3" id="a16"></rect><rect class="task task1" transform-origin="747px 444px" height="20" width="104" y="434" x="695" ry="3" rx="3" id="l16"></rect><rect class="task task0" transform-origin="850.5px 228px" height="20" width="103" y="218" x="799" ry="3" rx="3" id="a17"></rect><rect class="task task1" transform-origin="850.5px 468px" height="20" width="103" y="458" x="799" ry="3" rx="3" id="l17"></rect><rect class="task task0" transform-origin="954px 252px" height="20" width="104" y="242" x="902" ry="3" rx="3" id="a18"></rect><rect class="task task1" transform-origin="954px 492px" height="20" width="104" y="482" x="902" ry="3" rx="3" id="l18"></rect><rect class="task task0" transform-origin="1057.5px 276px" height="20" width="103" y="266" x="1006" ry="3" rx="3" id="a19"></rect><rect class="task task1" transform-origin="1057.5px 516px" height="20" width="103" y="506" x="1006" ry="3" rx="3" id="l19"></rect><text class="taskTextOutsideRight taskTextOutside0  width-203.046875" y="63.5" x="183" font-size="11" id="a10-text">Famous SOTA LLM models and JAIS model              </text><text class="taskText taskText1  width-44.9140625" y="303.5" x="126.5" font-size="11" id="l10-text">lesson 10 </text><text class="taskTextOutsideRight taskTextOutside0  width-205.453125" y="87.5" x="287" font-size="11" id="a11-text">Methods and Metrics for Model Evaluation           </text><text class="taskText taskText1  width-44.9140625" y="327.5" x="230" font-size="11" id="l11-text">lesson 11 </text><text class="taskTextOutsideRight taskTextOutside0  width-182.15625" y="111.5" x="390" font-size="11" id="a12-text">Model Inference and Function calling               </text><text class="taskText taskText1  width-44.9140625" y="351.5" x="333.5" font-size="11" id="l12-text">lesson 12 </text><text class="taskTextOutsideRight taskTextOutside0  width-249.0703125" y="135.5" x="494" font-size="11" id="a13-text">Prompt engineering - ChatGPT Prompt Engineering    </text><text class="taskText taskText1  width-44.9140625" y="375.5" x="437" font-size="11" id="l13-text">lesson 13 </text><text class="taskTextOutsideRight taskTextOutside0 activeText0 width-152.8671875" y="159.5" x="597" font-size="11" id="a14-text">Model Quantization Techniques                      </text><text class="taskText taskText1 activeText1 width-44.9140625" y="399.5" x="540.5" font-size="11" id="l14-text">lesson 14 </text><text class="taskTextOutsideRight taskTextOutside0  width-155.578125" y="183.5" x="700" font-size="11" id="a15-text">Introduction to Chatbot Project                    </text><text class="taskText taskText1  width-44.9140625" y="423.5" x="643.5" font-size="11" id="l15-text">lesson 15 </text><text class="taskTextOutsideRight taskTextOutside0  width-222.2109375" y="207.5" x="804" font-size="11" id="a16-text">Test Dataset Collection and Model Evaluation       </text><text class="taskText taskText1  width-44.9140625" y="447.5" x="747" font-size="11" id="l16-text">lesson 16 </text><text class="taskTextOutsideLeft taskTextOutside0" y="231.5" x="794" font-size="11" id="a17-text">Designing input and output formats for chatbot with context </text><text class="taskText taskText1  width-44.9140625" y="471.5" x="850.5" font-size="11" id="l17-text">lesson 17 </text><text class="taskTextOutsideLeft taskTextOutside0" y="255.5" x="897" font-size="11" id="a18-text">Model Deployment and Backend Development           </text><text class="taskText taskText1  width-44.9140625" y="495.5" x="954" font-size="11" id="l18-text">lesson 18 </text><text class="taskTextOutsideLeft taskTextOutside0" y="279.5" x="1001" font-size="11" id="a19-text">Frontend web page debugging                        </text><text class="taskText taskText1  width-44.9140625" y="519.5" x="1057.5" font-size="11" id="l19-text">lesson 19 </text></g><g><text class="sectionTitle sectionTitle0" font-size="11" y="170" x="10" dy="0em"><tspan x="10" alignment-baseline="central">Course Content</tspan></text><text class="sectionTitle sectionTitle1" font-size="11" y="410" x="10" dy="0em"><tspan x="10" alignment-baseline="central">Lessons</tspan></text></g><g class="today"><line class="today" y2="555" y1="25" x2="2066975" x1="2066975"></line></g><text class="titleText" y="25" x="592">LLM Course Timeline</text></svg>\" alt=\"Mermaid diagram 1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f18b9b1",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "In the rapidly evolving landscape of artificial intelligence and deep learning, the deployment of large, complex models on resource-constrained devices has become a significant challenge. Model quantization emerges as a crucial technique to address this issue, enabling efficient inference without sacrificing too much accuracy. This comprehensive guide delves into the intricacies of model quantization, exploring its foundations, various techniques, and practical implementations across different deep learning frameworks and hardware platforms.\n",
    "\n",
    "## 2. Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "\n",
    "1. Understand the fundamental principles and necessity of model quantization in modern AI deployment\n",
    "2. Distinguish between various quantization methods and their specific applications\n",
    "3. Implement different quantization techniques using popular deep learning frameworks\n",
    "4. Evaluate the impact of quantization on model performance, resource utilization, and energy consumption\n",
    "5. Apply appropriate quantization strategies to real-world deep learning models across various architectures\n",
    "6. Analyze the trade-offs between model size, inference speed, and accuracy in quantized models\n",
    "7. Comprehend the implications of hardware-aware quantization for different target devices\n",
    "\n",
    "## 3. Foundations of Model Quantization\n",
    "\n",
    "### 3.1 What is Model Quantization?\n",
    "\n",
    "Model quantization is a technique used to reduce the precision of weights and activations in neural networks. It's a critical approach for deploying large models on resource-constrained devices, improving inference speed, and reducing memory footprint.\n",
    "\n",
    "Key aspects of quantization include:\n",
    "\n",
    "- Reduced precision representation (e.g., converting from 32-bit floating-point to 8-bit integer)\n",
    "- Significant storage and computational benefits\n",
    "- Potential impact on model accuracy, which needs to be carefully managed\n",
    "\n",
    "### 3.2 Why is Quantization Important?\n",
    "\n",
    "The importance of quantization in modern AI deployment cannot be overstated:\n",
    "\n",
    "1. **Resource Efficiency**: Quantized models require less memory and computational power, making them suitable for edge devices and mobile applications.\n",
    "\n",
    "2. **Inference Speed**: Lower precision operations can be executed faster, especially on hardware optimized for integer arithmetic.\n",
    "\n",
    "3. **Energy Efficiency**: Reduced computational requirements lead to lower power consumption, crucial for battery-powered devices.\n",
    "\n",
    "4. **Bandwidth Reduction**: Smaller model sizes result in faster model loading and reduced bandwidth usage for over-the-air updates.\n",
    "\n",
    "5. **Enabling AI on Edge**: Quantization makes it possible to run complex models on devices with limited resources, enabling on-device AI capabilities.\n",
    "\n",
    "### 3.3 Case Study: MobileNetV2 Deployment on Smartphones\n",
    "\n",
    "Let's examine how Google successfully deployed MobileNetV2 on various smartphone devices using quantization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c130a36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the pre-trained MobileNetV2 model\n",
    "base_model = tf.keras.applications.MobileNetV2(weights='imagenet', include_top=True)\n",
    "\n",
    "# Convert the model to TensorFlow Lite format with quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(base_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the quantized model\n",
    "with open('mobilenetv2_quantized.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "# Load and run inference on the quantized model\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Prepare a sample input\n",
    "input_shape = input_details[0]['shape']\n",
    "input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
    "\n",
    "# Set the input tensor\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "# Run inference\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get the output tensor\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "print(f\"Output shape: {output_data.shape}\")\n",
    "print(f\"Top 5 predictions: {output_data[0].argsort()[-5:][::-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7917a01",
   "metadata": {},
   "source": [
    "In this case study, Google achieved:\n",
    "\n",
    "- A 4x reduction in model size\n",
    "- 2-3x speedup in inference time\n",
    "- Less than 1% drop in top-1 accuracy\n",
    "\n",
    "These improvements made it possible to run real-time object detection and classification on a wide range of smartphone devices, from high-end to budget models.\n",
    "\n",
    "### 3.4 Reflection\n",
    "\n",
    "Consider the following questions:\n",
    "\n",
    "1. How does the reduction in precision affect different types of neural network architectures?\n",
    "2. What are the implications for energy consumption in mobile and edge devices?\n",
    "3. How might quantization affect the interpretability of model decisions?\n",
    "4. In what scenarios might the trade-off between model size and accuracy not be worthwhile?\n",
    "\n",
    "## 4. Types of Quantization Techniques\n",
    "\n",
    "There are several quantization techniques, each with its own strengths and use cases:\n",
    "\n",
    "### 4.1 Post-Training Quantization\n",
    "\n",
    "Post-training quantization is applied after a model has been trained, without requiring any retraining or fine-tuning.\n",
    "\n",
    "#### 4.1.1 Dynamic Range Quantization\n",
    "\n",
    "- Quantizes weights to 8-bit precision\n",
    "- Dynamically quantizes activations based on their range during inference\n",
    "- Easiest to implement but may result in lower accuracy compared to other methods\n",
    "\n",
    "#### 4.1.2 Static Quantization\n",
    "\n",
    "- Quantizes both weights and activations to 8-bit precision\n",
    "- Requires a calibration step using a representative dataset\n",
    "- Generally provides better accuracy than dynamic range quantization\n",
    "\n",
    "### 4.2 Quantization-Aware Training\n",
    "\n",
    "- Simulates quantization effects during training\n",
    "- Allows the model to learn to compensate for quantization errors\n",
    "- Often achieves higher accuracy than post-training quantization but requires full training or fine-tuning\n",
    "\n",
    "### 4.3 Mixed-Precision Quantization\n",
    "\n",
    "- Uses different bit-widths for different layers or operations within the model\n",
    "- Allows for fine-grained control over the trade-off between model size and accuracy\n",
    "- Can be combined with other quantization techniques for optimal results\n",
    "\n",
    "Here's a comparison of these techniques:\n",
    "\n",
    "| Technique | Pros | Cons | Best For |\n",
    "|-----------|------|------|----------|\n",
    "| Dynamic Range Quantization | - Easy to implement<br>- No need for calibration data | - Lower accuracy than other methods<br>- Limited to weight quantization | - Quick deployment<br>- Models with dynamic activation ranges |\n",
    "| Static Quantization | - Better accuracy than dynamic<br>- Quantizes weights and activations | - Requires calibration data<br>- More complex implementation | - Models with stable activation ranges<br>- When accuracy is crucial |\n",
    "| Quantization-Aware Training | - Highest accuracy<br>- Can recover from quantization errors | - Requires full training or fine-tuning<br>- Most time-consuming | - Mission-critical applications<br>- When resources for retraining are available |\n",
    "| Mixed-Precision Quantization | - Balances performance and accuracy<br>- Flexible for different layer requirements | - Complex to implement<br>- Requires careful tuning | - Large models with varying layer sensitivities<br>- When fine-grained control is needed |\n",
    "\n",
    "### 4.4 Implementing Quantization-Aware Training in TensorFlow\n",
    "\n",
    "Let's implement quantization-aware training for a simple convolutional neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5fd060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define a simple model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Define quantization-aware training\n",
    "quantize_model = tf.keras.models.clone_model(\n",
    "    model,\n",
    "    clone_function=lambda layer: tf.keras.layers.quantization.quantize_annotate_layer(layer)\n",
    ")\n",
    "\n",
    "# Apply quantization to the model\n",
    "quantize_model = tf.keras.models.sequential.quantize_apply(quantize_model)\n",
    "\n",
    "# Compile the quantized model\n",
    "quantize_model.compile(optimizer='adam',\n",
    "                       loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                       metrics=['accuracy'])\n",
    "\n",
    "# Train the quantized model (assuming you have train_images and train_labels)\n",
    "quantize_model.fit(train_images, train_labels, epochs=5, validation_split=0.2)\n",
    "\n",
    "# Convert to TFLite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(quantize_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "quantized_tflite_model = converter.convert()\n",
    "\n",
    "# Save the quantized model\n",
    "with open('quantized_model.tflite', 'wb') as f:\n",
    "    f.write(quantized_tflite_model)\n",
    "\n",
    "# Evaluate the quantized model\n",
    "interpreter = tf.lite.Interpreter(model_content=quantized_tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "test_accuracy = 0\n",
    "\n",
    "for i, (test_image, test_label) in enumerate(zip(test_images, test_labels)):\n",
    "    interpreter.set_tensor(input_index, test_image)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_index)\n",
    "    predicted_label = output.argmax()\n",
    "    test_accuracy += 1 if predicted_label == test_label else 0\n",
    "\n",
    "print(f\"Quantized model accuracy: {test_accuracy / len(test_labels):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032808c2",
   "metadata": {},
   "source": [
    "This example demonstrates how to implement quantization-aware training, convert the model to TFLite format, and evaluate its performance.\n",
    "\n",
    "### 4.5 Reflection\n",
    "\n",
    "Consider the following questions:\n",
    "\n",
    "1. How does the choice of quantization method affect the development lifecycle?\n",
    "2. What are the implications for model maintenance and updates?\n",
    "3. How might different quantization techniques affect model robustness and generalization?\n",
    "4. In what scenarios would you choose post-training quantization over quantization-aware training, or vice versa?\n",
    "\n",
    "## 5. Quantization for Different Model Architectures\n",
    "\n",
    "Different neural network architectures may require specialized quantization approaches:\n",
    "\n",
    "### 5.1 Convolutional Neural Networks (CNNs)\n",
    "\n",
    "- Focus on quantizing convolutional and fully connected layers\n",
    "- Pay attention to activation functions, especially ReLU, which can be efficiently quantized\n",
    "\n",
    "### 5.2 Recurrent Neural Networks (RNNs)\n",
    "\n",
    "- Consider the impact of quantization on the recurrent state\n",
    "- Techniques like Recurrent Quantization (RecQuant) can be effective\n",
    "\n",
    "### 5.3 Transformer-based models\n",
    "\n",
    "- Attention mechanisms require careful quantization\n",
    "- Layer normalization and softmax operations present challenges for integer-only quantization\n",
    "\n",
    "### 5.4 Graph Neural Networks (GNNs)\n",
    "\n",
    "- Quantize both node features and edge weights\n",
    "- Consider the impact on message passing operations\n",
    "\n",
    "### 5.5 Case Study: Quantizing BERT for Natural Language Processing\n",
    "\n",
    "Let's examine how to quantize a pre-trained BERT model using the Hugging Face Transformers library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1b8d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Prepare the model for quantization\n",
    "model.eval()\n",
    "\n",
    "# Define quantization configuration\n",
    "model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "\n",
    "# Fuse modules\n",
    "model = torch.quantization.fuse_modules(model, [['bert.encoder.layer.0.attention.self.query', 'bert.encoder.layer.0.attention.self.key', 'bert.encoder.layer.0.attention.self.value']])\n",
    "\n",
    "# Prepare model for static quantization\n",
    "model_prepared = torch.quantization.prepare(model)\n",
    "\n",
    "# Calibrate the model (you would typically do this with a calibration dataset)\n",
    "def calibrate(model, data_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "calibrate(model_prepared, calibration_data_loader)\n",
    "\n",
    "# Convert to quantized model\n",
    "quantized_model = torch.quantization.convert(model_prepared)\n",
    "\n",
    "# Evaluate the quantized model\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            labels = batch['labels']\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "original_accuracy = evaluate(model, test_data_loader)\n",
    "quantized_accuracy = evaluate(quantized_model, test_data_loader)\n",
    "\n",
    "print(f\"Original accuracy: {original_accuracy:.4f}\")\n",
    "print(f\"Quantized accuracy: {quantized_accuracy:.4f}\")\n",
    "\n",
    "# Save the quantized model\n",
    "torch.save(quantized_model.state_dict(), 'quantized_bert.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48096a0b",
   "metadata": {},
   "source": [
    "This example demonstrates how to quantize a BERT model, which is a complex transformer architecture widely used in NLP tasks.\n",
    "\n",
    "### 5.6 Reflection\n",
    "\n",
    "Consider the following questions:\n",
    "\n",
    "1. How do attention mechanisms in Transformers affect quantization strategies?\n",
    "2. What are the specific considerations for quantizing recurrent layers in RNNs?\n",
    "3. How might quantization impact the graph structure in GNNs?\n",
    "4. What are the challenges in quantizing models with complex, non-linear operations?\n",
    "\n",
    "## 6. Hardware-Aware Quantization\n",
    "\n",
    "Different hardware platforms have varying support for quantized operations. Hardware-aware quantization optimizes models for specific devices:\n",
    "\n",
    "### 6.1 CPUs (e.g., x86, ARM)\n",
    "\n",
    "- Focus on integer arithmetic optimizations\n",
    "- Consider SIMD (Single Instruction, Multiple Data) capabilities\n",
    "\n",
    "### 6.2 GPUs (e.g., NVIDIA, AMD)\n",
    "\n",
    "- Leverage tensor core capabilities for mixed-precision operations\n",
    "- Optimize for parallel processing of quantized operations\n",
    "\n",
    "### 6.3 Specialized AI accelerators (e.g., Google TPU, Apple Neural Engine)\n",
    "\n",
    "- Take advantage of custom quantization schemes supported by the hardware\n",
    "- Optimize for specific bit-widths and arithmetic operations\n",
    "\n",
    "### 6.4 Case Study: TensorRT Optimization for NVIDIA GPUs\n",
    "\n",
    "Let's examine how to use NVIDIA's TensorRT to optimize a quantized model for GPU inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a9a584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import numpy as np\n",
    "\n",
    "# Load your trained model (assuming you have a trained PyTorch model)\n",
    "model = torch.load('your_model.pth')\n",
    "\n",
    "# Export the model to ONNX format\n",
    "torch.onnx.export(model, dummy_input, \"model.onnx\", opset_version=11)\n",
    "\n",
    "# Create a TensorRT builder and network\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "builder = trt.Builder(TRT_LOGGER)\n",
    "network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n",
    "\n",
    "# Parse the ONNX file\n",
    "parser = trt.OnnxParser(network, TRT_LOGGER)\n",
    "with open(\"model.onnx\", 'rb') as model:\n",
    "    parser.parse(model.read())\n",
    "\n",
    "# Create optimization profile\n",
    "config = builder.create_builder_config()\n",
    "config.max_workspace_size = 1 << 30  # 1GB\n",
    "config.set_flag(trt.BuilderFlag.INT8)\n",
    "\n",
    "# Set up INT8 calibrator\n",
    "class EntropyCalibrator(trt.IInt8EntropyCalibrator2):\n",
    "    def __init__(self, input_layers, stream, cache_file):\n",
    "        super().__init__()\n",
    "        self.input_layers = input_layers\n",
    "        self.stream = stream\n",
    "        self.d_input = cuda.mem_alloc(self.stream.nbytes)\n",
    "        self.cache_file = cache_file\n",
    "        self.count = 0\n",
    "\n",
    "    def get_batch_size(self):\n",
    "        return self.stream.shape[0]\n",
    "\n",
    "    def get_batch(self, names):\n",
    "        if self.count < self.stream.shape[0]:\n",
    "            cuda.memcpy_htod(self.d_input, self.stream[self.count].ravel())\n",
    "            self.count += 1\n",
    "            return [int(self.d_input)]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def read_calibration_cache(self):\n",
    "        if os.path.exists(self.cache_file):\n",
    "            with open(self.cache_file, \"rb\") as f:\n",
    "                return f.read()\n",
    "\n",
    "    def write_calibration_cache(self, cache):\n",
    "        with open(self.cache_file, \"wb\") as f:\n",
    "            f.write(cache)\n",
    "\n",
    "# Prepare calibration data (assuming you have a calibration dataset)\n",
    "calibration_data = np.random.randn(100, 3, 224, 224).astype(np.float32)  # Example for 100 images of size 224x224\n",
    "calibration_data = np.ascontiguousarray(calibration_data)\n",
    "\n",
    "# Create calibrator\n",
    "calibrator = EntropyCalibrator(['input'], calibration_data, 'calibration.cache')\n",
    "config.int8_calibrator = calibrator\n",
    "\n",
    "# Build the engine\n",
    "engine = builder.build_engine(network, config)\n",
    "\n",
    "# Save the engine\n",
    "with open(\"model_int8.engine\", \"wb\") as f:\n",
    "    f.write(engine.serialize())\n",
    "\n",
    "print(\"INT8 TensorRT engine created and saved.\")\n",
    "\n",
    "# Function to run inference\n",
    "def run_inference(engine, input_data):\n",
    "    context = engine.create_execution_context()\n",
    "    \n",
    "    # Allocate device memory\n",
    "    d_input = cuda.mem_alloc(input_data.nbytes)\n",
    "    d_output = cuda.mem_alloc(1 * 1000 * 4)  # Assuming 1000 classes output\n",
    "    \n",
    "    # Create a stream in which to copy inputs/outputs and run inference\n",
    "    stream = cuda.Stream()\n",
    "    \n",
    "    # Transfer input data to device\n",
    "    cuda.memcpy_htod_async(d_input, input_data, stream)\n",
    "    \n",
    "    # Run inference\n",
    "    context.execute_async(1, [int(d_input), int(d_output)], stream.handle)\n",
    "    \n",
    "    # Transfer predictions back\n",
    "    output = np.empty((1, 1000), dtype=np.float32)\n",
    "    cuda.memcpy_dtoh_async(output, d_output, stream)\n",
    "    \n",
    "    # Synchronize the stream\n",
    "    stream.synchronize()\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Load the saved engine\n",
    "with open(\"model_int8.engine\", \"rb\") as f:\n",
    "    engine = trt.Runtime(TRT_LOGGER).deserialize_cuda_engine(f.read())\n",
    "\n",
    "# Prepare input data\n",
    "input_data = np.random.randn(1, 3, 224, 224).astype(np.float32)  # Example for 1 image of size 224x224\n",
    "\n",
    "# Run inference\n",
    "output = run_inference(engine, input_data)\n",
    "print(\"Inference output shape:\", output.shape)\n",
    "print(\"Top 5 predictions:\", output.argsort()[0][-5:][::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15e8c41",
   "metadata": {},
   "source": [
    "This example demonstrates how to use TensorRT to create an INT8 quantized engine from a pre-trained model, optimized for NVIDIA GPUs. The process involves:\n",
    "\n",
    "1. Exporting the model to ONNX format\n",
    "2. Creating a TensorRT builder and network\n",
    "3. Setting up an INT8 calibrator for quantization\n",
    "4. Building and saving the optimized engine\n",
    "5. Running inference using the quantized engine\n",
    "\n",
    "The resulting engine leverages NVIDIA GPU capabilities for efficient INT8 inference, potentially providing significant speedups compared to FP32 or even FP16 operations.\n",
    "\n",
    "### 6.5 Reflection\n",
    "\n",
    "Consider the following questions:\n",
    "\n",
    "1. How does the choice of target hardware affect the quantization process?\n",
    "2. What are the trade-offs between model portability and optimized performance?\n",
    "3. How might hardware-aware quantization influence model design decisions?\n",
    "4. What are the challenges in maintaining a single model that performs well across different hardware platforms?\n",
    "\n",
    "## 7. Summary and Conclusion\n",
    "\n",
    "### 7.1 Key Takeaways\n",
    "\n",
    "Throughout this comprehensive lesson on model quantization techniques, we've explored:\n",
    "\n",
    "1. The foundations and importance of quantization in modern AI deployment\n",
    "2. Various quantization techniques, including post-training quantization, quantization-aware training, and mixed-precision quantization\n",
    "3. Practical implementation strategies using popular frameworks like PyTorch and TensorFlow\n",
    "4. Considerations for quantizing different model architectures, from CNNs to Transformers\n",
    "5. Hardware-aware quantization for optimizing models on specific devices\n",
    "\n",
    "Key points to remember:\n",
    "\n",
    "- Quantization is crucial for deploying large, complex models on resource-constrained devices\n",
    "- Different quantization techniques offer varying trade-offs between accuracy, model size, and inference speed\n",
    "- The choice of quantization method depends on factors like model architecture, target hardware, and performance requirements\n",
    "- Hardware-aware quantization can significantly improve performance on specific devices\n",
    "- Careful evaluation and testing are necessary to ensure quantized models meet accuracy and performance goals\n",
    "\n",
    "### 7.2 Future Trends\n",
    "\n",
    "As the field of AI continues to evolve, we can expect several trends in quantization:\n",
    "\n",
    "1. **Automated Quantization**: Development of tools that automatically select and apply the best quantization strategies for given models and hardware targets\n",
    "2. **Extreme Quantization**: Research into sub-8-bit quantization, including binary and ternary networks\n",
    "3. **Quantization-Friendly Architectures**: Design of neural network architectures that are inherently more amenable to quantization\n",
    "4. **Adaptive Quantization**: Dynamic quantization schemes that adjust precision based on input data or computational resources\n",
    "5. **Quantum-Inspired Quantization**: Exploration of quantization techniques inspired by quantum computing principles\n",
    "\n",
    "### 7.3 Practical Applications\n",
    "\n",
    "Model quantization enables a wide range of applications, including:\n",
    "\n",
    "- Real-time object detection and recognition on mobile devices\n",
    "- Natural language processing tasks on edge devices\n",
    "- Low-latency recommendation systems\n",
    "- Energy-efficient AI in Internet of Things (IoT) devices\n",
    "- On-device speech recognition and synthesis\n",
    "\n",
    "By mastering quantization techniques, you'll be well-equipped to deploy sophisticated AI models in resource-constrained environments, opening up new possibilities for AI applications across various domains.\n",
    "\n",
    "## 8. Further Reading and Resources\n",
    "\n",
    "To deepen your understanding of model quantization, consider exploring these resources:\n",
    "\n",
    "1. Jacob, B., et al. (2018). \"Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference.\" CVPR 2018.\n",
    "2. Krishnamoorthi, R. (2018). \"Quantizing deep convolutional networks for efficient inference: A whitepaper.\" arXiv:1806.08342.\n",
    "3. Wu, H., et al. (2020). \"Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation.\" arXiv:2004.09602.\n",
    "4. Gholami, A., et al. (2021). \"A Survey of Quantization Methods for Efficient Neural Network Inference.\" arXiv:2103.13630.\n",
    "5. PyTorch Quantization Documentation: <https://pytorch.org/docs/stable/quantization.html>\n",
    "6. TensorFlow Model Optimization Toolkit: <https://www.tensorflow.org/model_optimization>\n",
    "7. NVIDIA TensorRT Documentation: <https://developer.nvidia.com/tensorrt>\n",
    "\n",
    "By diving deeper into these resources, you'll gain a more comprehensive understanding of the theoretical foundations and cutting-edge developments in the field of model quantization."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
