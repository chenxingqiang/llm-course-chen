{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7152d704",
   "metadata": {},
   "source": [
    "# Lesson 14: Model Quantization Techniques\n",
    "\n",
    "## Introduction (5 minutes)\n",
    "\n",
    "Welcome to our lesson on Model Quantization Techniques. Today, we'll explore the importance of quantization in deploying large language models, different quantization methods, and their practical implementation. By the end of this session, you'll understand why quantization is necessary and how to apply it to real-world models.\n",
    "\n",
    "## Lesson Objectives\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "1. Understand the necessity of model quantization\n",
    "2. Differentiate between symmetric and asymmetric quantization\n",
    "3. Comprehend the differences between online and offline quantization\n",
    "4. Practically apply quantization to a model and evaluate its impact\n",
    "\n",
    "## Part 1: Theory (25 minutes)\n",
    "\n",
    "### 1. Why Quantization is Necessary (10 minutes)\n",
    "\n",
    "Model quantization is the process of reducing the precision of the model's weights and activations. It's necessary for several reasons:\n",
    "\n",
    "a) Reduced Model Size:\n",
    "   - Smaller storage requirements\n",
    "   - Easier deployment on edge devices\n",
    "\n",
    "b) Faster Inference:\n",
    "   - Reduced memory bandwidth\n",
    "   - More efficient computations, especially on specialized hardware\n",
    "\n",
    "c) Energy Efficiency:\n",
    "   - Lower power consumption, crucial for mobile and IoT devices\n",
    "\n",
    "d) Enabling Deployment on Resource-Constrained Devices:\n",
    "   - Makes it possible to run models on devices with limited memory or processing power\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff8d293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Before quantization\n",
    "print(f\"Model size before quantization: {model.num_parameters() * 4 / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15602546",
   "metadata": {},
   "source": [
    "### 2. Symmetric vs. Asymmetric Quantization (8 minutes)\n",
    "\n",
    "a) Symmetric Quantization:\n",
    "   - Uses a single scale factor for both positive and negative values\n",
    "   - Zero-point is typically fixed at 0\n",
    "   - Simpler to implement and compute\n",
    "   - Example: Q = round(X / scale)\n",
    "\n",
    "b) Asymmetric Quantization:\n",
    "   - Uses both a scale factor and a zero-point\n",
    "   - Can represent the original distribution more accurately\n",
    "   - Slightly more complex computations\n",
    "   - Example: Q = round(X / scale) + zero_point\n",
    "\n",
    "Comparison:\n",
    "- Symmetric: Simpler, faster, but may lose some precision\n",
    "- Asymmetric: More accurate representation, slightly more complex\n",
    "\n",
    "### 3. Online vs. Offline Quantization (7 minutes)\n",
    "\n",
    "a) Offline Quantization:\n",
    "   - Performed during or after training, before deployment\n",
    "   - Uses a representative dataset to determine quantization parameters\n",
    "   - Resulting model is fully quantized at inference time\n",
    "   - Generally provides better performance\n",
    "\n",
    "b) Online (Dynamic) Quantization:\n",
    "   - Performed at runtime, during inference\n",
    "   - Adapts to the specific input data\n",
    "   - Can be more flexible but may have higher computational overhead\n",
    "   - Useful when the deployment environment is unknown or variable\n",
    "\n",
    "Comparison:\n",
    "- Offline: Better performance, fixed quantization\n",
    "- Online: More flexible, adaptable to runtime conditions\n",
    "\n",
    "## Part 2: Practical Exercise (30 minutes)\n",
    "\n",
    "Now, let's apply what we've learned by quantizing a model and evaluating its impact.\n",
    "\n",
    "### Step 1: Load a pre-trained model (5 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef35577f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Check initial model size\n",
    "def get_model_size(model):\n",
    "    return sum(p.numel() for p in model.parameters()) * 4 / (1024 * 1024)\n",
    "\n",
    "print(f\"Original model size: {get_model_size(model):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97a9d1a",
   "metadata": {},
   "source": [
    "### Step 2: Implement INT8 Quantization (10 minutes)\n",
    "\n",
    "We'll use PyTorch's built-in quantization features to perform INT8 quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368dbdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.quantization\n",
    "\n",
    "# Define quantization configuration\n",
    "quantization_config = torch.quantization.get_default_qconfig('fbgemm')\n",
    "torch.quantization.prepare(model, inplace=True)\n",
    "\n",
    "# Calibrate the model (usually done with a representative dataset)\n",
    "dummy_input = torch.randint(0, 50257, (1, 512))\n",
    "model(dummy_input)\n",
    "\n",
    "# Convert to quantized model\n",
    "torch.quantization.convert(model, inplace=True)\n",
    "\n",
    "print(f\"Quantized model size: {get_model_size(model):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a2d4e4",
   "metadata": {},
   "source": [
    "### Step 3: Evaluate Model Performance (10 minutes)\n",
    "\n",
    "Let's compare the original and quantized models in terms of inference speed and output quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682f5f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def evaluate_model(model, tokenizer, text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=50)\n",
    "    end_time = time.time()\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text, end_time - start_time\n",
    "\n",
    "# Test text\n",
    "test_text = \"The future of artificial intelligence is\"\n",
    "\n",
    "# Evaluate original model\n",
    "original_output, original_time = evaluate_model(model, tokenizer, test_text)\n",
    "\n",
    "# Evaluate quantized model\n",
    "quantized_output, quantized_time = evaluate_model(model, tokenizer, test_text)\n",
    "\n",
    "print(f\"Original model time: {original_time:.4f} seconds\")\n",
    "print(f\"Quantized model time: {quantized_time:.4f} seconds\")\n",
    "print(f\"Speed improvement: {(original_time - quantized_time) / original_time * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nOriginal output:\", original_output)\n",
    "print(\"\\nQuantized output:\", quantized_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcbcc2d",
   "metadata": {},
   "source": [
    "### Step 4: Discussion and Analysis (5 minutes)\n",
    "\n",
    "- Compare the model sizes before and after quantization\n",
    "- Analyze the speed improvement\n",
    "- Discuss any differences in the generated outputs\n",
    "- Consider the trade-offs between model size, speed, and output quality\n",
    "\n",
    "## Conclusion and Q&A (5 minutes)\n",
    "\n",
    "We've explored the theory behind model quantization and applied it practically to a GPT-2 model. We've seen how quantization can significantly reduce model size and improve inference speed, with potential trade-offs in output quality.\n",
    "\n",
    "Key takeaways:\n",
    "1. Quantization is crucial for deploying large models in resource-constrained environments\n",
    "2. Different quantization techniques (symmetric/asymmetric, online/offline) offer various trade-offs\n",
    "3. Practical implementation requires careful consideration of the specific use case and deployment environment\n",
    "\n",
    "Are there any questions about the concepts we've covered or the practical exercise?\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "1. PyTorch Quantization Documentation: https://pytorch.org/docs/stable/quantization.html\n",
    "2. \"Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference\" paper: https://arxiv.org/abs/1712.05877\n",
    "3. \"A Survey of Quantization Methods for Efficient Neural Network Inference\" paper: https://arxiv.org/abs/2103.13630\n",
    "\n",
    "In our next lesson, we'll explore advanced techniques for fine-tuning quantized models to further improve their performance."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
