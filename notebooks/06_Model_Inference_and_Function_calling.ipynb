{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b74e0553",
   "metadata": {},
   "source": [
    "# Lesson 12: Model Inference and Function Calling\n",
    "\n",
    "## Introduction (5 minutes)\n",
    "\n",
    "Welcome to our lesson on Model Inference and Function Calling. Today, we'll explore how to use different Large Language Models (LLMs) for inference, both locally and through remote APIs. We'll cover PyTorch and Hugging Face for local models, OpenAI API for remote services, and introduce the JAIS model.\n",
    "\n",
    "## Lesson Objectives\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "1. Load and use local LLM models using PyTorch and Hugging Face\n",
    "2. Estimate model size and manage GPU resources\n",
    "3. Use the OpenAI API to access remote LLM services\n",
    "4. Implement inference using the JAIS model\n",
    "\n",
    "Let's dive in!\n",
    "\n",
    "## Part 1: Theory (25 minutes)\n",
    "\n",
    "### 1. Using PyTorch/Hugging Face for Local LLM Models (15 minutes)\n",
    "\n",
    "Loading and using local LLM models involves several key considerations:\n",
    "\n",
    "a) Model Loading:\n",
    "   - Use `from_pretrained()` method from Hugging Face's Transformers library\n",
    "   - Specify the model path or name\n",
    "\n",
    "b) Model Size Estimation:\n",
    "   - Use `model.num_parameters()` to get the number of parameters\n",
    "   - Multiply by 4 bytes (for float32) to estimate memory usage\n",
    "\n",
    "c) GPU Memory Management:\n",
    "   - Use `torch.cuda.get_device_properties(0).total_memory` to check total GPU memory\n",
    "   - Use `torch.cuda.memory_allocated()` to check currently allocated memory\n",
    "\n",
    "d) Configuring GPU Usage:\n",
    "   - Use `torch.cuda.device_count()` to check available GPUs\n",
    "   - Use `torch.cuda.set_device(device_num)` to set a specific GPU\n",
    "   - For multi-GPU setup, use `nn.DataParallel` or `nn.DistributedDataParallel`\n",
    "\n",
    "### 2. Using OpenAI API for Remote LLM Services (5 minutes)\n",
    "\n",
    "To use OpenAI's API:\n",
    "1. Install the OpenAI Python library: `pip install openai`\n",
    "2. Set up your API key\n",
    "3. Make API calls using the provided functions\n",
    "\n",
    "Key considerations:\n",
    "- API rate limits and costs\n",
    "- Latency compared to local models\n",
    "- Available models and their capabilities\n",
    "\n",
    "### 3. Introduction to JAIS Model (5 minutes)\n",
    "\n",
    "JAIS (Juelich AI Supercomputer) is a powerful language model developed by JÃ¼lich Supercomputing Centre. Key points:\n",
    "- Specialized in scientific and technical domains\n",
    "- Multilingual capabilities\n",
    "- High performance for specific tasks\n",
    "\n",
    "## Part 2: Practice (25 minutes)\n",
    "\n",
    "Let's put our theory into practice with hands-on examples.\n",
    "\n",
    "### 1. Local Model Inference with PyTorch/Hugging Face (10 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a4b134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def load_model(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    return tokenizer, model\n",
    "\n",
    "def estimate_model_size(model):\n",
    "    return model.num_parameters() * 4 / (1024 ** 3)  # Size in GB\n",
    "\n",
    "def check_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)  # Memory in GB\n",
    "    return 0\n",
    "\n",
    "def generate_text(model, tokenizer, prompt, max_length=50):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_length=max_length)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Example usage\n",
    "model_name = \"gpt2\"\n",
    "tokenizer, model = load_model(model_name)\n",
    "\n",
    "print(f\"Model size: {estimate_model_size(model):.2f} GB\")\n",
    "print(f\"Available GPU memory: {check_gpu_memory():.2f} GB\")\n",
    "\n",
    "prompt = \"The future of AI is\"\n",
    "generated_text = generate_text(model, tokenizer, prompt)\n",
    "print(f\"Generated text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6fdef6",
   "metadata": {},
   "source": [
    "### 2. Using OpenAI API (10 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb18bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Set your API key\n",
    "openai.api_key = \"your-api-key-here\"\n",
    "\n",
    "def generate_text_openai(prompt, model=\"text-davinci-002\", max_tokens=50):\n",
    "    response = openai.Completion.create(\n",
    "        engine=model,\n",
    "        prompt=prompt,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "# Example usage\n",
    "prompt = \"The future of AI is\"\n",
    "generated_text = generate_text_openai(prompt)\n",
    "print(f\"Generated text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78593eed",
   "metadata": {},
   "source": [
    "### 3. Using JAIS Model (5 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa9fa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def load_jais_model():\n",
    "    model_name = \"jais-model-name\"  # Replace with actual JAIS model name\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    return tokenizer, model\n",
    "\n",
    "def generate_text_jais(model, tokenizer, prompt, max_length=50):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_length=max_length)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Example usage\n",
    "tokenizer, model = load_jais_model()\n",
    "prompt = \"The latest advancements in quantum computing are\"\n",
    "generated_text = generate_text_jais(model, tokenizer, prompt)\n",
    "print(f\"Generated text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0eb5716",
   "metadata": {},
   "source": [
    "## Conclusion and Q&A (5 minutes)\n",
    "\n",
    "We've covered how to perform model inference using local models with PyTorch and Hugging Face, how to use the OpenAI API for remote services, and how to work with the JAIS model. Remember to consider model size, GPU memory, and specific model capabilities when choosing your inference method.\n",
    "\n",
    "Are there any questions about the topics we've covered?\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "1. Hugging Face Transformers Documentation: https://huggingface.co/transformers/\n",
    "2. PyTorch Documentation: https://pytorch.org/docs/stable/index.html\n",
    "3. OpenAI API Documentation: https://beta.openai.com/docs/\n",
    "4. JAIS Model Information: [Insert link to JAIS documentation when available]\n",
    "\n",
    "In our next lesson, we'll dive deeper into prompt engineering techniques to optimize our interactions with these powerful language models."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
