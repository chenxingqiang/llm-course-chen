{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "13117032",
            "metadata": {},
            "source": [
                "# 1.Course Title: Comprehensive NLP Fundamentals"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "82ffb60e",
            "metadata": {},
            "source": [
                "![Mermaid diagram](lesson_02_mermaid_1.png)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "816652ab",
            "metadata": {},
            "source": [
                "Advanced Natural Language Processing: From Fundamentals to State-of-the-Art\n",
                "\n",
                "# 2. Learning Objectives\n",
                "\n",
                "By the end of this comprehensive lesson, students will be able to:\n",
                "\n",
                "- 2.1 Thoroughly understand and explain the importance of NLP in modern AI applications\n",
                "- 2.2 Recognize and categorize the main types of NLP tasks, with the ability to provide relevant examples\n",
                "- 2.3 Comprehend the evolution of NLP algorithms and models, from traditional approaches to neural networks\n",
                "- 2.4 Gain deep insights into state-of-the-art NLP models and their applications\n",
                "- 2.5 Implement basic NLP tasks using popular libraries and frameworks\n",
                "- 2.6 Critically evaluate the strengths and limitations of different NLP approaches\n",
                "\n",
                "# 3. Overview\n",
                "\n",
                "This in-depth lesson covers four key concepts, providing a comprehensive exploration of Natural Language Processing (NLP) fundamentals:\n",
                "\n",
                "- 3.1 The Importance and Wide-ranging Applications of NLP\n",
                "- 3.2 Comprehensive Overview of NLP Tasks and Their Real-world Applications\n",
                "- 3.3 The Evolution of NLP: From Rule-based Systems to Transformers\n",
                "- 3.4 State-of-the-Art NLP Models and Their Groundbreaking Capabilities\n",
                "\n",
                "# 4. Detailed Content\n",
                "\n",
                "## 4.1 Concept 1: The Importance and Wide-ranging Applications of NLP\n",
                "\n",
                "## 4.1.1 Explanation\n",
                "Natural Language Processing (NLP) is a crucial subfield of artificial intelligence that focuses on the interaction between computers and humans using natural language. Its importance stems from several key factors:\n",
                "\n",
                "1. Improving Human-Computer Interaction: NLP enables more natural and intuitive interfaces.\n",
                "2. Information Extraction: It allows machines to extract meaningful information from unstructured text.\n",
                "3. Automation of Language-related Tasks: NLP can automate translation, summarization, and other language tasks.\n",
                "4. Enhancing Accessibility: It helps in creating tools for people with disabilities.\n",
                "5. Data-driven Decision Making: NLP can analyze large volumes of text data to provide insights.\n",
                "\n",
                "The applications of NLP are vast and growing, including:\n",
                "- Virtual Assistants (e.g., Siri, Alexa)\n",
                "- Machine Translation (e.g., Google Translate)\n",
                "- Sentiment Analysis for Social Media Monitoring\n",
                "- Chatbots for Customer Service\n",
                "- Content Recommendation Systems\n",
                "- Automatic Text Summarization\n",
                "- Speech Recognition Systems\n",
                "\n",
                "## 4.1.2 Case Study: NLP in Healthcare\n",
                "Let's consider the application of NLP in healthcare. Electronic Health Records (EHRs) contain vast amounts of unstructured text data. NLP can help in:\n",
                "\n",
                "1. Extracting relevant medical information from clinical notes\n",
                "2. Identifying potential drug interactions\n",
                "3. Assisting in clinical decision support systems\n",
                "4. Automating medical coding for billing purposes\n",
                "\n",
                "For example, a study by Liao et al. (2015) showed that NLP could identify patients with heart failure from clinical notes with high accuracy, potentially improving early diagnosis and treatment.\n",
                "\n",
                "## 4.1.3 Code: Advanced Sentiment Analysis with VADER\n",
                "\n",
                "Let's implement a more sophisticated sentiment analysis using VADER (Valence Aware Dictionary and sEntiment Reasoner), which is particularly attuned to sentiments expressed in social media:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8bf4a0ad",
            "metadata": {},
            "outputs": [],
            "source": [
                "import nltk\n",
                "from nltk.sentiment import SentimentIntensityAnalyzer\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "nltk.download('vader_lexicon')\n",
                "sia = SentimentIntensityAnalyzer()\n",
                "\n",
                "def analyze_sentiment(text):\n",
                "    return sia.polarity_scores(text)\n",
                "\n",
                "texts = [\n",
                "    \"I absolutely love this NLP course! It's incredibly informative.\",\n",
                "    \"The content is okay, but I find some parts confusing.\",\n",
                "    \"This is the worst course I've ever taken. Total waste of time.\",\n",
                "    \"While the course has some interesting points, it could be better organized.\",\n",
                "    \"I'm amazed by how much I've learned about NLP in such a short time!\"\n",
                "]\n",
                "\n",
                "sentiments = [analyze_sentiment(text) for text in texts]\n",
                "\n",
                "# Plotting\n",
                "fig, ax = plt.subplots(figsize=(10, 6))\n",
                "x = range(len(texts))\n",
                "ax.bar([i-0.2 for i in x], [s['pos'] for s in sentiments], width=0.2, align='center', label='Positive', color='green')\n",
                "ax.bar([i for i in x], [s['neu'] for s in sentiments], width=0.2, align='center', label='Neutral', color='gray')\n",
                "ax.bar([i+0.2 for i in x], [s['neg'] for s in sentiments], width=0.2, align='center', label='Negative', color='red')\n",
                "ax.set_ylabel('Sentiment Score')\n",
                "ax.set_title('Sentiment Analysis of Course Feedback')\n",
                "ax.set_xticks(x)\n",
                "ax.set_xticklabels([f'Text {i+1}' for i in x], rotation=45, ha='right')\n",
                "ax.legend()\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "for i, (text, sentiment) in enumerate(zip(texts, sentiments)):\n",
                "    print(f\"Text {i+1}: {text}\")\n",
                "    print(f\"Sentiment: {sentiment}\")\n",
                "    print(f\"Overall: {'Positive' if sentiment['compound'] > 0 else 'Negative' if sentiment['compound'] < 0 else 'Neutral'}\")\n",
                "    print()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a6a9da1e",
            "metadata": {},
            "source": [
                "This code performs sentiment analysis on multiple pieces of text feedback about a course, visualizing the results and providing a more nuanced understanding of sentiment.\n",
                "\n",
                "## 4.1.4 Reflection\n",
                "\n",
                "The wide-ranging applications of NLP demonstrate its crucial role in modern AI. From improving user experiences to enabling data-driven decision making in complex fields like healthcare, NLP is transforming how we interact with and extract value from textual data. However, it's important to consider the ethical implications, such as privacy concerns in healthcare applications or potential biases in sentiment analysis systems.\n",
                "\n",
                "## 4.2 Concept 2: Comprehensive Overview of NLP Tasks and Their Real-world Applications\n",
                "\n",
                "## 4.2.1 Explanation\n",
                "\n",
                "NLP encompasses a wide range of tasks, which can be broadly categorized into three main types:\n",
                "\n",
                "1. Classification Tasks: Assigning predefined categories to text.\n",
                "   - Examples: Sentiment Analysis, Topic Classification, Spam Detection\n",
                "\n",
                "2. Extraction Tasks: Identifying and extracting specific information from text.\n",
                "   - Examples: Named Entity Recognition (NER), Keyword Extraction, Relationship Extraction\n",
                "\n",
                "3. Generation Tasks: Creating human-like text based on input.\n",
                "   - Examples: Machine Translation, Text Summarization, Question Answering\n",
                "\n",
                "Each of these task types serves different purposes in processing and understanding natural language, often working together in complex NLP systems.\n",
                "\n",
                "## 4.2.2 Case Study: Multilingual Customer Support System\n",
                "\n",
                "Imagine developing a multilingual customer support system for a global e-commerce platform. This system would incorporate multiple NLP tasks:\n",
                "\n",
                "1. Language Detection (Classification): Identify the language of incoming customer queries.\n",
                "2. Intent Classification (Classification): Determine the purpose of the customer's message (e.g., return request, product inquiry).\n",
                "3. Named Entity Recognition (Extraction): Identify product names, order numbers, or customer names in the query.\n",
                "4. Sentiment Analysis (Classification): Gauge the customer's mood from their message.\n",
                "5. Machine Translation (Generation): Translate the customer's message if it's not in the support agent's language.\n",
                "6. Response Generation (Generation): Automatically generate appropriate responses based on the query's intent and extracted information.\n",
                "\n",
                "This case study demonstrates how different NLP tasks can be combined to create a sophisticated, real-world application.\n",
                "\n",
                "## 4.2.3 Code: Multi-task NLP Pipeline\n",
                "\n",
                "Let's implement a simplified version of the multilingual customer support system described above:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "914de138",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langdetect import detect\n",
                "from nltk import word_tokenize, pos_tag, ne_chunk\n",
                "from nltk.sentiment import SentimentIntensityAnalyzer\n",
                "from transformers import pipeline\n",
                "\n",
                "# Ensure necessary NLTK data is downloaded\n",
                "import nltk\n",
                "nltk.download('punkt')\n",
                "nltk.download('averaged_perceptron_tagger')\n",
                "nltk.download('maxent_ne_chunker')\n",
                "nltk.download('words')\n",
                "nltk.download('vader_lexicon')\n",
                "\n",
                "# Initialize models\n",
                "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
                "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-ROMANCE\")\n",
                "\n",
                "def process_query(query):\n",
                "    # Language Detection\n",
                "    lang = detect(query)\n",
                "    \n",
                "    # Translate to English if not already in English\n",
                "    if lang != 'en':\n",
                "        translation = translator(query, max_length=512)[0]['translation_text']\n",
                "        print(f\"Translated query: {translation}\")\n",
                "        query = translation\n",
                "    \n",
                "    # Tokenization and NER\n",
                "    tokens = word_tokenize(query)\n",
                "    pos_tags = pos_tag(tokens)\n",
                "    named_entities = ne_chunk(pos_tags)\n",
                "    \n",
                "    # Extract product names (simplified)\n",
                "    products = [word for word, pos in pos_tags if pos == 'NN']\n",
                "    \n",
                "    # Sentiment Analysis\n",
                "    sentiment = sentiment_analyzer.polarity_scores(query)\n",
                "    \n",
                "    # Intent Classification (simplified)\n",
                "    intent = \"product_inquiry\" if \"how\" in query.lower() or \"what\" in query.lower() else \"general_query\"\n",
                "    \n",
                "    return {\n",
                "        \"original_language\": lang,\n",
                "        \"named_entities\": named_entities,\n",
                "        \"products\": products,\n",
                "        \"sentiment\": sentiment,\n",
                "        \"intent\": intent\n",
                "    }\n",
                "\n",
                "# Example usage\n",
                "queries = [\n",
                "    \"How do I return the faulty smartphone I bought last week?\",\n",
                "    \"J'adore votre nouveau produit! Quand sera-t-il disponible en France ?\",\n",
                "    \"This laptop is terrible. I want a refund immediately!\"\n",
                "]\n",
                "\n",
                "for query in queries:\n",
                "    print(f\"\\nProcessing query: {query}\")\n",
                "    result = process_query(query)\n",
                "    print(f\"Original Language: {result['original_language']}\")\n",
                "    print(f\"Named Entities: {result['named_entities']}\")\n",
                "    print(f\"Products mentioned: {result['products']}\")\n",
                "    print(f\"Sentiment: {result['sentiment']}\")\n",
                "    print(f\"Detected Intent: {result['intent']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "de97252b",
            "metadata": {},
            "source": [
                "This code demonstrates a multi-task NLP pipeline that processes customer queries, handling language detection, translation, named entity recognition, sentiment analysis, and a simplified form of intent classification.\n",
                "\n",
                "## 4.2.4 Reflection\n",
                "\n",
                "The diversity of NLP tasks allows for the creation of sophisticated systems that can understand and generate human language in nuanced ways. However, each task comes with its own challenges:\n",
                "\n",
                "1. Classification tasks may struggle with context and nuance.\n",
                "2. Extraction tasks can be sensitive to variations in text structure and vocabulary.\n",
                "3. Generation tasks might produce fluent but inaccurate or biased text.\n",
                "\n",
                "Understanding these challenges is crucial for developing robust NLP systems and choosing the right approaches for specific applications.\n",
                "\n",
                "## 4.3 Concept 3: The Evolution of NLP: From Rule-based Systems to Transformers\n",
                "\n",
                "## 4.3.1 Explanation\n",
                "\n",
                "The field of NLP has seen significant advancements over the years, evolving from simple rule-based systems to sophisticated neural network-based approaches. This evolution can be broadly categorized into several stages:\n",
                "\n",
                "1. Rule-based Systems (1950s-1980s): Hand-crafted rules for language processing.\n",
                "2. Statistical Methods (1980s-2000s): Probabilistic models like n-grams and Hidden Markov Models.\n",
                "3. Machine Learning Approaches (2000s-2010s): Including Naive Bayes, SVMs, and Decision Trees.\n",
                "4. Deep Learning Revolution (2010s-present): Neural networks, particularly sequence models like RNNs and LSTMs.\n",
                "5. Transformer Era (2017-present): Attention-based models revolutionizing NLP tasks.\n",
                "\n",
                "Each stage brought significant improvements in NLP capabilities, with the most recent advancements enabling human-like language understanding and generation.\n",
                "\n",
                "## 4.3.2 Case Study: Evolution of Machine Translation\n",
                "\n",
                "Let's trace the evolution of machine translation as an example:\n",
                "\n",
                "1. Rule-based MT (e.g., Systran, 1970s): Used hand-crafted linguistic rules.\n",
                "   - Pros: Worked well for closely related languages.\n",
                "   - Cons: Struggled with exceptions and idioms.\n",
                "\n",
                "2. Statistical MT (e.g., Google Translate, 2000s): Used statistical models trained on parallel corpora.\n",
                "   - Pros: Better handling of language variations.\n",
                "   - Cons: Required large parallel corpora, struggled with long-range dependencies.\n",
                "\n",
                "3. Neural MT (e.g., Google's GNMT, 2016): Used sequence-to-sequence models with attention.\n",
                "   - Pros: Significant improvement in fluency and accuracy.\n",
                "   - Cons: Computationally intensive, potential for hallucination.\n",
                "\n",
                "4. Transformer-based MT (e.g., DeepL, 2017): Used the transformer architecture.\n",
                "   - Pros: State-of-the-art performance, better handling of context.\n",
                "   - Cons: Requires large amounts of data and computational resources.\n",
                "\n",
                "## 4.3.3 Code: Comparing Different Text Classification Approaches\n",
                "\n",
                "Let's implement and compare different text classification approaches to see the evolution in practice:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "019358f8",
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "from sklearn.feature_extraction.text import CountVectorizer\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.naive_bayes import MultinomialNB\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.metrics import accuracy_score\n",
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.layers import Dense, Embedding, LSTM\n",
                "from tensorflow.keras.preprocessing.text import Tokenizer\n",
                "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Sample data\n",
                "texts = [\n",
                "    \"I love this movie\",\n",
                "    \"Great film, highly recommended\",\n",
                "    \"Terrible movie, waste of time\",\n",
                "    \"I hated every minute of it\",\n",
                "    \"Awesome acting and plot\",\n",
                "    \"Boring and predictable\",\n",
                "    \"Excellent cinematography\",\n",
                "    \"Poor direction and script\"\n",
                "]\n",
                "labels = [1, 1, 0, 0, 1, 0, 1, 0]  # 1 for positive, 0 for negative\n",
                "\n",
                "# Split data\n",
                "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
                "\n",
                "# 1. Naive Bayes with Bag of Words"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "975c6460",
            "metadata": {},
            "source": [
                "mermaid\n",
                "gantt\n",
                "    title LLM Course Timeline\n",
                "    dateFormat X\n",
                "    axisFormat %d\n",
                "    section Course Content\n",
                "    Course Overview                                    :a1, 0, 1d\n",
                "    NLP Fundamentals                                   :active,a2, after a1, 1d\n",
                "    Basic knowledge and architectural characteristics of LLM :a3, after a2, 1d\n",
                "    LLM Development Fundamentals                       :a4, after a3, 1d\n",
                "    Introduction and Setup of the Experimental Environment :a5, after a4, 1d\n",
                "    The concept of the tokenizer and common types      :a6, after a5, 1d\n",
                "    Text data preprocessing and preparation            :a7, after a6, 1d\n",
                "    LLM training - Fine-tuning                         :a8, after a7, 1d\n",
                "    LLM training - Reward Modeling and Proximal Policy Optimization :a9, after a8, 1d\n",
                "    Famous SOTA LLM models and JAIS model              :a10, after a9, 1d\n",
                "    section Lessons\n",
                "    lesson 1  :l1, 0, 1d\n",
                "    lesson 2  :active,l2, after l1, 1d\n",
                "    lesson 3  :l3, after l2, 1d\n",
                "    lesson 4  :l4, after l3, 1d\n",
                "    lesson 5  :l5, after l4, 1d\n",
                "    lesson 6  :l6, after l5, 1d\n",
                "    lesson 7  :l7, after l6, 1d\n",
                "    lesson 8  :l8, after l7, 1d\n",
                "    lesson 9  :l9, after l8, 1d\n",
                "    lesson 10 :l10, after l9, 1d\n",
                "```\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "This mind map provides a visual overview of the key concepts covered in this lesson, illustrating the relationships between different aspects of NLP fundamentals, tasks, evolution, and state-of-the-art models.\n",
                "\n",
                "# 6. Homework\n",
                "\n",
                "1. Implement a text classification system using three different approaches: a traditional machine learning method (e.g., Naive Bayes or SVM), a recurrent neural network (LSTM or GRU), and a fine-tuned BERT model. Compare their performance on a dataset of your choice (e.g., movie reviews, news articles).\n",
                "\n",
                "2. Develop a named entity recognition (NER) system using spaCy. Train it on a custom dataset related to a specific domain (e.g., medical texts, legal documents) and evaluate its performance.\n",
                "\n",
                "3. Experiment with the GPT-2 model (or GPT-3 if you have access) to generate text in different styles (e.g., news articles, poetry, technical documentation). Analyze the strengths and weaknesses of the generated text and discuss potential applications and ethical considerations.\n",
                "\n",
                "4. Implement a simple chatbot using a combination of rule-based and machine learning techniques. The chatbot should be able to handle basic queries in a specific domain (e.g., customer support for an e-commerce platform).\n",
                "\n",
                "5. Conduct a literature review on recent advancements in multilingual NLP models. Write a report (1500-2000 words) discussing the challenges in multilingual NLP and how recent models address these challenges.\n",
                "\n",
                "6. Develop a simple machine translation system using sequence-to-sequence learning with attention. Train it on a small parallel corpus (e.g., English-French) and compare its performance with popular online translation services.\n",
                "\n",
                "# 7. Reference and Citation\n",
                "\n",
                "[1] Jurafsky, D., & Martin, J. H. (2020). Speech and Language Processing (3rd ed. draft). Retrieved from <https://web.stanford.edu/~jurafsky/slp3/>\n",
                "\n",
                "[2] Goldberg, Y. (2017). Neural Network Methods for Natural Language Processing. Synthesis Lectures on Human Language Technologies, 10(1), 1-309.\n",
                "\n",
                "[3] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.\n",
                "\n",
                "[4] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv preprint arXiv:1810.04805.\n",
                "\n",
                "[5] Brown, T. B., Mann, B., Ryder,[5] Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., ... & Amodei, D. (2020). Language Models are Few-Shot Learners. arXiv preprint arXiv:2005.14165.\n",
                "\n",
                "[6] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language models are unsupervised multitask learners. OpenAI Blog, 1(8), 9.\n",
                "\n",
                "[7] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., ... & Liu, P. J. (2020). Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of Machine Learning Research, 21, 1-67.\n",
                "\n",
                "[8] Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R., & Le, Q. V. (2019). XLNet: Generalized Autoregressive Pretraining for Language Understanding. arXiv preprint arXiv:1906.08237.\n",
                "\n",
                "[9] Liao, K. P., Cai, T., Savova, G. K., Murphy, S. N., Karlson, E. W., Ananthakrishnan, A. N., ... & Kohane, I. S. (2015). Development of phenotype algorithms using electronic medical records and incorporating natural language processing. bmj, 350.\n",
                "\n",
                "[10] Hutto, C. J., & Gilbert, E. (2014). Vader: A parsimonious rule-based model for sentiment analysis of social media text. In Eighth international AAAI conference on weblogs and social media.\n",
                "\n",
                "[11] Honnibal, M., & Montani, I. (2017). spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing.\n",
                "\n",
                "[12] Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., ... & Rush, A. M. (2020). Transformers: State-of-the-art Natural Language Processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations (pp. 38-45).\n",
                "\n",
                "[13] Bahdanau, D., Cho, K., & Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.\n",
                "\n",
                "[14] Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., & Zettlemoyer, L. (2018). Deep contextualized word representations. arXiv preprint arXiv:1802.05365.\n",
                "\n",
                "[15] Conneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G., Guzm√°n, F., ... & Stoyanov, V. (2020). Unsupervised Cross-lingual Representation Learning at Scale. arXiv preprint arXiv:1911.02116."
            ]
        }
    ],
    "metadata": {},
    "nbformat": 4,
    "nbformat_minor": 5
}
