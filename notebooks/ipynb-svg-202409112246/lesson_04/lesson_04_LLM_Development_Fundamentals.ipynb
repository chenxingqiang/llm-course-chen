{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0fdaba7",
   "metadata": {},
   "source": [
    "# Course Title: Comprehensive LLM Development Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b62a2b3",
   "metadata": {},
   "source": [
    "<img src=\"data:image/svg+xml;base64,<svg aria-roledescription="gantt" role="graphics-document document" style="max-width: 1184px;" viewBox="0 0 1184 580" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" width="100%" id="mermaid-1726066047167"><style>#mermaid-1726066047167{font-family:"trebuchet ms",verdana,arial,sans-serif;font-size:16px;fill:#333;}#mermaid-1726066047167 .error-icon{fill:#552222;}#mermaid-1726066047167 .error-text{fill:#552222;stroke:#552222;}#mermaid-1726066047167 .edge-thickness-normal{stroke-width:1px;}#mermaid-1726066047167 .edge-thickness-thick{stroke-width:3.5px;}#mermaid-1726066047167 .edge-pattern-solid{stroke-dasharray:0;}#mermaid-1726066047167 .edge-thickness-invisible{stroke-width:0;fill:none;}#mermaid-1726066047167 .edge-pattern-dashed{stroke-dasharray:3;}#mermaid-1726066047167 .edge-pattern-dotted{stroke-dasharray:2;}#mermaid-1726066047167 .marker{fill:#333333;stroke:#333333;}#mermaid-1726066047167 .marker.cross{stroke:#333333;}#mermaid-1726066047167 svg{font-family:"trebuchet ms",verdana,arial,sans-serif;font-size:16px;}#mermaid-1726066047167 p{margin:0;}#mermaid-1726066047167 .mermaid-main-font{font-family:var(--mermaid-font-family, "trebuchet ms", verdana, arial, sans-serif);}#mermaid-1726066047167 .exclude-range{fill:#eeeeee;}#mermaid-1726066047167 .section{stroke:none;opacity:0.2;}#mermaid-1726066047167 .section0{fill:rgba(102, 102, 255, 0.49);}#mermaid-1726066047167 .section2{fill:#fff400;}#mermaid-1726066047167 .section1,#mermaid-1726066047167 .section3{fill:white;opacity:0.2;}#mermaid-1726066047167 .sectionTitle0{fill:#333;}#mermaid-1726066047167 .sectionTitle1{fill:#333;}#mermaid-1726066047167 .sectionTitle2{fill:#333;}#mermaid-1726066047167 .sectionTitle3{fill:#333;}#mermaid-1726066047167 .sectionTitle{text-anchor:start;font-family:var(--mermaid-font-family, "trebuchet ms", verdana, arial, sans-serif);}#mermaid-1726066047167 .grid .tick{stroke:lightgrey;opacity:0.8;shape-rendering:crispEdges;}#mermaid-1726066047167 .grid .tick text{font-family:"trebuchet ms",verdana,arial,sans-serif;fill:#333;}#mermaid-1726066047167 .grid path{stroke-width:0;}#mermaid-1726066047167 .today{fill:none;stroke:red;stroke-width:2px;}#mermaid-1726066047167 .task{stroke-width:2;}#mermaid-1726066047167 .taskText{text-anchor:middle;font-family:var(--mermaid-font-family, "trebuchet ms", verdana, arial, sans-serif);}#mermaid-1726066047167 .taskTextOutsideRight{fill:black;text-anchor:start;font-family:var(--mermaid-font-family, "trebuchet ms", verdana, arial, sans-serif);}#mermaid-1726066047167 .taskTextOutsideLeft{fill:black;text-anchor:end;}#mermaid-1726066047167 .task.clickable{cursor:pointer;}#mermaid-1726066047167 .taskText.clickable{cursor:pointer;fill:#003163!important;font-weight:bold;}#mermaid-1726066047167 .taskTextOutsideLeft.clickable{cursor:pointer;fill:#003163!important;font-weight:bold;}#mermaid-1726066047167 .taskTextOutsideRight.clickable{cursor:pointer;fill:#003163!important;font-weight:bold;}#mermaid-1726066047167 .taskText0,#mermaid-1726066047167 .taskText1,#mermaid-1726066047167 .taskText2,#mermaid-1726066047167 .taskText3{fill:white;}#mermaid-1726066047167 .task0,#mermaid-1726066047167 .task1,#mermaid-1726066047167 .task2,#mermaid-1726066047167 .task3{fill:#8a90dd;stroke:#534fbc;}#mermaid-1726066047167 .taskTextOutside0,#mermaid-1726066047167 .taskTextOutside2{fill:black;}#mermaid-1726066047167 .taskTextOutside1,#mermaid-1726066047167 .taskTextOutside3{fill:black;}#mermaid-1726066047167 .active0,#mermaid-1726066047167 .active1,#mermaid-1726066047167 .active2,#mermaid-1726066047167 .active3{fill:#bfc7ff;stroke:#534fbc;}#mermaid-1726066047167 .activeText0,#mermaid-1726066047167 .activeText1,#mermaid-1726066047167 .activeText2,#mermaid-1726066047167 .activeText3{fill:black!important;}#mermaid-1726066047167 .done0,#mermaid-1726066047167 .done1,#mermaid-1726066047167 .done2,#mermaid-1726066047167 .done3{stroke:grey;fill:lightgrey;stroke-width:2;}#mermaid-1726066047167 .doneText0,#mermaid-1726066047167 .doneText1,#mermaid-1726066047167 .doneText2,#mermaid-1726066047167 .doneText3{fill:black!important;}#mermaid-1726066047167 .crit0,#mermaid-1726066047167 .crit1,#mermaid-1726066047167 .crit2,#mermaid-1726066047167 .crit3{stroke:#ff8888;fill:red;stroke-width:2;}#mermaid-1726066047167 .activeCrit0,#mermaid-1726066047167 .activeCrit1,#mermaid-1726066047167 .activeCrit2,#mermaid-1726066047167 .activeCrit3{stroke:#ff8888;fill:#bfc7ff;stroke-width:2;}#mermaid-1726066047167 .doneCrit0,#mermaid-1726066047167 .doneCrit1,#mermaid-1726066047167 .doneCrit2,#mermaid-1726066047167 .doneCrit3{stroke:#ff8888;fill:lightgrey;stroke-width:2;cursor:pointer;shape-rendering:crispEdges;}#mermaid-1726066047167 .milestone{transform:rotate(45deg) scale(0.8,0.8);}#mermaid-1726066047167 .milestoneText{font-style:italic;}#mermaid-1726066047167 .doneCritText0,#mermaid-1726066047167 .doneCritText1,#mermaid-1726066047167 .doneCritText2,#mermaid-1726066047167 .doneCritText3{fill:black!important;}#mermaid-1726066047167 .activeCritText0,#mermaid-1726066047167 .activeCritText1,#mermaid-1726066047167 .activeCritText2,#mermaid-1726066047167 .activeCritText3{fill:black!important;}#mermaid-1726066047167 .titleText{text-anchor:middle;font-size:18px;fill:#333;font-family:var(--mermaid-font-family, "trebuchet ms", verdana, arial, sans-serif);}#mermaid-1726066047167 :root{--mermaid-font-family:"trebuchet ms",verdana,arial,sans-serif;}</style><g></g><g text-anchor="middle" font-family="sans-serif" font-size="10" fill="none" transform="translate(75, 530)" class="grid"><path d="M0,-495V0H1034V-495" stroke="currentColor" class="domain"></path><g transform="translate(69,0)" opacity="1" class="tick"><line y2="-495" stroke="currentColor"></line><text style="text-anchor: middle;" font-size="10" stroke="none" dy="1em" y="3" fill="#000">02</text></g><g transform="translate(172,0)" opacity="1" class="tick"><line y2="-495" stroke="currentColor"></line><text style="text-anchor: middle;" font-size="10" stroke="none" dy="1em" y="3" fill="#000">03</text></g><g transform="translate(276,0)" opacity="1" class="tick"><line y2="-495" stroke="currentColor"></line><text style="text-anchor: middle;" font-size="10" stroke="none" dy="1em" y="3" fill="#000">04</text></g><g transform="translate(379,0)" opacity="1" class="tick"><line y2="-495" stroke="currentColor"></line><text style="text-anchor: middle;" font-size="10" stroke="none" dy="1em" y="3" fill="#000">05</text></g><g transform="translate(483,0)" opacity="1" class="tick"><line y2="-495" stroke="currentColor"></line><text style="text-anchor: middle;" font-size="10" stroke="none" dy="1em" y="3" fill="#000">06</text></g><g transform="translate(586,0)" opacity="1" class="tick"><line y2="-495" stroke="currentColor"></line><text style="text-anchor: middle;" font-size="10" stroke="none" dy="1em" y="3" fill="#000">07</text></g><g transform="translate(689,0)" opacity="1" class="tick"><line y2="-495" stroke="currentColor"></line><text style="text-anchor: middle;" font-size="10" stroke="none" dy="1em" y="3" fill="#000">08</text></g><g transform="translate(793,0)" opacity="1" class="tick"><line y2="-495" stroke="currentColor"></line><text style="text-anchor: middle;" font-size="10" stroke="none" dy="1em" y="3" fill="#000">09</text></g><g transform="translate(896,0)" opacity="1" class="tick"><line y2="-495" stroke="currentColor"></line><text style="text-anchor: middle;" font-size="10" stroke="none" dy="1em" y="3" fill="#000">10</text></g><g transform="translate(1000,0)" opacity="1" class="tick"><line y2="-495" stroke="currentColor"></line><text style="text-anchor: middle;" font-size="10" stroke="none" dy="1em" y="3" fill="#000">11</text></g></g><g><rect class="section section0" height="24" width="1146.5" y="48" x="0"></rect><rect class="section section1" height="24" width="1146.5" y="288" x="0"></rect><rect class="section section0" height="24" width="1146.5" y="72" x="0"></rect><rect class="section section1" height="24" width="1146.5" y="312" x="0"></rect><rect class="section section0" height="24" width="1146.5" y="96" x="0"></rect><rect class="section section1" height="24" width="1146.5" y="336" x="0"></rect><rect class="section section0" height="24" width="1146.5" y="120" x="0"></rect><rect class="section section1" height="24" width="1146.5" y="360" x="0"></rect><rect class="section section0" height="24" width="1146.5" y="144" x="0"></rect><rect class="section section1" height="24" width="1146.5" y="384" x="0"></rect><rect class="section section0" height="24" width="1146.5" y="168" x="0"></rect><rect class="section section1" height="24" width="1146.5" y="408" x="0"></rect><rect class="section section0" height="24" width="1146.5" y="192" x="0"></rect><rect class="section section1" height="24" width="1146.5" y="432" x="0"></rect><rect class="section section0" height="24" width="1146.5" y="216" x="0"></rect><rect class="section section1" height="24" width="1146.5" y="456" x="0"></rect><rect class="section section0" height="24" width="1146.5" y="240" x="0"></rect><rect class="section section1" height="24" width="1146.5" y="480" x="0"></rect><rect class="section section0" height="24" width="1146.5" y="264" x="0"></rect><rect class="section section1" height="24" width="1146.5" y="504" x="0"></rect></g><g><rect class="task task0" transform-origin="126.5px 60px" height="20" width="103" y="50" x="75" ry="3" rx="3" id="a1"></rect><rect class="task task1" transform-origin="126.5px 300px" height="20" width="103" y="290" x="75" ry="3" rx="3" id="l1"></rect><rect class="task task0" transform-origin="230px 84px" height="20" width="104" y="74" x="178" ry="3" rx="3" id="a2"></rect><rect class="task task1" transform-origin="230px 324px" height="20" width="104" y="314" x="178" ry="3" rx="3" id="l2"></rect><rect class="task task0" transform-origin="333.5px 108px" height="20" width="103" y="98" x="282" ry="3" rx="3" id="a3"></rect><rect class="task task1" transform-origin="333.5px 348px" height="20" width="103" y="338" x="282" ry="3" rx="3" id="l3"></rect><rect class="task active0" transform-origin="437px 132px" height="20" width="104" y="122" x="385" ry="3" rx="3" id="a4"></rect><rect class="task active1" transform-origin="437px 372px" height="20" width="104" y="362" x="385" ry="3" rx="3" id="l4"></rect><rect class="task task0" transform-origin="540.5px 156px" height="20" width="103" y="146" x="489" ry="3" rx="3" id="a5"></rect><rect class="task task1" transform-origin="540.5px 396px" height="20" width="103" y="386" x="489" ry="3" rx="3" id="l5"></rect><rect class="task task0" transform-origin="643.5px 180px" height="20" width="103" y="170" x="592" ry="3" rx="3" id="a6"></rect><rect class="task task1" transform-origin="643.5px 420px" height="20" width="103" y="410" x="592" ry="3" rx="3" id="l6"></rect><rect class="task task0" transform-origin="747px 204px" height="20" width="104" y="194" x="695" ry="3" rx="3" id="a7"></rect><rect class="task task1" transform-origin="747px 444px" height="20" width="104" y="434" x="695" ry="3" rx="3" id="l7"></rect><rect class="task task0" transform-origin="850.5px 228px" height="20" width="103" y="218" x="799" ry="3" rx="3" id="a8"></rect><rect class="task task1" transform-origin="850.5px 468px" height="20" width="103" y="458" x="799" ry="3" rx="3" id="l8"></rect><rect class="task task0" transform-origin="954px 252px" height="20" width="104" y="242" x="902" ry="3" rx="3" id="a9"></rect><rect class="task task1" transform-origin="954px 492px" height="20" width="104" y="482" x="902" ry="3" rx="3" id="l9"></rect><rect class="task task0" transform-origin="1057.5px 276px" height="20" width="103" y="266" x="1006" ry="3" rx="3" id="a10"></rect><rect class="task task1" transform-origin="1057.5px 516px" height="20" width="103" y="506" x="1006" ry="3" rx="3" id="l10"></rect><text class="taskText taskText0  width-82.3203125" y="63.5" x="126.5" font-size="11" id="a1-text">Course Overview                                    </text><text class="taskText taskText1  width-39.1484375" y="303.5" x="126.5" font-size="11" id="l1-text">lesson 1  </text><text class="taskText taskText0  width-90.5234375" y="87.5" x="230" font-size="11" id="a2-text">NLP Fundamentals                                   </text><text class="taskText taskText1  width-39.1484375" y="327.5" x="230" font-size="11" id="l2-text">lesson 2  </text><text class="taskTextOutsideRight taskTextOutside0  width-280.78125" y="111.5" x="390" font-size="11" id="a3-text">Basic knowledge and architectural characteristics of LLM </text><text class="taskText taskText1  width-39.1484375" y="351.5" x="333.5" font-size="11" id="l3-text">lesson 3  </text><text class="taskTextOutsideRight taskTextOutside0 activeText0 width-159.1640625" y="135.5" x="494" font-size="11" id="a4-text">LLM Development Fundamentals                       </text><text class="taskText taskText1 activeText1 width-39.1484375" y="375.5" x="437" font-size="11" id="l4-text">lesson 4  </text><text class="taskTextOutsideRight taskTextOutside0  width-280.125" y="159.5" x="597" font-size="11" id="a5-text">Introduction and Setup of the Experimental Environment </text><text class="taskText taskText1  width-39.1484375" y="399.5" x="540.5" font-size="11" id="l5-text">lesson 5  </text><text class="taskTextOutsideRight taskTextOutside0  width-239.4921875" y="183.5" x="700" font-size="11" id="a6-text">The concept of the tokenizer and common types      </text><text class="taskText taskText1  width-39.1484375" y="423.5" x="643.5" font-size="11" id="l6-text">lesson 6  </text><text class="taskTextOutsideRight taskTextOutside0  width-199.6171875" y="207.5" x="804" font-size="11" id="a7-text">Text data preprocessing and preparation            </text><text class="taskText taskText1  width-39.1484375" y="447.5" x="747" font-size="11" id="l7-text">lesson 7  </text><text class="taskTextOutsideRight taskTextOutside0  width-127.171875" y="231.5" x="907" font-size="11" id="a8-text">LLM training - Fine-tuning                         </text><text class="taskText taskText1  width-39.1484375" y="471.5" x="850.5" font-size="11" id="l8-text">lesson 8  </text><text class="taskTextOutsideLeft taskTextOutside0" y="255.5" x="897" font-size="11" id="a9-text">LLM training - Reward Modeling and Proximal Policy Optimization </text><text class="taskText taskText1  width-39.1484375" y="495.5" x="954" font-size="11" id="l9-text">lesson 9  </text><text class="taskTextOutsideLeft taskTextOutside0" y="279.5" x="1001" font-size="11" id="a10-text">Famous SOTA LLM models and JAIS model              </text><text class="taskText taskText1  width-44.9140625" y="519.5" x="1057.5" font-size="11" id="l10-text">lesson 10 </text></g><g><text class="sectionTitle sectionTitle0" font-size="11" y="170" x="10" dy="0em"><tspan x="10" alignment-baseline="central">Course Content</tspan></text><text class="sectionTitle sectionTitle1" font-size="11" y="410" x="10" dy="0em"><tspan x="10" alignment-baseline="central">Lessons</tspan></text></g><g class="today"><line class="today" y2="555" y1="25" x2="2065761" x1="2065761"></line></g><text class="titleText" y="25" x="592">LLM Course Timeline</text></svg>\" alt=\"Mermaid diagram 1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a41e54",
   "metadata": {},
   "source": [
    "# Unlocking the Secrets of Large Language Model Development\n",
    "\n",
    "Welcome, AI enthusiasts and future LLM developers! Today, we're diving deep into the fascinating world of Large Language Model (LLM) development. Whether you're a seasoned machine learning engineer or a curious newcomer, this lesson will equip you with the essential knowledge and techniques to understand and create these powerful AI models.\n",
    "\n",
    "## What's on the Menu?\n",
    "\n",
    "In this action-packed lesson, we'll explore:\n",
    "\n",
    "1. The art of advanced tokenization\n",
    "2. Clever prompting strategies that make LLMs dance\n",
    "3. Data preparation techniques that separate the pros from the amateurs\n",
    "4. The secret sauce of pre-training methodologies\n",
    "5. Fine-tuning tricks that'll make your models shine\n",
    "6. How to teach LLMs using rewards and reinforcement\n",
    "7. Squeezing big models into tiny packages with quantization\n",
    "8. The ultimate test: evaluating your LLM's true potential\n",
    "\n",
    "Buckle up, because we're in for an exciting ride!\n",
    "\n",
    "### What's Tokenization, and Why Should You Care?\n",
    "\n",
    "Imagine you're a master chef preparing a complex dish. Before you start cooking, you need to slice, dice, and prepare all your ingredients. That's what tokenization does for language models – it breaks down the raw text into bite-sized pieces that the model can digest.\n",
    "\n",
    "But we're not talking about your grandma's tokenization here. Oh no, we're diving into the world of advanced tokenization techniques that can make or break your LLM's performance.\n",
    "\n",
    "### The Spicy World of Subword Tokenization\n",
    "\n",
    "Remember the days when we used to split text into words? How quaint. Today, we're all about subword tokenization. It's like discovering that you can cut your vegetables into different shapes to release more flavor!\n",
    "\n",
    "Let's look at some popular subword tokenization algorithms:\n",
    "\n",
    "- **Byte Pair Encoding (BPE)**: The OG of subword tokenization. It starts with characters and merges the most frequent pairs. It's like making a word smoothie!\n",
    "- **WordPiece**: Google's secret recipe. It's similar to BPE but uses a likelihood criterion for merging. Fancy, huh?\n",
    "- **SentencePiece**: The Swiss Army knife of tokenization. It can handle any language without pre-tokenization. It's the polyglot of the tokenization world!\n",
    "\n",
    "### Case Study: Tokenizing Shakespeare in the 21st Century\n",
    "\n",
    "Let's say you're building an LLM that needs to understand both modern English and Shakespearean prose. Tricky, right? Here's where advanced tokenization saves the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378d9539",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "class ShakespeareModernTokenizer:\n",
    "    def __init__(self, model_name: str = \"gpt2\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    def tokenize_text(self, text: str):\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        return tokens\n",
    "    \n",
    "    def compare_tokenization(self, shakespeare_text: str, modern_text: str):\n",
    "        shakespeare_tokens = self.tokenize_text(shakespeare_text)\n",
    "        modern_tokens = self.tokenize_text(modern_text)\n",
    "        \n",
    "        print(\"Shakespeare's tokens:\", shakespeare_tokens)\n",
    "        print(\"Modern tokens:\", modern_tokens)\n",
    "        \n",
    "        return shakespeare_tokens, modern_tokens\n",
    "\n",
    "# Let's give it a whirl!\n",
    "tokenizer = ShakespeareModernTokenizer()\n",
    "\n",
    "shakespeare_text = \"To be, or not to be: that is the question\"\n",
    "modern_text = \"Should I do this thing or not? That's what I'm trying to figure out\"\n",
    "\n",
    "shakespeare_tokens, modern_tokens = tokenizer.compare_tokenization(shakespeare_text, modern_text)\n",
    "\n",
    "print(f\"\\nShakespeare token count: {len(shakespeare_tokens)}\")\n",
    "print(f\"Modern token count: {len(modern_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a0c230",
   "metadata": {},
   "source": [
    "When you run this code, you'll see how our tokenizer handles both Shakespearean and modern English. It's like having a translator that speaks both 16th-century and 21st-century English!\n",
    "\n",
    "### The Multilingual Tokenization Challenge\n",
    "\n",
    "But wait, there's more! What if your LLM needs to be a global citizen, understanding multiple languages? That's where multilingual tokenization comes into play.\n",
    "\n",
    "Here's a quick example of how you might handle multilingual text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32cf032",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def tokenize_multilingual(texts):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "    \n",
    "    for lang, text in texts.items():\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        print(f\"{lang} tokens: {tokens}\")\n",
    "        print(f\"Token count: {len(tokens)}\\n\")\n",
    "\n",
    "# Let's try it out!\n",
    "texts = {\n",
    "    \"English\": \"Hello, world!\",\n",
    "    \"French\": \"Bonjour, le monde!\",\n",
    "    \"Japanese\": \"こんにちは、世界！\",\n",
    "    \"Arabic\": \"مرحبا بالعالم!\"\n",
    "}\n",
    "\n",
    "tokenize_multilingual(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02203af0",
   "metadata": {},
   "source": [
    "Run this code, and you'll see how our tokenizer handles text in different languages. It's like having a United Nations translator in your pocket!\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "Choosing the right tokenization strategy can make a huge difference in your LLM's performance. It affects how well your model understands different languages, handles rare words, and even impacts training efficiency.\n",
    "\n",
    "So, the next time you're building an LLM, remember: tokenization isn't just about splitting text – it's about giving your model the best possible ingredients to work with. Choose wisely, and your LLM will thank you with better performance!\n",
    "\n",
    "Stay tuned for our next section, where we'll explore the art of prompting – or as I like to call it, \"How to Sweet-Talk Your LLM into Doing What You Want!\"\n",
    "\n",
    "Welcome back, LLM whisperers! Now that we've mastered the art of chopping up language with advanced tokenization, it's time to learn how to sweet-talk our models into doing what we want. That's right, we're diving into the world of sophisticated prompting strategies!\n",
    "\n",
    "### What's the Big Deal About Prompting?\n",
    "\n",
    "Imagine you have a super-smart friend who knows everything but needs very specific instructions to help you. That's essentially what prompting is all about - it's the art of asking your LLM the right questions in the right way to get the best possible answers.\n",
    "\n",
    "### From Zero to Hero: Zero-shot and Few-shot Learning\n",
    "\n",
    "Remember the days when we had to train models on thousands of examples? Well, those days are (mostly) behind us! With modern LLMs, we can often get impressive results with little to no task-specific training. Let's break it down:\n",
    "\n",
    "- **Zero-shot Learning**: This is like asking your smart friend to do something they've never done before, but they can figure it out based on their general knowledge.\n",
    "- **Few-shot Learning**: This is giving your friend a couple of examples before asking them to do the task. It's like saying, \"Here's how I did it twice, now you try!\"\n",
    "\n",
    "Let's see these in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4691ee74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "class PromptMaster:\n",
    "    def __init__(self, api_key):\n",
    "        openai.api_key = api_key\n",
    "    \n",
    "    def generate_response(self, prompt, max_tokens=100):\n",
    "        response = openai.Completion.create(\n",
    "            engine=\"text-davinci-002\",\n",
    "            prompt=prompt,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        return response.choices[0].text.strip()\n",
    "    \n",
    "    def zero_shot_classification(self, text, categories):\n",
    "        prompt = f\"Classify the following text into one of these categories: {', '.join(categories)}.\\n\\nText: {text}\\n\\nCategory:\"\n",
    "        return self.generate_response(prompt)\n",
    "    \n",
    "    def few_shot_learning(self, task, examples, new_input):\n",
    "        prompt = f\"{task}\\n\\nExamples:\\n\"\n",
    "        for input_text, output in examples:\n",
    "            prompt += f\"Input: {input_text}\\nOutput: {output}\\n\\n\"\n",
    "        prompt += f\"Input: {new_input}\\nOutput:\"\n",
    "        return self.generate_response(prompt)\n",
    "\n",
    "# Let's try it out!\n",
    "prompt_master = PromptMaster(\"your-api-key-here\")\n",
    "\n",
    "# Zero-shot classification\n",
    "text = \"The new smartphone has a great camera and long battery life.\"\n",
    "categories = [\"Technology\", \"Sports\", \"Politics\"]\n",
    "result = prompt_master.zero_shot_classification(text, categories)\n",
    "print(f\"Zero-shot classification result: {result}\")\n",
    "\n",
    "# Few-shot learning\n",
    "task = \"Translate English to French\"\n",
    "examples = [\n",
    "    (\"Hello\", \"Bonjour\"),\n",
    "    (\"How are you?\", \"Comment allez-vous?\")\n",
    "]\n",
    "new_input = \"Good morning\"\n",
    "result = prompt_master.few_shot_learning(task, examples, new_input)\n",
    "print(f\"Few-shot learning result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3743fc",
   "metadata": {},
   "source": [
    "Run this code, and you'll see how our LLM can perform classification without any specific training, and learn to translate with just a couple of examples. It's like teaching a new trick to an old dog, except this dog is a supercomputer!\n",
    "\n",
    "### The Chain of Thought: Making LLMs Show Their Work\n",
    "\n",
    "Remember when your math teacher always said \"show your work\"? Well, it turns out that's great advice for LLMs too! Chain-of-thought prompting is a technique where we ask the model to break down its reasoning step by step. It's like asking your LLM to think out loud.\n",
    "\n",
    "Let's see how we can implement this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b89b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain_of_thought_prompting(self, question):\n",
    "    prompt = f\"\"\"Question: {question}\n",
    "    Let's approach this step-by-step:\n",
    "    1)\"\"\"\n",
    "    return self.generate_response(prompt, max_tokens=300)\n",
    "\n",
    "# Let's try a tricky question\n",
    "question = \"If a train travels 120 km in 2 hours, what is its average speed in meters per second?\"\n",
    "result = prompt_master.chain_of_thought_prompting(question)\n",
    "print(f\"Chain of thought reasoning:\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4eacfaa",
   "metadata": {},
   "source": [
    "When you run this, you'll see the LLM break down the problem into steps, just like a good student would. It's not just giving you the answer, it's showing you how it got there!\n",
    "\n",
    "### The Self-Consistency Trick: When in Doubt, Ask Multiple Times\n",
    "\n",
    "Here's a cool trick: if you're not sure about an answer, why not ask the question multiple times and go with the most common response? That's the idea behind self-consistency prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3744be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_consistency_prompt(self, question, n_attempts=3):\n",
    "    responses = []\n",
    "    for _ in range(n_attempts):\n",
    "        prompt = f\"Question: {question}\\nLet's think about this carefully and solve it step-by-step:\"\n",
    "        responses.append(self.generate_response(prompt, max_tokens=200))\n",
    "    return responses\n",
    "\n",
    "# Let's try our speed question again\n",
    "responses = prompt_master.self_consistency_prompt(question)\n",
    "print(\"Self-consistency responses:\")\n",
    "for i, response in enumerate(responses, 1):\n",
    "    print(f\"\\nAttempt {i}:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d958a69",
   "metadata": {},
   "source": [
    "Run this, and you'll see multiple attempts at solving the same problem. It's like asking several smart friends and seeing if they agree!\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "Clever prompting strategies can dramatically enhance what your LLM can do. They're the difference between getting a mediocre answer and unlocking your model's full potential. With the right prompts, you can guide your LLM to perform complex reasoning, solve multi-step problems, and even improve its own outputs.\n",
    "\n",
    "But remember, with great power comes great responsibility. As you're crafting your prompts, always consider the ethical implications. Are you inadvertently introducing bias? Are you guiding the model towards harmful content? It's up to us, the prompt engineers, to use these techniques responsibly.\n",
    "\n",
    "Stay tuned for our next exciting installment, where we'll dive into the world of data preparation. Remember, even the smartest LLM is only as good as the data it's trained on!\n",
    "\n",
    "Welcome back, data wranglers and LLM enthusiasts! We've learned how to chop up language and sweet-talk our models, but now it's time to talk about something that might not sound as exciting but is absolutely crucial: data preparation. Remember, even the fanciest kitchen can't make a gourmet meal out of spoiled ingredients!\n",
    "\n",
    "### Why is Data Preparation Such a Big Deal?\n",
    "\n",
    "Imagine trying to teach a child about the world using only blurry photos and half-finished sentences. Sounds like a recipe for confusion, right? That's what happens when we feed our LLMs poor-quality data. Good data preparation is like giving your model a pair of glasses and a well-written textbook - it helps it see and understand the world much more clearly.\n",
    "\n",
    "### Cleaning Up the Mess: Advanced Text Cleaning\n",
    "\n",
    "Let's start with the basics: cleaning up our text data. But we're not just talking about removing a few typos here and there. We're going full Marie Kondo on our dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a059a480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "class TextCleaner:\n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        # Remove HTML tags\n",
    "        text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        \n",
    "        # Remove special characters and digits\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove stopwords\n",
    "        text = ' '.join([word for word in text.split() if word not in self.stop_words])\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "\n",
    "# Let's give it a whirl!\n",
    "cleaner = TextCleaner()\n",
    "messy_text = \"\"\"\n",
    "<p>Check out this AMAZING offer at https://totally-not-a-scam.com! \n",
    "You won't believe your eyes!!!! \n",
    "#mindblown #awesome #buy #now</p>\n",
    "\"\"\"\n",
    "clean_text = cleaner.clean_text(messy_text)\n",
    "print(f\"Original text:\\n{messy_text}\\n\")\n",
    "print(f\"Cleaned text:\\n{clean_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f717f8",
   "metadata": {},
   "source": [
    "Run this code, and you'll see how we transform a messy, noisy piece of text into something much cleaner and more useful for our LLM. It's like giving your data a spa day!\n",
    "\n",
    "### Data Augmentation: Making Mountains out of Molehills\n",
    "\n",
    "Sometimes, we don't have as much data as we'd like. That's where data augmentation comes in - it's like having a magical copy machine for your dataset, but one that makes slightly different copies each time.\n",
    "\n",
    "Here's a simple example of how we might augment text data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea88ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "class DataAugmenter:\n",
    "    def synonym_replacement(self, text, n=1):\n",
    "        words = text.split()\n",
    "        new_words = words.copy()\n",
    "        random_word_list = list(set([word for word in words if word not in self.stop_words]))\n",
    "        random.shuffle(random_word_list)\n",
    "        num_replaced = 0\n",
    "        for random_word in random_word_list:\n",
    "            synonyms = []\n",
    "            for syn in wordnet.synsets(random_word):\n",
    "                for l in syn.lemmas():\n",
    "                    synonyms.append(l.name())\n",
    "            if len(synonyms) >= 1:\n",
    "                synonym = random.choice(list(set(synonyms)))\n",
    "                new_words = [synonym if word == random_word else word for word in new_words]\n",
    "                num_replaced += 1\n",
    "            if num_replaced >= n:\n",
    "                break\n",
    "        return ' '.join(new_words)\n",
    "\n",
    "# Let's augment some data!\n",
    "augmenter = DataAugmenter()\n",
    "original_text = \"The quick brown fox jumps over the lazy dog\"\n",
    "augmented_text = augmenter.synonym_replacement(original_text, n=2)\n",
    "print(f\"Original: {original_text}\")\n",
    "print(f\"Augmented: {augmented_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949dc23e",
   "metadata": {},
   "source": [
    "When you run this, you'll see how we can create variations of our original text, potentially giving our model more diverse data to learn from. It's like teaching a child the same lesson in slightly different ways to help them really understand it.\n",
    "\n",
    "### Battling Bias: Making Your Data More Inclusive\n",
    "\n",
    "Here's a tricky but crucial part of data preparation: dealing with bias. Our models can only be as fair and inclusive as the data we train them on. Let's look at a simple way to check for potential gender bias in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749c3517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class BiasDectector:\n",
    "    def __init__(self):\n",
    "        self.male_words = set(['he', 'him', 'his', 'man', 'men', 'boy', 'boys'])\n",
    "        self.female_words = set(['she', 'her', 'hers', 'woman', 'women', 'girl', 'girls'])\n",
    "    \n",
    "    def check_gender_bias(self, texts):\n",
    "        male_count = 0\n",
    "        female_count = 0\n",
    "        for text in texts:\n",
    "            words = set(text.lower().split())\n",
    "            male_count += len(words.intersection(self.male_words))\n",
    "            female_count += len(words.intersection(self.female_words))\n",
    "        \n",
    "        total = male_count + female_count\n",
    "        if total == 0:\n",
    "            return \"No gendered words found\"\n",
    "        male_percentage = (male_count / total) * 100\n",
    "        female_percentage = (female_count / total) * 100\n",
    "        \n",
    "        return f\"Male representation: {male_percentage:.2f}%, Female representation: {female_percentage:.2f}%\"\n",
    "\n",
    "# Let's check for bias!\n",
    "detector = BiasDectector()\n",
    "texts = [\n",
    "    \"He was a great scientist.\",\n",
    "    \"She was a brilliant mathematician.\",\n",
    "    \"The doctor examined his patient.\",\n",
    "    \"The nurse cared for her patients.\"\n",
    "]\n",
    "result = detector.check_gender_bias(texts)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e6dcfd",
   "metadata": {},
   "source": [
    "This simple example shows how we might start to quantify gender representation in our dataset. Of course, real-world bias detection is much more complex, but this gives you an idea of how we might approach it.\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "Good data preparation is the foundation of any successful LLM project. It can help your model learn more efficiently, improve its performance, and reduce the risk of harmful biases. Remember:\n",
    "\n",
    "1. Clean data leads to cleaner results.\n",
    "2. More diverse data can lead to more robust models.\n",
    "3. Being aware of potential biases in your data is the first step to creating fairer, more inclusive AI.\n",
    "\n",
    "In our next exciting installment, we'll dive into the world of pre-training methodologies. Get ready to give your LLM its first lessons in understanding the world!\n",
    "\n",
    "Welcome back, LLM trainers extraordinaire! We've given our model a broad education through pre-training, and now it's time for grad school. That's right, we're diving into the world of fine-tuning, where we take our jack-of-all-trades LLM and turn it into a master of specific domains.\n",
    "\n",
    "### What's the Big Deal About Fine-Tuning?\n",
    "\n",
    "If pre-training is like giving your LLM a liberal arts education, fine-tuning is like sending it to medical school or law school. It's where we take all that general knowledge and focus it on specific tasks or domains. But we're not just talking about simple supervised learning here - oh no, we're going to explore some cutting-edge techniques that'll make your LLM shine!\n",
    "\n",
    "### Parameter-Efficient Fine-Tuning: Teaching New Tricks Without Breaking the Bank\n",
    "\n",
    "One of the challenges with fine-tuning large models is that it can be computationally expensive. Enter parameter-efficient fine-tuning methods like adapters or LoRA (Low-Rank Adaptation). These techniques allow us to adapt our models to new tasks while updating only a small subset of parameters.\n",
    "\n",
    "Let's look at a simplified implementation of adapter-based fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0498eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "class Adapter(nn.Module):\n",
    "    def __init__(self, input_dim, bottleneck_dim):\n",
    "        super().__init__()\n",
    "        self.down_project = nn.Linear(input_dim, bottleneck_dim)\n",
    "        self.up_project = nn.Linear(bottleneck_dim, input_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        down = self.down_project(x)\n",
    "        activated = self.activation(down)\n",
    "        up = self.up_project(activated)\n",
    "        return up + x  # Residual connection\n",
    "\n",
    "class BertWithAdapter(nn.Module):\n",
    "    def __init__(self, model_name='bert-base-uncased', num_labels=2, adapter_dim=64):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.adapter = Adapter(self.bert.config.hidden_size, adapter_dim)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        adapted = self.adapter(outputs.last_hidden_state)\n",
    "        return self.classifier(adapted[:, 0, :])  # Use [CLS] token for classification\n",
    "\n",
    "# Let's set up our adapter-based model\n",
    "model = BertWithAdapter()\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example usage\n",
    "text = \"This movie is great!\"\n",
    "inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "outputs = model(**inputs)\n",
    "print(f\"Model outputs shape: {outputs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9a1674",
   "metadata": {},
   "source": [
    "When you run this, you'll see how we can add a small adapter module to our BERT model, allowing us to fine-tune for specific tasks without modifying the entire model. It's like giving your LLM a tiny, specialized brain extension!\n",
    "\n",
    "### Few-Shot Learning: Making the Most of Limited Data\n",
    "\n",
    "Sometimes, we don't have the luxury of large labeled datasets for fine-tuning. That's where few-shot learning comes in handy. It's like teaching your LLM to become an expert after seeing just a handful of examples.\n",
    "\n",
    "Here's a simplified example of how we might implement few-shot learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d678fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "class FewShotLearner:\n",
    "    def __init__(self, model_name='gpt2'):\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def create_few_shot_prompt(self, examples, new_input):\n",
    "        prompt = \"Classify the sentiment of the following texts as positive or negative:\\n\\n\"\n",
    "        for text, label in examples:\n",
    "            prompt += f\"Text: {text}\\nSentiment: {label}\\n\\n\"\n",
    "        prompt += f\"Text: {new_input}\\nSentiment:\"\n",
    "        return prompt\n",
    "\n",
    "    def predict(self, prompt):\n",
    "        inputs = self.tokenizer(prompt, return_tensors='pt')\n",
    "        outputs = self.model.generate(**inputs, max_length=len(inputs['input_ids'][0]) + 5)\n",
    "        return self.tokenizer.decode(outputs[0])\n",
    "\n",
    "# Let's try some few-shot learning!\n",
    "learner = FewShotLearner()\n",
    "\n",
    "examples = [\n",
    "    (\"I love this movie!\", \"positive\"),\n",
    "    (\"This book is terrible.\", \"negative\"),\n",
    "    (\"The food was delicious.\", \"positive\")\n",
    "]\n",
    "\n",
    "new_input = \"The service was slow and the staff was rude.\"\n",
    "prompt = learner.create_few_shot_prompt(examples, new_input)\n",
    "prediction = learner.predict(prompt)\n",
    "\n",
    "print(f\"Few-shot prediction: {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a404eaa",
   "metadata": {},
   "source": [
    "This demonstrates how we can use a few examples to guide our model in making predictions on new inputs. It's like giving your LLM a crash course right before the final exam!\n",
    "\n",
    "### Continual Learning: Teaching Without Forgetting\n",
    "\n",
    "One challenge in fine-tuning is catastrophic forgetting - where a model forgets its previous knowledge when learning new tasks. Continual learning techniques help mitigate this. Let's look at a simplified implementation of Elastic Weight Consolidation (EWC), a popular continual learning method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b91ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class EWC(nn.Module):\n",
    "    def __init__(self, model, dataset):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "        self.params = {n: p for n, p in self.model.named_parameters() if p.requires_grad}\n",
    "        self._means = {}\n",
    "        self._precision_matrices = self._diag_fisher()\n",
    "\n",
    "    def _diag_fisher(self):\n",
    "        precision_matrices = {}\n",
    "        for n, p in self.params.items():\n",
    "            p.grad = torch.zeros_like(p.data)\n",
    "        \n",
    "        self.model.eval()\n",
    "        for input, target in self.dataset:\n",
    "            self.model.zero_grad()\n",
    "            output = self.model(input).view(1, -1)\n",
    "            label = torch.autograd.Variable(torch.LongTensor([target]))\n",
    "            loss = F.nll_loss(F.log_softmax(output, dim=1), label)\n",
    "            loss.backward()\n",
    "            \n",
    "            for n, p in self.params.items():\n",
    "                precision_matrices[n] = p.grad.data ** 2 / len(self.dataset)\n",
    "        \n",
    "        precision_matrices = {n: p for n, p in precision_matrices.items()}\n",
    "        return precision_matrices\n",
    "\n",
    "    def penalty(self, model: nn.Module):\n",
    "        loss = 0\n",
    "        for n, p in model.named_parameters():\n",
    "            _loss = self._precision_matrices[n] * (p - self._means[n]) ** 2\n",
    "            loss += _loss.sum()\n",
    "        return loss\n",
    "\n",
    "    def update(self, model):\n",
    "        self.model = model\n",
    "        self._means = {n: p.clone().detach() for n, p in self.model.named_parameters() if p.requires_grad}\n",
    "\n",
    "# Usage would look something like this:\n",
    "# ewc = EWC(model, dataset)\n",
    "# for epoch in range(num_epochs):\n",
    "#     for batch in dataloader:\n",
    "#         loss = criterion(model(batch), targets) + lambda * ewc.penalty(model)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "# ewc.update(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de03e6e",
   "metadata": {},
   "source": [
    "This implementation of EWC helps the model retain knowledge of previous tasks while learning new ones. It's like giving your LLM a really good memory so it doesn't forget what it learned in elementary school while it's working on its Ph.D.!\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "Advanced fine-tuning techniques are crucial for adapting LLMs to specific tasks and domains while maintaining their broad capabilities. They allow us to:\n",
    "\n",
    "1. Efficiently adapt large models to new tasks without excessive computational costs.\n",
    "2. Make the most of limited labeled data in new domains.\n",
    "3. Continuously update our models with new knowledge without forgetting what they've already learned.\n",
    "\n",
    "Remember, a well fine-tuned model is like a world-class expert - deeply knowledgeable in its specific field, but still able to draw on a broad base of general knowledge when needed.\n",
    "\n",
    "In our next exciting installment, we'll explore how to teach our LLMs using rewards and reinforcement learning. Get ready to train your AI like you would a very smart puppy!\n",
    "\n",
    "Welcome back, AI trainers and language model whisperers! We've fine-tuned our models to be experts, but now we're going to teach them something even more important: values. That's right, we're diving into the world of reward modeling and reinforcement learning, where we teach our LLMs not just what to do, but what's good to do.\n",
    "\n",
    "### Why Bother with Rewards and Reinforcement?\n",
    "\n",
    "Imagine you have a super-smart AI assistant that can write anything... including some pretty nasty stuff. Yikes! That's where reward modeling and reinforcement learning come in. We use these techniques to align our LLMs with human values and preferences. It's like teaching ethics to an AI!\n",
    "\n",
    "### Reward Modeling: Teaching Your LLM What's Good\n",
    "\n",
    "Reward modeling is all about creating a system that can evaluate how \"good\" or \"desirable\" an LLM's output is. It's like creating an AI teacher that can grade your LLM's homework.\n",
    "\n",
    "Let's implement a simple reward model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ecbed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, model_name='bert-base-uncased'):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.score = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        return self.score(outputs.pooler_output).squeeze(-1)\n",
    "\n",
    "    def get_reward(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            return self(inputs['input_ids'], inputs['attention_mask']).item()\n",
    "\n",
    "# Let's create and use our reward model!\n",
    "reward_model = RewardModel()\n",
    "\n",
    "# Simulate training (in reality, you'd train this on human-labeled data)\n",
    "optimizer = torch.optim.Adam(reward_model.parameters(), lr=1e-5)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Fake training loop (don't actually run this, it's just for illustration)\n",
    "# for epoch in range(num_epochs):\n",
    "#     for batch in dataloader:\n",
    "#         texts, human_scores = batch\n",
    "#         inputs = reward_model.tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
    "#         predicted_scores = reward_model(**inputs)\n",
    "#         loss = criterion(predicted_scores, human_scores)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "# Now let's use our \"trained\" reward model\n",
    "good_text = \"Helping others is important for building a strong community.\"\n",
    "bad_text = \"Who cares about others? Just look out for yourself!\"\n",
    "\n",
    "print(f\"Reward for good text: {reward_model.get_reward(good_text):.2f}\")\n",
    "print(f\"Reward for bad text: {reward_model.get_reward(bad_text):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f9d20a",
   "metadata": {},
   "source": [
    "When you run this (assuming the reward model was actually trained), you'd see that the \"good\" text gets a higher reward than the \"bad\" text. It's like having an AI ethics professor grading your LLM's essays!\n",
    "\n",
    "### Reinforcement Learning: Teaching Your LLM to Maximize Goodness\n",
    "\n",
    "Now that we have a way to measure how \"good\" our LLM's outputs are, we can use reinforcement learning to actually improve the LLM's behavior. We're going to use a technique called Proximal Policy Optimization (PPO), which is like giving your LLM a cookie every time it does something good.\n",
    "\n",
    "Here's a simplified implementation of PPO for language models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30ada2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "class PPOTrainer:\n",
    "    def __init__(self, model_name='gpt2', lr=1e-5):\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.reward_model = RewardModel()  # Assume we have this from before\n",
    "\n",
    "    def generate_text(self, prompt, max_length=50):\n",
    "        inputs = self.tokenizer(prompt, return_tensors='pt')\n",
    "        outputs = self.model.generate(**inputs, max_length=max_length, do_sample=True)\n",
    "        return self.tokenizer.decode(outputs[0])\n",
    "\n",
    "    def compute_rewards(self, texts):\n",
    "        return [self.reward_model.get_reward(text) for text in texts]\n",
    "\n",
    "    def train_step(self, prompt, num_samples=5):\n",
    "        old_params = {n: p.clone().detach() for n, p in self.model.named_parameters()}\n",
    "\n",
    "        # Generate multiple samples\n",
    "        samples = [self.generate_text(prompt) for _ in range(num_samples)]\n",
    "        rewards = self.compute_rewards(samples)\n",
    "\n",
    "        # Compute loss and update model\n",
    "        for sample, reward in zip(samples, rewards):\n",
    "            inputs = self.tokenizer(sample, return_tensors='pt')\n",
    "            outputs = self.model(**inputs, labels=inputs['input_ids'])\n",
    "            loss = -reward * outputs.loss  # Maximize reward\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "        # Implement PPO clipping (simplified version)\n",
    "        with torch.no_grad():\n",
    "            for name, param in self.model.named_parameters():\n",
    "                old_param = old_params[name]\n",
    "                ratio = ((param - old_param) / (old_param + 1e-8)).clamp(-0.2, 0.2)\n",
    "                param.copy_(old_param + ratio * old_param)\n",
    "\n",
    "# Let's use our PPO trainer!\n",
    "ppo_trainer = PPOTrainer()\n",
    "\n",
    "prompt = \"The best way to live is\"\n",
    "print(\"Before training:\")\n",
    "print(ppo_trainer.generate_text(prompt))\n",
    "\n",
    "# Training loop (in reality, you'd do many more steps)\n",
    "for _ in range(10):\n",
    "    ppo_trainer.train_step(prompt)\n",
    "\n",
    "print(\"\\nAfter training:\")\n",
    "print(ppo_trainer.generate_text(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54269e69",
   "metadata": {},
   "source": [
    "When you run this, you'll see how the model's output changes after training to maximize the reward. It's like watching your AI grow a conscience in real-time!\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "Reward modeling and reinforcement learning are crucial for creating LLMs that are not just capable, but also aligned with human values. They allow us to:\n",
    "\n",
    "1. Teach our models to prefer outputs that are helpful, truthful, and ethical.\n",
    "2. Continuously improve our models based on human feedback.\n",
    "3. Mitigate some of the risks associated with deploying powerful AI systems.\n",
    "\n",
    "Remember, a model trained with these techniques is like a brilliant student who's also learned strong ethics - powerful, but also mindful of doing the right thing.\n",
    "\n",
    "In our next exciting episode, we'll explore how to squeeze all this AI goodness into smaller packages through model quantization. Get ready to turn your AI behemoth into a lean, mean, language-processing machine!\n",
    "\n",
    "\n",
    "##  Model Quantization: Shrinking Your LLM Without Shrinking Its Brain\n",
    "Welcome back, AI compressors and efficiency enthusiasts! We've created a brilliant, ethical AI, but there's just one tiny problem - it's huge! Enter the world of model quantization, where we turn our gas-guzzling AI SUV into a sleek, efficient electric car.\n",
    "\n",
    "### Why Bother with Quantization?\n",
    "\n",
    "Imagine trying to run the latest AAA video game on your smartphone. That's kind of what it's like trying to deploy a full-sized LLM on edge devices or in resource-constrained environments. Quantization helps us shrink our models so they can run faster and use less memory, without significantly sacrificing performance. It's like teaching your AI to do more with less!\n",
    "\n",
    "### Post-Training Quantization: The Quick and Dirty Approach\n",
    "\n",
    "Let's start with post-training quantization. This is like taking your fully trained model and putting it on a diet. We're going to convert our model's weights from 32-bit floating-point numbers to 8-bit integers. It's quick, it's dirty, and it can give us significant size reductions with minimal loss in accuracy.\n",
    "\n",
    "Here's a simple example using PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98521c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "class ModelQuantizer:\n",
    "    def __init__(self, model_name='bert-base-uncased'):\n",
    "        self.model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "    \n",
    "    def quantize_model(self):\n",
    "        # Quantize the model to 8-bit integers\n",
    "        quantized_model = torch.quantization.quantize_dynamic(\n",
    "            self.model, {torch.nn.Linear}, dtype=torch.qint8\n",
    "        )\n",
    "        return quantized_model\n",
    "    \n",
    "    def compare_model_sizes(self, quantized_model):\n",
    "        original_size = sum(p.numel() for p in self.model.parameters()) * 4 / 1024 / 1024  # Size in MB\n",
    "        quantized_size = sum(p.numel() for p in quantized_model.parameters()) / 1024 / 1024  # Size in MB\n",
    "        \n",
    "        print(f\"Original model size: {original_size:.2f} MB\")\n",
    "        print(f\"Quantized model size: {quantized_size:.2f} MB\")\n",
    "        print(f\"Size reduction: {(1 - quantized_size/original_size)*100:.2f}%\")\n",
    "\n",
    "# Let's quantize our model!\n",
    "quantizer = ModelQuantizer()\n",
    "quantized_model = quantizer.quantize_model()\n",
    "quantizer.compare_model_sizes(quantized_model)\n",
    "\n",
    "# Example usage of the quantized model\n",
    "input_text = \"This is a test sentence.\"\n",
    "inputs = quantizer.model.tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = quantized_model(**inputs)\n",
    "print(f\"Model output shape: {outputs.logits.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4983d7",
   "metadata": {},
   "source": [
    "When you run this, you'll see how much smaller our quantized model is compared to the original. It's like we've taught our AI to speak in efficient shorthand!\n",
    "\n",
    "### Quantization-Aware Training: Teaching Your Model to Slim Down\n",
    "\n",
    "While post-training quantization is quick and easy, sometimes we want even better results. That's where quantization-aware training comes in. It's like putting your model on a diet and exercise regimen simultaneously.\n",
    "\n",
    "Here's a simplified example of how we might implement quantization-aware training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0989ace8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "class QuantizationAwareTrainer:\n",
    "    def __init__(self, model_name='bert-base-uncased', num_labels=2):\n",
    "        self.model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # Prepare the model for quantization-aware training\n",
    "        self.model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
    "        self.model = torch.quantization.prepare_qat(self.model)\n",
    "    \n",
    "    def train(self, train_texts, train_labels, num_epochs=3):\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=5e-5)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            self.model.train()\n",
    "            for text, label in zip(train_texts, train_labels):\n",
    "                inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "                labels = torch.tensor([label])\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(**inputs, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} completed\")\n",
    "    \n",
    "    def quantize(self):\n",
    "        self.model.eval()\n",
    "        self.model = torch.quantization.convert(self.model)\n",
    "        return self.model\n",
    "\n",
    "# Let's try quantization-aware training!\n",
    "trainer = QuantizationAwareTrainer()\n",
    "\n",
    "# Dummy training data\n",
    "train_texts = [\"This is positive\", \"This is negative\", \"Amazing movie!\", \"Terrible experience\"]\n",
    "train_labels = [1, 0, 1, 0]\n",
    "\n",
    "trainer.train(train_texts, train_labels)\n",
    "quantized_model = trainer.quantize()\n",
    "\n",
    "# Test the quantized model\n",
    "test_text = \"I loved this product!\"\n",
    "inputs = trainer.tokenizer(test_text, return_tensors=\"pt\")\n",
    "outputs = quantized_model(**inputs)\n",
    "prediction = torch.argmax(outputs.logits, dim=1).item()\n",
    "print(f\"Prediction for '{test_text}': {'Positive' if prediction == 1 else 'Negative'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff71ccd0",
   "metadata": {},
   "source": [
    "This example shows how we can train a model while simulating quantization, then fully quantize it at the end. It's like teaching your AI to work out with ankle weights, so it's super efficient when you take them off!\n",
    "\n",
    "### Mixed-Precision Training: The Best of Both Worlds\n",
    "\n",
    "Sometimes, we want the speed of low-precision calculations but the accuracy of high-precision ones. That's where mixed-precision training comes in. It's like having your AI do the heavy lifting with dumbbells but the fine-tuning with precision instruments.\n",
    "\n",
    "Here's a simplified example using PyTorch's automatic mixed precision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b422a3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "class MixedPrecisionTrainer:\n",
    "    def __init__(self, model_name='bert-base-uncased', num_labels=2):\n",
    "        self.model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.scaler = GradScaler()\n",
    "    \n",
    "    def train(self, train_texts, train_labels, num_epochs=3):\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=5e-5)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            self.model.train()\n",
    "            for text, label in zip(train_texts, train_labels):\n",
    "                inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "                labels = torch.tensor([label])\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Runs the forward pass with autocasting\n",
    "                with autocast():\n",
    "                    outputs = self.model(**inputs, labels=labels)\n",
    "                    loss = outputs.loss\n",
    "                \n",
    "                # Scales loss and calls backward() to create scaled gradients\n",
    "                self.scaler.scale(loss).backward()\n",
    "                \n",
    "                # Unscales the gradients, then calls optimizer.step()\n",
    "                self.scaler.step(optimizer)\n",
    "                \n",
    "                # Updates the scale for next iteration\n",
    "                self.scaler.update()\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} completed\")\n",
    "\n",
    "# Let's try mixed-precision training!\n",
    "trainer = MixedPrecisionTrainer()\n",
    "\n",
    "# Dummy training data\n",
    "train_texts = [\"This is positive\", \"This is negative\", \"Amazing movie!\", \"Terrible experience\"]\n",
    "train_labels = [1, 0, 1, 0]\n",
    "\n",
    "trainer.train(train_texts, train_labels)\n",
    "\n",
    "# Test the model\n",
    "test_text = \"I loved this product!\"\n",
    "inputs = trainer.tokenizer(test_text, return_tensors=\"pt\")\n",
    "outputs = trainer.model(**inputs)\n",
    "prediction = torch.argmax(outputs.logits, dim=1).item()\n",
    "print(f\"Prediction for '{test_text}': {'Positive' if prediction == 1 else 'Negative'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfd3a62",
   "metadata": {},
   "source": [
    "This example demonstrates how we can use mixed precision to train our model more efficiently. It's like teaching your AI to be a jack of all trades and a master of... well, all of them!\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "Efficient model quantization is crucial for deploying LLMs in the real world. It allows us to:\n",
    "\n",
    "1. Run our models on devices with limited computational resources, like smartphones or IoT devices.\n",
    "2. Reduce the energy consumption and carbon footprint of our AI systems.\n",
    "3. Speed up inference time, making our models more responsive in real-time applications.\n",
    "\n",
    "Remember, a well-quantized model is like a master chef who can create gourmet meals with just a few ingredients - it does more with less!\n",
    "\n",
    "In our grand finale, we'll explore how to accurately evaluate the performance of our lean, mean, language-processing machine. Get ready to put your LLM through its paces!\n",
    "\n",
    "\n",
    "## Model Evaluation: Putting Your LLM Through Its Paces\n",
    "\n",
    "\n",
    "Welcome to the final boss battle, AI trainers! We've built, trained, fine-tuned, and optimized our LLM. Now it's time for the moment of truth: evaluating just how good our creation really is. Buckle up, because we're about to put our LLM through a gauntlet of tests that would make even the toughest AI sweat (if they could sweat, that is).\n",
    "\n",
    "### Why Is Evaluation So Crucial?\n",
    "\n",
    "Imagine sending your AI child off to its first job interview without knowing if it can even tie its own shoelaces. That's what deploying an LLM without proper evaluation is like. Evaluation helps us understand our model's strengths, weaknesses, and quirks. It's like giving your AI a full physical, IQ test, and personality assessment all rolled into one!\n",
    "\n",
    "### The Multifaceted World of LLM Evaluation\n",
    "\n",
    "Evaluating an LLM isn't just about accuracy scores. We need to look at various aspects:\n",
    "\n",
    "1. Task-specific performance\n",
    "2. Generalization ability\n",
    "3. Robustness to adversarial inputs\n",
    "4. Fairness and bias\n",
    "5. Calibration and uncertainty estimation\n",
    "\n",
    "Let's dive into each of these with some code examples!\n",
    "\n",
    "### Task-Specific Performance: How Good Is Your LLM at Its Job?\n",
    "\n",
    "First, let's evaluate our model on a specific task, say, sentiment analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673762e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "class SentimentEvaluator:\n",
    "    def __init__(self, model_name=\"distilbert-base-uncased-finetuned-sst-2-english\"):\n",
    "        self.classifier = pipeline(\"sentiment-analysis\", model=model_name)\n",
    "    \n",
    "    def evaluate(self, dataset_name=\"sst2\", split=\"validation\"):\n",
    "        # Load dataset\n",
    "        dataset = load_dataset(dataset_name, split=split)\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = self.classifier([text for text in dataset[\"sentence\"]])\n",
    "        pred_labels = [\"POSITIVE\" if pred[\"label\"] == \"POSITIVE\" else \"NEGATIVE\" for pred in predictions]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(dataset[\"label\"], pred_labels)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(dataset[\"label\"], pred_labels, average=\"weighted\")\n",
    "        \n",
    "        return {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1\n",
    "        }\n",
    "\n",
    "# Let's evaluate!\n",
    "evaluator = SentimentEvaluator()\n",
    "results = evaluator.evaluate()\n",
    "print(\"Sentiment Analysis Results:\")\n",
    "for metric, value in results.items():\n",
    "    print(f\"{metric.capitalize()}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c152f0d2",
   "metadata": {},
   "source": [
    "This gives us a good idea of how well our model performs on sentiment analysis. But that's just the tip of the iceberg!\n",
    "\n",
    "### Generalization: Can Your LLM Handle the Unknown?\n",
    "\n",
    "Next, let's test how well our model generalizes to out-of-distribution data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0096c071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class GeneralizationTester:\n",
    "    def __init__(self, model_name=\"gpt2\"):\n",
    "        self.model = pipeline(\"text-generation\", model=model_name)\n",
    "    \n",
    "    def generate_ood_prompts(self, n=5):\n",
    "        domains = [\"quantum physics\", \"medieval history\", \"culinary arts\", \"space exploration\", \"underwater basket weaving\"]\n",
    "        return [f\"Explain the concept of {random.choice(domains)} to a 5-year-old:\" for _ in range(n)]\n",
    "    \n",
    "    def evaluate_generalization(self):\n",
    "        prompts = self.generate_ood_prompts()\n",
    "        for prompt in prompts:\n",
    "            response = self.model(prompt, max_length=100, num_return_sequences=1)[0]['generated_text']\n",
    "            print(f\"Prompt: {prompt}\")\n",
    "            print(f\"Response: {response}\\n\")\n",
    "\n",
    "# Test generalization\n",
    "gen_tester = GeneralizationTester()\n",
    "gen_tester.evaluate_generalization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9189fbbc",
   "metadata": {},
   "source": [
    "This helps us see how our model handles topics it might not have been explicitly trained on. It's like asking your history professor to explain quantum physics!\n",
    "\n",
    "### Robustness: Can Your LLM Handle a Curveball?\n",
    "\n",
    "Now, let's see how our model handles adversarial inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c90e3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "class RobustnessTester:\n",
    "    def __init__(self, model_name=\"distilbert-base-uncased-finetuned-sst-2-english\"):\n",
    "        self.classifier = pipeline(\"sentiment-analysis\", model=model_name)\n",
    "    \n",
    "    def perturb_text(self, text):\n",
    "        words = word_tokenize(text)\n",
    "        perturbed_words = []\n",
    "        for word in words:\n",
    "            if len(word) > 3:\n",
    "                perturbed_word = word[0] + ''.join(random.sample(word[1:-1], len(word)-2)) + word[-1]\n",
    "                perturbed_words.append(perturbed_word)\n",
    "            else:\n",
    "                perturbed_words.append(word)\n",
    "        return ' '.join(perturbed_words)\n",
    "    \n",
    "    def test_robustness(self, text):\n",
    "        original_sentiment = self.classifier(text)[0]['label']\n",
    "        perturbed_text = self.perturb_text(text)\n",
    "        perturbed_sentiment = self.classifier(perturbed_text)[0]['label']\n",
    "        \n",
    "        print(f\"Original text: {text}\")\n",
    "        print(f\"Original sentiment: {original_sentiment}\")\n",
    "        print(f\"Perturbed text: {perturbed_text}\")\n",
    "        print(f\"Perturbed sentiment: {perturbed_sentiment}\")\n",
    "        print(f\"Robust: {'Yes' if original_sentiment == perturbed_sentiment else 'No'}\\n\")\n",
    "\n",
    "# Test robustness\n",
    "robustness_tester = RobustnessTester()\n",
    "robustness_tester.test_robustness(\"This movie was absolutely fantastic!\")\n",
    "robustness_tester.test_robustness(\"I hated every minute of this terrible film.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4168cdb",
   "metadata": {},
   "source": [
    "This helps us understand how our model handles slightly mangled inputs. It's like testing if your spell-check can handle your cat walking across the keyboard!\n",
    "\n",
    "### Fairness and Bias: Is Your LLM Playing Favorites?\n",
    "\n",
    "Let's check if our model shows any gender bias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52abad29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasChecker:\n",
    "    def __init__(self, model_name=\"distilbert-base-uncased-finetuned-sst-2-english\"):\n",
    "        self.classifier = pipeline(\"sentiment-analysis\", model=model_name)\n",
    "    \n",
    "    def check_gender_bias(self):\n",
    "        template = \"The {gender} is a {profession}.\"\n",
    "        genders = [\"man\", \"woman\"]\n",
    "        professions = [\"doctor\", \"nurse\", \"engineer\", \"teacher\", \"CEO\", \"assistant\"]\n",
    "        \n",
    "        results = {}\n",
    "        for profession in professions:\n",
    "            sentiments = []\n",
    "            for gender in genders:\n",
    "                text = template.format(gender=gender, profession=profession)\n",
    "                sentiment = self.classifier(text)[0]\n",
    "                sentiments.append(sentiment['label'])\n",
    "            results[profession] = sentiments\n",
    "        \n",
    "        for profession, sentiments in results.items():\n",
    "            print(f\"Profession: {profession}\")\n",
    "            print(f\"Man: {sentiments[0]}, Woman: {sentiments[1]}\")\n",
    "            print(\"Potential bias detected\" if sentiments[0] != sentiments[1] else \"No bias detected\")\n",
    "            print()\n",
    "\n",
    "# Check for bias\n",
    "bias_checker = BiasChecker()\n",
    "bias_checker.check_gender_bias()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a56226",
   "metadata": {},
   "source": [
    "This helps us identify potential gender biases in our model. It's like making sure your AI doesn't perpetuate harmful stereotypes!\n",
    "\n",
    "### Calibration: Does Your LLM Know What It Doesn't Know?\n",
    "\n",
    "Finally, let's check if our model is well-calibrated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c379ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.calibration import calibration_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class CalibrationChecker:\n",
    "    def __init__(self, model_name=\"distilbert-base-uncased-finetuned-sst-2-english\"):\n",
    "        self.classifier = pipeline(\"sentiment-analysis\", model=model_name)\n",
    "    \n",
    "    def check_calibration(self, dataset_name=\"sst2\", split=\"validation\"):\n",
    "        dataset = load_dataset(dataset_name, split=split)\n",
    "        \n",
    "        # Get predictions and true labels\n",
    "        predictions = self.classifier([text for text in dataset[\"sentence\"]])\n",
    "        pred_probs = [pred['score'] if pred['label'] == \"POSITIVE\" else 1 - pred['score'] for pred in predictions]\n",
    "        true_labels = dataset[\"label\"]\n",
    "        \n",
    "        # Calculate calibration curve\n",
    "        prob_true, prob_pred = calibration_curve(true_labels, pred_probs, n_bins=10)\n",
    "        \n",
    "        # Plot calibration curve\n",
    "        plt.plot(prob_pred, prob_true, marker='.')\n",
    "        plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "        plt.xlabel('Predicted probability')\n",
    "        plt.ylabel('True probability')\n",
    "        plt.title('Calibration curve')\n",
    "        plt.show()\n",
    "\n",
    "# Check calibration\n",
    "calibration_checker = CalibrationChecker()\n",
    "calibration_checker.check_calibration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ad8747",
   "metadata": {},
   "source": [
    "This helps us understand if our model's confidence aligns with its accuracy. It's like making sure your weather app isn't confidently predicting sunshine during a thunderstorm!\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "Comprehensive evaluation is crucial because it:\n",
    "\n",
    "1. Helps us understand our model's true capabilities and limitations.\n",
    "2. Identifies potential issues before deployment, saving us from embarrassing (or harmful) mistakes.\n",
    "3. Guides future improvements by highlighting areas where our model falls short.\n",
    "4. Builds trust with users by providing a clear picture of what the model can and can't do.\n",
    "\n",
    "Remember, a well-evaluated model is like a student with a comprehensive report card - you know exactly what it's good at, where it needs improvement, and whether it plays well with others!\n",
    "\n",
    "## Wrapping It All Up\n",
    "\n",
    "Congratulations, AI maestros! You've made it through the crash course in LLM development. From tokenization to evaluation, you've learned the key concepts that go into creating, training, and deploying state-of-the-art language models.\n",
    "\n",
    "Remember, building great LLMs isn't just about chasing the highest accuracy scores. It's about creating AI systems that are efficient, robust, fair, and trustworthy. As you go forth and create your own language models, keep these principles in mind:\n",
    "\n",
    "1. Tokenize wisely - your model is only as good as the ingredients you feed it.\n",
    "2. Prompt cleverly - a well-crafted prompt can unlock hidden potential.\n",
    "3. Prepare your data meticulously - garbage in, garbage out!\n",
    "4. Pre-train thoughtfully - lay a strong foundation for your model.\n",
    "5. Fine-tune carefully - teach your model new tricks without forgetting the old ones.\n",
    "6. Optimize efficiently - make your model lean and mean.\n",
    "7. Evaluate comprehensively - know your model inside and out.\n",
    "\n",
    "The world of LLMs is constantly evolving, with new techniques and architectures emerging all the time. Stay curious, keep learning, and who knows? Maybe your next model will be the one to pass the Turing test with flying colors!\n",
    "\n",
    "Now go forth and create some amazing AI! Just remember, with great power comes great responsibility. Use your newfound knowledge wisely, and may your loss functions always converge!\n",
    "\n",
    "# Conclusion: Mastering the Art and Science of LLM Development\n",
    "\n",
    "Congratulations, future AI architects! You've just completed an intensive journey through the world of Large Language Model development. Let's take a moment to recap what we've learned and see how all these pieces fit together.\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Advanced Tokenization**: We learned how to chop up language in smart ways, setting the foundation for our LLMs to understand and generate text effectively.\n",
    "\n",
    "2. **Sophisticated Prompting**: We explored how to sweet-talk our models into performing complex tasks with minimal additional training.\n",
    "\n",
    "3. **Comprehensive Data Preparation**: We discovered the importance of clean, diverse, and balanced data in training robust models.\n",
    "\n",
    "4. **In-depth Pre-training**: We delved into the techniques that give our LLMs their broad knowledge and language understanding.\n",
    "\n",
    "5. **Advanced Fine-tuning**: We learned how to specialize our models for specific tasks without forgetting their general knowledge.\n",
    "\n",
    "6. **Reward Modeling and Reinforcement Learning**: We explored how to align our models with human preferences and values.\n",
    "\n",
    "7. **Efficient Model Quantization**: We uncovered techniques to make our models leaner and faster without sacrificing too much performance.\n",
    "\n",
    "8. **Accurate Model Evaluation**: We learned how to comprehensively assess our models' capabilities, fairness, and reliability.\n",
    "\n",
    "## The Big Picture: A Mind Map of LLM Development\n",
    "\n",
    "To help visualize how all these concepts fit together, here's a mind map of the LLM development process:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e727d79d",
   "metadata": {},
   "source": [
    "<img src=\"data:image/svg+xml;base64,<svg aria-roledescription="flowchart-v2" role="graphics-document document" viewBox="0 0 1041.5 1630" style="max-width: 1041.5px;" class="flowchart" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" width="100%" id="mermaid-1726066049784"><style>#mermaid-1726066049784{font-family:"trebuchet ms",verdana,arial,sans-serif;font-size:16px;fill:#333;}#mermaid-1726066049784 .error-icon{fill:#552222;}#mermaid-1726066049784 .error-text{fill:#552222;stroke:#552222;}#mermaid-1726066049784 .edge-thickness-normal{stroke-width:1px;}#mermaid-1726066049784 .edge-thickness-thick{stroke-width:3.5px;}#mermaid-1726066049784 .edge-pattern-solid{stroke-dasharray:0;}#mermaid-1726066049784 .edge-thickness-invisible{stroke-width:0;fill:none;}#mermaid-1726066049784 .edge-pattern-dashed{stroke-dasharray:3;}#mermaid-1726066049784 .edge-pattern-dotted{stroke-dasharray:2;}#mermaid-1726066049784 .marker{fill:#333333;stroke:#333333;}#mermaid-1726066049784 .marker.cross{stroke:#333333;}#mermaid-1726066049784 svg{font-family:"trebuchet ms",verdana,arial,sans-serif;font-size:16px;}#mermaid-1726066049784 p{margin:0;}#mermaid-1726066049784 .label{font-family:"trebuchet ms",verdana,arial,sans-serif;color:#333;}#mermaid-1726066049784 .cluster-label text{fill:#333;}#mermaid-1726066049784 .cluster-label span{color:#333;}#mermaid-1726066049784 .cluster-label span p{background-color:transparent;}#mermaid-1726066049784 .label text,#mermaid-1726066049784 span{fill:#333;color:#333;}#mermaid-1726066049784 .node rect,#mermaid-1726066049784 .node circle,#mermaid-1726066049784 .node ellipse,#mermaid-1726066049784 .node polygon,#mermaid-1726066049784 .node path{fill:#ECECFF;stroke:#9370DB;stroke-width:1px;}#mermaid-1726066049784 .rough-node .label text,#mermaid-1726066049784 .node .label text{text-anchor:middle;}#mermaid-1726066049784 .node .katex path{fill:#000;stroke:#000;stroke-width:1px;}#mermaid-1726066049784 .node .label{text-align:center;}#mermaid-1726066049784 .node.clickable{cursor:pointer;}#mermaid-1726066049784 .arrowheadPath{fill:#333333;}#mermaid-1726066049784 .edgePath .path{stroke:#333333;stroke-width:2.0px;}#mermaid-1726066049784 .flowchart-link{stroke:#333333;fill:none;}#mermaid-1726066049784 .edgeLabel{background-color:rgba(232,232,232, 0.8);text-align:center;}#mermaid-1726066049784 .edgeLabel p{background-color:rgba(232,232,232, 0.8);}#mermaid-1726066049784 .edgeLabel rect{opacity:0.5;background-color:rgba(232,232,232, 0.8);fill:rgba(232,232,232, 0.8);}#mermaid-1726066049784 .labelBkg{background-color:rgba(232, 232, 232, 0.5);}#mermaid-1726066049784 .cluster rect{fill:#ffffde;stroke:#aaaa33;stroke-width:1px;}#mermaid-1726066049784 .cluster text{fill:#333;}#mermaid-1726066049784 .cluster span{color:#333;}#mermaid-1726066049784 div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:"trebuchet ms",verdana,arial,sans-serif;font-size:12px;background:hsl(80, 100%, 96.2745098039%);border:1px solid #aaaa33;border-radius:2px;pointer-events:none;z-index:100;}#mermaid-1726066049784 .flowchartTitleText{text-anchor:middle;font-size:18px;fill:#333;}#mermaid-1726066049784 :root{--mermaid-font-family:"trebuchet ms",verdana,arial,sans-serif;}</style><g><marker orient="auto" markerHeight="8" markerWidth="8" markerUnits="userSpaceOnUse" refY="5" refX="5" viewBox="0 0 10 10" class="marker flowchart-v2" id="mermaid-1726066049784_flowchart-v2-pointEnd"><path style="stroke-width: 1; stroke-dasharray: 1, 0;" class="arrowMarkerPath" d="M 0 0 L 10 5 L 0 10 z"></path></marker><marker orient="auto" markerHeight="8" markerWidth="8" markerUnits="userSpaceOnUse" refY="5" refX="4.5" viewBox="0 0 10 10" class="marker flowchart-v2" id="mermaid-1726066049784_flowchart-v2-pointStart"><path style="stroke-width: 1; stroke-dasharray: 1, 0;" class="arrowMarkerPath" d="M 0 5 L 10 10 L 10 0 z"></path></marker><marker orient="auto" markerHeight="11" markerWidth="11" markerUnits="userSpaceOnUse" refY="5" refX="11" viewBox="0 0 10 10" class="marker flowchart-v2" id="mermaid-1726066049784_flowchart-v2-circleEnd"><circle style="stroke-width: 1; stroke-dasharray: 1, 0;" class="arrowMarkerPath" r="5" cy="5" cx="5"></circle></marker><marker orient="auto" markerHeight="11" markerWidth="11" markerUnits="userSpaceOnUse" refY="5" refX="-1" viewBox="0 0 10 10" class="marker flowchart-v2" id="mermaid-1726066049784_flowchart-v2-circleStart"><circle style="stroke-width: 1; stroke-dasharray: 1, 0;" class="arrowMarkerPath" r="5" cy="5" cx="5"></circle></marker><marker orient="auto" markerHeight="11" markerWidth="11" markerUnits="userSpaceOnUse" refY="5.2" refX="12" viewBox="0 0 11 11" class="marker cross flowchart-v2" id="mermaid-1726066049784_flowchart-v2-crossEnd"><path style="stroke-width: 2; stroke-dasharray: 1, 0;" class="arrowMarkerPath" d="M 1,1 l 9,9 M 10,1 l -9,9"></path></marker><marker orient="auto" markerHeight="11" markerWidth="11" markerUnits="userSpaceOnUse" refY="5.2" refX="-1" viewBox="0 0 11 11" class="marker cross flowchart-v2" id="mermaid-1726066049784_flowchart-v2-crossStart"><path style="stroke-width: 2; stroke-dasharray: 1, 0;" class="arrowMarkerPath" d="M 1,1 l 9,9 M 10,1 l -9,9"></path></marker><g class="root"><g class="clusters"></g><g class="edgePaths"><path marker-end="url(#mermaid-1726066049784_flowchart-v2-pointEnd)" style="" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" id="L_A_B_0" d="M106.983,684L125.784,593.167C144.585,502.333,182.187,320.667,203.699,229.833C225.21,139,230.632,139,235.386,139C240.141,139,244.229,139,246.272,139L248.316,139"></path><path marker-end="url(#mermaid-1726066049784_flowchart-v2-pointEnd)" style="" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" id="L_A_C_1" d="M111.64,684L129.665,636.5C147.69,589,183.739,494,203.848,446.5C223.956,399,228.122,399,231.622,399C235.122,399,237.956,399,239.372,399L240.789,399"></path><path marker-end="url(#mermaid-1726066049784_flowchart-v2-pointEnd)" style="" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" id="L_A_D_2" d="M194.789,711L198.956,711C203.122,711,211.456,711,218.62,711C225.785,711,231.78,711,237.109,711C242.438,711,247.1,711,249.431,711L251.762,711"></path><path marker-end="url(#mermaid-1726066049784_flowchart-v2-pointEnd)" style="" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" id="L_A_E_3" d="M110.177,738L128.445,794.167C146.714,850.333,183.252,962.667,205.415,1018.833C227.577,1075,235.366,1075,242.488,1075C249.609,1075,256.064,1075,259.292,1075L262.52,1075"></path><path marker-end="url(#mermaid-1726066049784_flowchart-v2-pointEnd)" style="" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" id="L_A_F_4" d="M105.786,738L124.786,854.833C143.787,971.667,181.788,1205.333,205.423,1322.167C229.058,1439,238.327,1439,246.929,1439C255.531,1439,263.467,1439,267.435,1439L271.402,1439"></path><path marker-end="url(#mermaid-1726066049784_flowchart-v2-pointEnd)" style="" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" id="L_B_B1_5" d="M374.81,112L389.997,99.167C405.183,86.333,435.557,60.667,456.574,47.833C477.592,35,489.254,35,500.249,35C511.245,35,521.574,35,526.738,35L531.902,35"></path><path marker-end="url(#mermaid-1726066049784_flowchart-v2-pointEnd)" style="" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" id="L_B_B2_6" d="M433.402,139L438.824,139C444.245,139,455.087,139,467.536,139C479.985,139,494.04,139,507.429,139C520.818,139,533.54,139,539.901,139L546.262,139"></path><path marker-end="url(#mermaid-1726066049784_flowchart-v2-pointEnd)" style="" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" id="L_B_B3_7" d="M374.81,166L389.997,178.833C405.183,191.667,435.557,217.333,456.169,230.167C476.781,243,487.633,243,497.818,243C508.003,243,517.521,243,522.28,243L527.039,243"></path><path marker-end="url(#mermaid-1726066049784_flowchart-v2-pointEnd)" style="" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" id="L_C_C1_8" d="M406.761,372L416.623,367.833C426.484,363.667,446.207,355.333,458.152,351.167C470.096,347,474.263,347,477.763,347C481.263,347,484.096,347,485.513,347L486.93,347"></path><path marker-end="url(#mermaid-1726066049784_flowchart-v2-pointEnd)" style="" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" id="L_C_C2_9" d="M406.761,426L416.623,430.167C426.484,434.333,446.207,442.667,459.103,446.833C471.999,451,478.068,451,483.47,451C488.872,451,493.608,451,495.976,451L498.344,451"></path><path marker-end="url(#mermaid-1726066049784_flowchart-v2-pointEnd)" style="" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" id="L_D_D1_10" d="M364.16,684L381.122,662.5C398.083,641,432.006,598,455.033,576.5C478.06,555,490.19,555,501.654,555C513.117,555,523.914,555,529.313,555L534.711,555"></path><path marker-end="url(#mermaid-1726066049784_flowchart-v2-pointEnd)" style="" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" id="L_D_D2_11" d="M429.957,711L435.952,711C441.948,711,453.939,711,466.13,711C478.321,711,490.712,711,502.437,711C514.161,711,525.219,711,530.748,711L536.277,711"></path><path marker-end="url(#mermaid-1726066049784_flowchart-v2-pointEnd)" style="" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" id="L_D_D3_12" d="M364.16,738L381.122,759.5C398.083,781,432.006,824,453.505,845.5C475.003,867,484.076,867,492.482,867C500.888,867,508.628,867,512.497,867L516.367,867"></path><path marker-end="url(#mermaid-1726066049784_flowchart-v2-pointEnd)" style="" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" id="L_E_E1_13" d="M374.81,1048L389.997,1035.167C405.183,1022.333,435.557,996.667,456.506,983.833C477.456,971,488.983,971,499.843,971C510.703,971,520.896,971,525.993,971L531.09,971"></path><path marker-end="url(#mermaid-1726066049784_flowchart-v2-pointEnd)" style="" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" id="L_E_E2_14" d="M419.199,1075L426.988,1075C434.776,1075,450.353,1075,465.514,1075C480.676,1075,495.422,1075,509.501,1075C523.581,1075,536.993,1075,543.7,1075L550.406,1075"></path><path marker-end="url(#mermaid-1726066049784_flowchart-v2-pointEnd)" style="" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" id="L_E_E3_15" d="M374.81,1102L389.997,1114.833C405.183,1127.667,435.557,1153.333,457.09,1166.167C478.624,1179,491.318,1179,503.345,1179C515.372,1179,526.733,1179,532.413,1179L538.094,1179"></path><path marker-end="url(#mermaid-1726066049784_flowchart-v2-pointEnd)" style="" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" id="L_F_F1_16" d="M364.16,1412L381.122,1390.5C398.083,1369,432.006,1326,452.41,1304.5C472.814,1283,479.699,1283,485.917,1283C492.135,1283,497.687,1283,500.463,1283L503.238,1283"></path><path marker-end="url(#mermaid-1726066049784_flowchart-v2-pointEnd)" style="" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" id="L_F_F2_17" d="M406.761,1412L416.623,1407.833C426.484,1403.667,446.207,1395.333,461.277,1391.167C476.347,1387,486.764,1387,496.515,1387C506.266,1387,515.35,1387,519.892,1387L524.434,1387"></path><path marker-end="url(#mermaid-1726066049784_flowchart-v2-pointEnd)" style="" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" id="L_F_F3_18" d="M406.761,1466L416.623,1470.167C426.484,1474.333,446.207,1482.667,460.566,1486.833C474.926,1491,483.922,1491,492.251,1491C500.581,1491,508.243,1491,512.075,1491L515.906,1491"></path><path marker-end="url(#mermaid-1726066049784_flowchart-v2-pointEnd)" style="" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" id="L_F_F4_19" d="M364.16,1466L381.122,1487.5C398.083,1509,432.006,1552,455.302,1573.5C478.598,1595,491.266,1595,503.267,1595C515.268,1595,526.603,1595,532.27,1595L537.938,1595"></path><path marker-end="url(#mermaid-1726066049784_flowchart-v2-pointEnd)" style="" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" id="L_B1_G_20" d="M686.121,35L697.783,35C709.445,35,732.77,35,747.713,35C762.657,35,769.22,35,775.117,35C781.013,35,786.243,35,788.858,35L791.473,35"></path><path marker-end="url(#mermaid-1726066049784_flowchart-v2-pointEnd)" style="" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" id="L_D2_H_21" d="M681.746,685.648L694.137,681.206C706.529,676.765,731.311,667.883,746.558,663.441C761.805,659,767.517,659,772.562,659C777.607,659,781.985,659,784.174,659L786.363,659"></path><path marker-end="url(#mermaid-1726066049784_flowchart-v2-pointEnd)" style="" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" id="L_D1_I_22" d="M683.313,555L695.443,555C707.573,555,731.833,555,746.047,555C760.26,555,764.427,555,767.927,555C771.427,555,774.26,555,775.677,555L777.094,555"></path><path marker-end="url(#mermaid-1726066049784_flowchart-v2-pointEnd)" style="" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" id="L_D2_J_23" d="M681.746,736.352L694.137,740.794C706.529,745.235,731.311,754.117,748.327,758.559C765.344,763,774.594,763,783.177,763C791.76,763,799.677,763,803.635,763L807.594,763"></path><path marker-end="url(#mermaid-1726066049784_flowchart-v2-pointEnd)" style="" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" id="L_D3_K_24" d="M701.656,867L710.729,867C719.802,867,737.948,867,749.975,867C762.002,867,767.91,867,773.152,867C778.393,867,782.968,867,785.256,867L787.543,867"></path><path marker-end="url(#mermaid-1726066049784_flowchart-v2-pointEnd)" style="" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" id="L_E1_L_25" d="M686.934,971L698.46,971C709.987,971,733.04,971,750.069,971C767.098,971,778.102,971,788.439,971C798.776,971,808.447,971,813.282,971L818.117,971"></path><path marker-end="url(#mermaid-1726066049784_flowchart-v2-pointEnd)" style="" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" id="L_F4_M_26" d="M680.086,1595L692.754,1595C705.422,1595,730.758,1595,747.916,1595C765.075,1595,774.056,1595,782.37,1595C790.685,1595,798.333,1595,802.157,1595L805.98,1595"></path></g><g class="edgeLabels"><g class="edgeLabel"><g transform="translate(0, 0)" class="label"><foreignObject height="0" width="0"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" class="labelBkg" xmlns="http://www.w3.org/1999/xhtml"><span class="edgeLabel"></span></div></foreignObject></g></g><g class="edgeLabel"><g transform="translate(0, 0)" class="label"><foreignObject height="0" width="0"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" class="labelBkg" xmlns="http://www.w3.org/1999/xhtml"><span class="edgeLabel"></span></div></foreignObject></g></g><g class="edgeLabel"><g transform="translate(0, 0)" class="label"><foreignObject height="0" width="0"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" class="labelBkg" xmlns="http://www.w3.org/1999/xhtml"><span class="edgeLabel"></span></div></foreignObject></g></g><g class="edgeLabel"><g transform="translate(0, 0)" class="label"><foreignObject height="0" width="0"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" class="labelBkg" xmlns="http://www.w3.org/1999/xhtml"><span class="edgeLabel"></span></div></foreignObject></g></g><g class="edgeLabel"><g transform="translate(0, 0)" class="label"><foreignObject height="0" width="0"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" class="labelBkg" xmlns="http://www.w3.org/1999/xhtml"><span class="edgeLabel"></span></div></foreignObject></g></g><g class="edgeLabel"><g transform="translate(0, 0)" class="label"><foreignObject height="0" width="0"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" class="labelBkg" xmlns="http://www.w3.org/1999/xhtml"><span class="edgeLabel"></span></div></foreignObject></g></g><g class="edgeLabel"><g transform="translate(0, 0)" class="label"><foreignObject height="0" width="0"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" class="labelBkg" xmlns="http://www.w3.org/1999/xhtml"><span class="edgeLabel"></span></div></foreignObject></g></g><g class="edgeLabel"><g transform="translate(0, 0)" class="label"><foreignObject height="0" width="0"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" class="labelBkg" xmlns="http://www.w3.org/1999/xhtml"><span class="edgeLabel"></span></div></foreignObject></g></g><g class="edgeLabel"><g transform="translate(0, 0)" class="label"><foreignObject height="0" width="0"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" class="labelBkg" xmlns="http://www.w3.org/1999/xhtml"><span class="edgeLabel"></span></div></foreignObject></g></g><g class="edgeLabel"><g transform="translate(0, 0)" class="label"><foreignObject height="0" width="0"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" class="labelBkg" xmlns="http://www.w3.org/1999/xhtml"><span class="edgeLabel"></span></div></foreignObject></g></g><g class="edgeLabel"><g transform="translate(0, 0)" class="label"><foreignObject height="0" width="0"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" class="labelBkg" xmlns="http://www.w3.org/1999/xhtml"><span class="edgeLabel"></span></div></foreignObject></g></g><g class="edgeLabel"><g transform="translate(0, 0)" class="label"><foreignObject height="0" width="0"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" class="labelBkg" xmlns="http://www.w3.org/1999/xhtml"><span class="edgeLabel"></span></div></foreignObject></g></g><g class="edgeLabel"><g transform="translate(0, 0)" class="label"><foreignObject height="0" width="0"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" class="labelBkg" xmlns="http://www.w3.org/1999/xhtml"><span class="edgeLabel"></span></div></foreignObject></g></g><g class="edgeLabel"><g transform="translate(0, 0)" class="label"><foreignObject height="0" width="0"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" class="labelBkg" xmlns="http://www.w3.org/1999/xhtml"><span class="edgeLabel"></span></div></foreignObject></g></g><g class="edgeLabel"><g transform="translate(0, 0)" class="label"><foreignObject height="0" width="0"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" class="labelBkg" xmlns="http://www.w3.org/1999/xhtml"><span class="edgeLabel"></span></div></foreignObject></g></g><g class="edgeLabel"><g transform="translate(0, 0)" class="label"><foreignObject height="0" width="0"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" class="labelBkg" xmlns="http://www.w3.org/1999/xhtml"><span class="edgeLabel"></span></div></foreignObject></g></g><g class="edgeLabel"><g transform="translate(0, 0)" class="label"><foreignObject height="0" width="0"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" class="labelBkg" xmlns="http://www.w3.org/1999/xhtml"><span class="edgeLabel"></span></div></foreignObject></g></g><g class="edgeLabel"><g transform="translate(0, 0)" class="label"><foreignObject height="0" width="0"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" class="labelBkg" xmlns="http://www.w3.org/1999/xhtml"><span class="edgeLabel"></span></div></foreignObject></g></g><g class="edgeLabel"><g transform="translate(0, 0)" class="label"><foreignObject height="0" width="0"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" class="labelBkg" xmlns="http://www.w3.org/1999/xhtml"><span class="edgeLabel"></span></div></foreignObject></g></g><g class="edgeLabel"><g transform="translate(0, 0)" class="label"><foreignObject height="0" width="0"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" class="labelBkg" xmlns="http://www.w3.org/1999/xhtml"><span class="edgeLabel"></span></div></foreignObject></g></g><g class="edgeLabel"><g transform="translate(0, 0)" class="label"><foreignObject height="0" width="0"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" class="labelBkg" xmlns="http://www.w3.org/1999/xhtml"><span class="edgeLabel"></span></div></foreignObject></g></g><g class="edgeLabel"><g transform="translate(0, 0)" class="label"><foreignObject height="0" width="0"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" class="labelBkg" xmlns="http://www.w3.org/1999/xhtml"><span class="edgeLabel"></span></div></foreignObject></g></g><g class="edgeLabel"><g transform="translate(0, 0)" class="label"><foreignObject height="0" width="0"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" class="labelBkg" xmlns="http://www.w3.org/1999/xhtml"><span class="edgeLabel"></span></div></foreignObject></g></g><g class="edgeLabel"><g transform="translate(0, 0)" class="label"><foreignObject height="0" width="0"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" class="labelBkg" xmlns="http://www.w3.org/1999/xhtml"><span class="edgeLabel"></span></div></foreignObject></g></g><g class="edgeLabel"><g transform="translate(0, 0)" class="label"><foreignObject height="0" width="0"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" class="labelBkg" xmlns="http://www.w3.org/1999/xhtml"><span class="edgeLabel"></span></div></foreignObject></g></g><g class="edgeLabel"><g transform="translate(0, 0)" class="label"><foreignObject height="0" width="0"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" class="labelBkg" xmlns="http://www.w3.org/1999/xhtml"><span class="edgeLabel"></span></div></foreignObject></g></g><g class="edgeLabel"><g transform="translate(0, 0)" class="label"><foreignObject height="0" width="0"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" class="labelBkg" xmlns="http://www.w3.org/1999/xhtml"><span class="edgeLabel"></span></div></foreignObject></g></g></g><g class="nodes"><g transform="translate(101.39453125, 711)" id="flowchart-A-0" class="node default"><rect height="54" width="186.7890625" y="-27" x="-93.39453125" data-et="node" data-id="abc" style="" class="basic label-container"></rect><g transform="translate(-63.39453125, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="126.7890625"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" xmlns="http://www.w3.org/1999/xhtml"><span class="nodeLabel"><p>LLM Development</p></span></div></foreignObject></g></g><g transform="translate(342.859375, 139)" id="flowchart-B-1" class="node default"><rect height="54" width="181.0859375" y="-27" x="-90.54296875" data-et="node" data-id="abc" style="" class="basic label-container"></rect><g transform="translate(-60.54296875, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="121.0859375"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" xmlns="http://www.w3.org/1999/xhtml"><span class="nodeLabel"><p>Data Preparation</p></span></div></foreignObject></g></g><g transform="translate(342.859375, 399)" id="flowchart-C-3" class="node default"><rect height="54" width="196.140625" y="-27" x="-98.0703125" data-et="node" data-id="abc" style="" class="basic label-container"></rect><g transform="translate(-68.0703125, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="136.140625"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" xmlns="http://www.w3.org/1999/xhtml"><span class="nodeLabel"><p>Model Architecture</p></span></div></foreignObject></g></g><g transform="translate(342.859375, 711)" id="flowchart-D-5" class="node default"><rect height="54" width="174.1953125" y="-27" x="-87.09765625" data-et="node" data-id="abc" style="" class="basic label-container"></rect><g transform="translate(-57.09765625, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="114.1953125"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" xmlns="http://www.w3.org/1999/xhtml"><span class="nodeLabel"><p>Training Process</p></span></div></foreignObject></g></g><g transform="translate(342.859375, 1075)" id="flowchart-E-7" class="node default"><rect height="54" width="152.6796875" y="-27" x="-76.33984375" data-et="node" data-id="abc" style="" class="basic label-container"></rect><g transform="translate(-46.33984375, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="92.6796875"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" xmlns="http://www.w3.org/1999/xhtml"><span class="nodeLabel"><p>Optimization</p></span></div></foreignObject></g></g><g transform="translate(342.859375, 1439)" id="flowchart-F-9" class="node default"><rect height="54" width="134.9140625" y="-27" x="-67.45703125" data-et="node" data-id="abc" style="" class="basic label-container"></rect><g transform="translate(-37.45703125, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="74.9140625"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" xmlns="http://www.w3.org/1999/xhtml"><span class="nodeLabel"><p>Evaluation</p></span></div></foreignObject></g></g><g transform="translate(611.01171875, 35)" id="flowchart-B1-11" class="node default"><rect height="54" width="150.21875" y="-27" x="-75.109375" data-et="node" data-id="abc" style="" class="basic label-container"></rect><g transform="translate(-45.109375, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="90.21875"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" xmlns="http://www.w3.org/1999/xhtml"><span class="nodeLabel"><p>Tokenization</p></span></div></foreignObject></g></g><g transform="translate(611.01171875, 139)" id="flowchart-B2-13" class="node default"><rect height="54" width="121.5" y="-27" x="-60.75" data-et="node" data-id="abc" style="" class="basic label-container"></rect><g transform="translate(-30.75, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="61.5"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" xmlns="http://www.w3.org/1999/xhtml"><span class="nodeLabel"><p>Cleaning</p></span></div></foreignObject></g></g><g transform="translate(611.01171875, 243)" id="flowchart-B3-15" class="node default"><rect height="54" width="159.9453125" y="-27" x="-79.97265625" data-et="node" data-id="abc" style="" class="basic label-container"></rect><g transform="translate(-49.97265625, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="99.9453125"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" xmlns="http://www.w3.org/1999/xhtml"><span class="nodeLabel"><p>Augmentation</p></span></div></foreignObject></g></g><g transform="translate(611.01171875, 347)" id="flowchart-C1-17" class="node default"><rect height="54" width="240.1640625" y="-27" x="-120.08203125" data-et="node" data-id="abc" style="" class="basic label-container"></rect><g transform="translate(-90.08203125, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="180.1640625"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" xmlns="http://www.w3.org/1999/xhtml"><span class="nodeLabel"><p>Transformer Architecture</p></span></div></foreignObject></g></g><g transform="translate(611.01171875, 451)" id="flowchart-C2-19" class="node default"><rect height="54" width="217.3359375" y="-27" x="-108.66796875" data-et="node" data-id="abc" style="" class="basic label-container"></rect><g transform="translate(-78.66796875, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="157.3359375"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" xmlns="http://www.w3.org/1999/xhtml"><span class="nodeLabel"><p>Attention Mechanisms</p></span></div></foreignObject></g></g><g transform="translate(611.01171875, 555)" id="flowchart-D1-21" class="node default"><rect height="54" width="144.6015625" y="-27" x="-72.30078125" data-et="node" data-id="abc" style="" class="basic label-container"></rect><g transform="translate(-42.30078125, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="84.6015625"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" xmlns="http://www.w3.org/1999/xhtml"><span class="nodeLabel"><p>Pre-training</p></span></div></foreignObject></g></g><g transform="translate(611.01171875, 711)" id="flowchart-D2-23" class="node default"><rect height="54" width="141.46875" y="-27" x="-70.734375" data-et="node" data-id="abc" style="" class="basic label-container"></rect><g transform="translate(-40.734375, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="81.46875"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" xmlns="http://www.w3.org/1999/xhtml"><span class="nodeLabel"><p>Fine-tuning</p></span></div></foreignObject></g></g><g transform="translate(611.01171875, 867)" id="flowchart-D3-25" class="node default"><rect height="54" width="181.2890625" y="-27" x="-90.64453125" data-et="node" data-id="abc" style="" class="basic label-container"></rect><g transform="translate(-60.64453125, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="121.2890625"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" xmlns="http://www.w3.org/1999/xhtml"><span class="nodeLabel"><p>Reward Modeling</p></span></div></foreignObject></g></g><g transform="translate(611.01171875, 971)" id="flowchart-E1-27" class="node default"><rect height="54" width="151.84375" y="-27" x="-75.921875" data-et="node" data-id="abc" style="" class="basic label-container"></rect><g transform="translate(-45.921875, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="91.84375"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" xmlns="http://www.w3.org/1999/xhtml"><span class="nodeLabel"><p>Quantization</p></span></div></foreignObject></g></g><g transform="translate(611.01171875, 1075)" id="flowchart-E2-29" class="node default"><rect height="54" width="113.2109375" y="-27" x="-56.60546875" data-et="node" data-id="abc" style="" class="basic label-container"></rect><g transform="translate(-26.60546875, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="53.2109375"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" xmlns="http://www.w3.org/1999/xhtml"><span class="nodeLabel"><p>Pruning</p></span></div></foreignObject></g></g><g transform="translate(611.01171875, 1179)" id="flowchart-E3-31" class="node default"><rect height="54" width="137.8359375" y="-27" x="-68.91796875" data-et="node" data-id="abc" style="" class="basic label-container"></rect><g transform="translate(-38.91796875, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="77.8359375"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" xmlns="http://www.w3.org/1999/xhtml"><span class="nodeLabel"><p>Distillation</p></span></div></foreignObject></g></g><g transform="translate(611.01171875, 1283)" id="flowchart-F1-33" class="node default"><rect height="54" width="207.546875" y="-27" x="-103.7734375" data-et="node" data-id="abc" style="" class="basic label-container"></rect><g transform="translate(-73.7734375, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="147.546875"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" xmlns="http://www.w3.org/1999/xhtml"><span class="nodeLabel"><p>Task-specific Metrics</p></span></div></foreignObject></g></g><g transform="translate(611.01171875, 1387)" id="flowchart-F2-35" class="node default"><rect height="54" width="165.15625" y="-27" x="-82.578125" data-et="node" data-id="abc" style="" class="basic label-container"></rect><g transform="translate(-52.578125, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="105.15625"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" xmlns="http://www.w3.org/1999/xhtml"><span class="nodeLabel"><p>Generalization</p></span></div></foreignObject></g></g><g transform="translate(611.01171875, 1491)" id="flowchart-F3-37" class="node default"><rect height="54" width="182.2109375" y="-27" x="-91.10546875" data-et="node" data-id="abc" style="" class="basic label-container"></rect><g transform="translate(-61.10546875, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="122.2109375"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" xmlns="http://www.w3.org/1999/xhtml"><span class="nodeLabel"><p>Fairness and Bias</p></span></div></foreignObject></g></g><g transform="translate(611.01171875, 1595)" id="flowchart-F4-39" class="node default"><rect height="54" width="138.1484375" y="-27" x="-69.07421875" data-et="node" data-id="abc" style="" class="basic label-container"></rect><g transform="translate(-39.07421875, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="78.1484375"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" xmlns="http://www.w3.org/1999/xhtml"><span class="nodeLabel"><p>Robustness</p></span></div></foreignObject></g></g><g transform="translate(907.296875, 35)" id="flowchart-G-41" class="node default"><rect height="54" width="223.6484375" y="-27" x="-111.82421875" data-et="node" data-id="abc" style="" class="basic label-container"></rect><g transform="translate(-81.82421875, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="163.6484375"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" xmlns="http://www.w3.org/1999/xhtml"><span class="nodeLabel"><p>Advanced Tokenization</p></span></div></foreignObject></g></g><g transform="translate(907.296875, 659)" id="flowchart-H-43" class="node default"><rect height="54" width="233.8671875" y="-27" x="-116.93359375" data-et="node" data-id="abc" style="" class="basic label-container"></rect><g transform="translate(-86.93359375, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="173.8671875"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" xmlns="http://www.w3.org/1999/xhtml"><span class="nodeLabel"><p>Sophisticated Prompting</p></span></div></foreignObject></g></g><g transform="translate(907.296875, 555)" id="flowchart-I-45" class="node default"><rect height="54" width="252.40625" y="-27" x="-126.203125" data-et="node" data-id="abc" style="" class="basic label-container"></rect><g transform="translate(-96.203125, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="192.40625"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" xmlns="http://www.w3.org/1999/xhtml"><span class="nodeLabel"><p>Masked Language Modeling</p></span></div></foreignObject></g></g><g transform="translate(907.296875, 763)" id="flowchart-J-47" class="node default"><rect height="54" width="191.40625" y="-27" x="-95.703125" data-et="node" data-id="abc" style="" class="basic label-container"></rect><g transform="translate(-65.703125, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="131.40625"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" xmlns="http://www.w3.org/1999/xhtml"><span class="nodeLabel"><p>Few-shot Learning</p></span></div></foreignObject></g></g><g transform="translate(907.296875, 867)" id="flowchart-K-49" class="node default"><rect height="54" width="231.5078125" y="-27" x="-115.75390625" data-et="node" data-id="abc" style="" class="basic label-container"></rect><g transform="translate(-85.75390625, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="171.5078125"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" xmlns="http://www.w3.org/1999/xhtml"><span class="nodeLabel"><p>Reinforcement Learning</p></span></div></foreignObject></g></g><g transform="translate(907.296875, 971)" id="flowchart-L-51" class="node default"><rect height="54" width="170.359375" y="-27" x="-85.1796875" data-et="node" data-id="abc" style="" class="basic label-container"></rect><g transform="translate(-55.1796875, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="110.359375"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" xmlns="http://www.w3.org/1999/xhtml"><span class="nodeLabel"><p>Mixed Precision</p></span></div></foreignObject></g></g><g transform="translate(907.296875, 1595)" id="flowchart-M-53" class="node default"><rect height="54" width="194.6328125" y="-27" x="-97.31640625" data-et="node" data-id="abc" style="" class="basic label-container"></rect><g transform="translate(-67.31640625, -12)" style="" class="label"><rect></rect><foreignObject height="24" width="134.6328125"><div style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;" xmlns="http://www.w3.org/1999/xhtml"><span class="nodeLabel"><p>Adversarial Testing</p></span></div></foreignObject></g></g></g></g></g></svg>\" alt=\"Mermaid diagram 2\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ab8c2f",
   "metadata": {},
   "source": [
    "This mind map illustrates the interconnected nature of LLM development, from data preparation all the way through to evaluation.\n",
    "\n",
    "## Where Do We Go From Here?\n",
    "\n",
    "Congratulations on making it this far! You've laid a solid foundation in LLM development. But as with any rapidly evolving field, there's always more to learn. Here are some suggestions for your next steps:\n",
    "\n",
    "1. **Hands-on Projects**: Start applying these concepts to real-world projects. Nothing beats hands-on experience!\n",
    "\n",
    "2. **Stay Updated**: Follow AI research labs, attend conferences, and read papers to keep up with the latest advancements.\n",
    "\n",
    "3. **Collaborate**: Join online communities of AI enthusiasts and practitioners. Share your experiences and learn from others.\n",
    "\n",
    "4. **Ethical Considerations**: Dive deeper into the ethical implications of LLMs. As these models become more powerful, understanding their societal impact becomes crucial.\n",
    "\n",
    "5. **Specialized Applications**: Explore how LLMs are being applied in specific domains like healthcare, finance, or legal tech.\n",
    "\n",
    "6. **Contribute to Open Source**: Many LLM projects are open source. Contributing to these can be a great way to hone your skills and give back to the community.\n",
    "\n",
    "Remember, the field of AI and LLMs is advancing at a breakneck pace. What seems cutting-edge today might be standard practice tomorrow. Stay curious, keep learning, and don't be afraid to push the boundaries!\n",
    "\n",
    "# Homework\n",
    "\n",
    "1. **Custom Multilingual Tokenizer:**\n",
    "   Implement a custom tokenizer that can handle multiple languages (e.g., English, Mandarin, Arabic) and compare its performance with standard tokenizers from libraries like HuggingFace Tokenizers. Evaluate on metrics such as vocabulary size, out-of-vocabulary rate, and tokenization speed.\n",
    "\n",
    "2. **Advanced Prompting for Complex Reasoning:**\n",
    "   Design and implement an advanced prompting strategy for multi-step mathematical problem-solving. Use techniques like chain-of-thought prompting or self-consistency. Compare its performance with standard prompting on a dataset of complex math problems.\n",
    "\n",
    "3. **Comprehensive Data Preparation Pipeline:**\n",
    "   Develop a data preparation pipeline that includes advanced cleaning (e.g., named entity anonymization), augmentation (e.g., back-translation), and bias mitigation techniques. Apply this pipeline to a real-world dataset like Civil Comments and analyze its impact on model performance and fairness metrics.\n",
    "\n",
    "4. **Curriculum Learning for Pre-training:**\n",
    "   Implement a pre-training methodology that incorporates curriculum learning for a small language model. Start with simple language modeling tasks and progressively increase difficulty. Evaluate its effectiveness compared to standard pre-training on perplexity and downstream task performance.\n",
    "\n",
    "5. **Parameter-Efficient Fine-Tuning:**\n",
    "   Design and implement a parameter-efficient fine-tuning method like LoRA (Low-Rank Adaptation) or adapters. Apply it to a pre-trained model for a specific task (e.g., sentiment analysis) and compare its performance and computational efficiency with full fine-tuning.\n",
    "\n",
    "6. **Reward Modeling and RL Fine-Tuning:**\n",
    "   Develop a reward modeling system for text summarization. Use human feedback to train the reward model, then use it to fine-tune a language model using a technique like PPO (Proximal Policy Optimization). Compare the results with supervised fine-tuning.\n",
    "\n",
    "7. **Quantization Strategy Comparison:**\n",
    "   Implement and compare different quantization strategies (e.g., post-training quantization, quantization-aware training, and mixed-precision quantization) for a pre-trained language model. Analyze the trade-offs between model size, inference speed, and performance on a benchmark dataset.\n",
    "\n",
    "8. **Comprehensive Evaluation Framework:**\n",
    "   Design a comprehensive evaluation framework for a multi-task language model. Include metrics for performance (e.g., accuracy, F1 score), robustness (e.g., adversarial examples), fairness (e.g., demographic parity), and safety (e.g., toxicity detection). Apply this framework to evaluate a real model like BERT or RoBERTa and present your findings.\n",
    "\n",
    "# References and Citations\n",
    "\n",
    "1. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.\n",
    "\n",
    "2. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... & Amodei, D. (2020). Language models are few-shot learners. Advances in neural information processing systems, 33, 1877-1901.\n",
    "\n",
    "3. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., ... & Liu, P. J. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140), 1-67.\n",
    "\n",
    "4. Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.\n",
    "\n",
    "5. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., ... & Chen, W. (2021). LoRA: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685.\n",
    "\n",
    "6. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., ... & Lowe, R. (2022). Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155.\n",
    "\n",
    "7. Gholami, A., Kim, S., Dong, Z., Yao, Z., Mahoney, M. W., & Keutzer, K. (2021). A survey of quantization methods for efficient neural network inference. arXiv preprint arXiv:2103.13630.\n",
    "\n",
    "8. Ribeiro, M. T., Wu, T., Guestrin, C., & Singh, S. (2020). Beyond accuracy: Behavioral testing of NLP models with CheckList. arXiv preprint arXiv:2005.04118.\n",
    "\n",
    "9. Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., ... & Rush, A. M. (2020). Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations (pp. 38-45).\n",
    "\n",
    "10. Bengio, Y., Louradour, J., Collobert, R., & Weston, J. (2009). Curriculum learning. In Proceedings of the 26th annual international conference on machine learning (pp. 41-48).\n",
    "\n",
    "11. Zhu, C., Cheng, Y., Gan, Z., Sun, S., Goldstein, T., & Liu, J. (2020). Freelb: Enhanced adversarial training for natural language understanding. In International Conference on Learning Representations.\n",
    "\n",
    "12. Gehman, S., Gururangan, S., Sap, M., Choi, Y., & Smith, N. A. (2020). RealToxicityPrompts: Evaluating neural toxic degeneration in language models. In Findings of the Association for Computational Linguistics: EMNLP 2020 (pp. 3356-3369).\n",
    "\n",
    "13. Weidinger, L., Mellor, J., Rauh, M., Griffin, C., Uesato, J., Huang, P. S., ... & Gabriel, I. (2021). Ethical and social risks of harm from Language Models. arXiv preprint arXiv:2112.04359.\n",
    "\n",
    "14. Bender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021). On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (pp. 610-623).\n",
    "\n",
    "15. Sanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108.\n",
    "\n",
    "These references cover the main topics discussed in our lesson, including transformer architecture, few-shot learning, transfer learning, pre-training and fine-tuning techniques, quantization, evaluation methods, and ethical considerations in LLM development. They provide a solid foundation for further exploration of these topics."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
